{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Joint HPC Exchange","text":"<ul> <li> <p> About Us</p> <p>The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This fee-for-service core is a long-standing collaborative effort between Biostatistics and the Computational Biology &amp; Research Computing group in the department of Molecular Microbiology and Immunology. Thefacility is open to all Johns Hopkins affiliated researchers. </p> </li> <li> <p> Community</p> <p>The facility is used primarily by labs and research groups in the Johns Hopkins Bloomberg School of Public Health (SPH), the Johns Hopkins School of Medicine (SOM) and the Lieber Institute for Brain Development (LIBD). We have over 1000 user accounts, 400 active users and         approximately 100 unique users per quarter.</p> </li> <li> <p> Cluster Details</p> <p>The computing and storage systems are optimized for genomics and biomedical research. The cluster has 65 compute nodes, providing about 2800 cores, 30TB of DRAM and over 14 PB of networked mass storage. The network fabric consists of a 10 Gbps ethernet. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ. Networked mass storage uses open-source file systems (ZFS and Lustre-over-ZFS) to provide low cost file systems. We also have a 2PB disk-to-disk backup system off site for backing up more critical data. The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc.  Job scheduling is performed with the Simple Linux Utility for Resource Management (SLURM).</p> </li> <li> <p> Cost Recovery</p> <p>The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders.</p> </li> </ul> <p>If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the directors (jhpce@jhu.edu) to determine whether we can accommodate your needs.</p> <p>If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form.</p> <p>Mark Miller and Brian Caffo Co-Directors, JHPCE</p>","tags":["refers-to-old-website"]},{"location":"aboutus/model/","title":"Joint HPC Exchange","text":"","tags":["refers-to-old-website"]},{"location":"aboutus/model/#about-us","title":"About Us","text":"<p>The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This fee-for-service core is a long-standing collaborative effort between Biostatistics and the Computational Biology &amp; Research Computing group in the department of Molecular Microbiology and Immunology. The facility is open to all Johns Hopkins affiliated researchers.</p>","tags":["refers-to-old-website"]},{"location":"aboutus/model/#community","title":"Community","text":"<p>The facility is used primarily by labs and research groups in the Johns Hopkins Bloomberg School of Public Health (SPH), the Johns Hopkins School of Medicine (SOM) and the Lieber Institute for Brain Development (LIBD). We have over 1000 user accounts, 400 active users and approximately 100 unique users per quarter.</p>","tags":["refers-to-old-website"]},{"location":"aboutus/model/#cluster-details","title":"Cluster Details","text":"<p>The computing and storage systems are optimized for genomics and biomedical research. The cluster has 65 compute nodes, providing about 2800 cores, 30TB of DRAM and over 14 PB of networked mass storage. The network fabric consists of a 10 Gbps ethernet. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ.</p> <p>Networked mass storage uses open-source file systems (ZFS and Lustre-over-ZFS) to provide low cost file systems. We also have a 2PB disk-to-disk backup system off site for backing up more critical data.</p> <p>The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc.  Job scheduling is performed with the Simple Linux Utility for Resource Management (SLURM).</p>","tags":["refers-to-old-website"]},{"location":"aboutus/model/#cost-recovery","title":"Cost Recovery","text":"<p>The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders.</p> <p>If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the directors (jhpce@jhu.edu) to determine whether we can accommodate your needs.</p> <p>If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form.</p> <p>Mark Miller and Brian Caffo Co-Directors, JHPCE</p>","tags":["refers-to-old-website"]},{"location":"aboutus/staff/","title":"Staff","text":"<p>Who we are:</p>","tags":["needs-to-be-written"]},{"location":"access/access-overview/","title":"Accessing the Cluster: Overview","text":"","tags":["done","topic-overview"]},{"location":"access/access-overview/#cluster-structure-overview","title":"Cluster structure overview","text":"<p>The cluster consists of some public-facing hosts with the remaining computers \"behind them\" on private networks. An incomplete approximation:</p> <pre><code>graph TD\n   A[Your computer] --&gt; B([Login nodes])\n   A[Your computer] -- Only via Hopkins/VPN --&gt; D((Web Portal))\n   A[Your computer] --&gt; C([Transfer node])\n   B([Login nodes]) --&gt; E[Compute node1]\n   D((Web Portal)) --&gt; E[Compute node1]\n   B([Login nodes]) --&gt; F[Compute node2]\n   D((Web Portal)) --&gt; F[Compute node2]\n   B([Login nodes]) --&gt; G[Compute node3]\n   D((Web Portal)) --&gt; G[Compute node3]\n   C([Transfer node]) &lt;--&gt; H[(storage servers)]</code></pre>","tags":["done","topic-overview"]},{"location":"access/access-overview/#public-facing-login-and-transfer","title":"Public-facing: Login and Transfer","text":"<p>The login and transfer nodes are accessible to the wider Internet.  For security reasons, the web portal is only available to computers on Hopkins networks. If you are not on a Hopkins campus, that means that you need to use the VPN to be able to see that node.</p> <ul> <li>login: Normally jhpce01.jhsph.edu and jhpce02.jhsph.edu (currently jhpce03.jhsph.edu)</li> <li>transfer: jhpce-transfer01.jhsph.edu</li> <li>web portal: jhpce-app02.jhsph.edu</li> </ul>","tags":["done","topic-overview"]},{"location":"access/access-overview/#ssh-is-the-primary-method","title":"SSH Is The Primary Method","text":"<p>Access to the JHPCE cluster requires the use of SSH.</p> <p>SSH stands for Secure SHell. SSH is actually a set of internet standard protocols. Programs implementing these protocols include both command line interface (CLI) tools and those with graphic user interfaces (GUI).  They all enable you to make secure, encrypted connections from one computer to the next.</p> <p>This document provides more information on SSH.</p>","tags":["done","topic-overview"]},{"location":"access/access-overview/#x11the-x-window-system","title":"X11/The X Window System","text":"<p>The X Window System (aka X11, aka X) is the primary GUI system for UNIX computers. X allows a program (an X \"client\") running on a UNIX computer in the cluster to be displayed on a remote computer (running an X \"server\"<sup>1</sup>) over the network. To use it, your local computer needs to have X server software installed on it.</p> <p>SSH provides support for tunnelling X11 over an encrypted connection. You may need to tell SSH that you want that service, by, for example, adding the -X flag to an ssh command in a macOS Terminal.</p> <ul> <li>macOS users need to install XQuartz from xquartz.org.</li> <li>Windows users need to use a program like MobaXterm (highly recommended) or Cygwin. We have a page describing installing and using MobaXterm.</li> <li>Linux laptop or desktop users are already using X as their windowing system.</li> </ul> <p>For more information, see our X11 document.</p>","tags":["done","topic-overview"]},{"location":"access/access-overview/#multi-factor-authentication-mfa","title":"Multi-factor authentication (MFA)","text":"<p>There are two basic \"factors\" required to log into a computer, whether your laptop or a remote UNIX cluster login node -- your username and a password. JHPCE requires the use of an additional factor, either a one-time password (OTP) six digit code, or the use of SSH key pairs. </p>","tags":["done","topic-overview"]},{"location":"access/access-overview/#one-time-passwords","title":"One Time Passwords","text":"<p>When you SSH into JHPCE, you will be prompted for a \u201cVerification Code:\u201d This is your cue to enter in a one-time password six digit code.</p> <p>Programs like <code>Google Authenticator</code> and <code>Micrsoft Authenticator</code> generate one-time password codes (OTP). These are only good for a single use, whether you successfully log in or not. Typically they are used to generate a stream of time-based OTPs, or TOTPs. These are only good for one minute, adding another layer of difficulty for someone trying to impersonate you.</p> <p>These programs are usually used on smartphones, but there are programs available to create them on laptops and desktops. The key with using ANY OTP program is to get it from a trusted source. We will default to mentioning the <code>Google Authenticator</code>.</p> <p>After you log into JHPCE for the first time, you should immediately configure your OTP program using a \"secret\" accessible to you on the cluster via the <code>auth_util</code> program. Instructions for doing that are found in the Orientation document.</p>","tags":["done","topic-overview"]},{"location":"access/access-overview/#web-portal","title":"Web Portal","text":"<p>We have a web server named jhpce-app02.jhsph.edu configured to offer a growing number of services. Click on the links to learn more.</p> <ul> <li>Reset your password or generate a OTP (learn more)</li> <li>Run applications on the cluster (RStudio, JupyterLab, VS Code) (learn more)</li> <li>Inspect a catalog of research databases (under development) (learn more)</li> </ul>","tags":["done","topic-overview"]},{"location":"access/access-overview/#safe-desktop","title":"SAFE Desktop","text":"<p>A virtual desktop named the Secure Analytic Framework Environment (SAFE) is a resource that some people find useful for their computing, as well as a means to access JHPCE (via the MobaXterm application). It is a virtual Windows computer equipped with many applications JHPCE members use for their research, including SAS and Stata. It includes 100GB of secure data storage for sensitive (PHI, PII) information. That data can be shared by research groups. Free for Johns Hopkins Medicine staff and students, it requires filling out a form and waiting for approval.</p>","tags":["done","topic-overview"]},{"location":"access/access-overview/#file-transfers","title":"File Transfers","text":"<p>We have a transfer server jhpce-transfer01.jhsph.edu for file transfers into and out of the cluster. It is connected by a 40G Ethernet link to Hopkins networks. This computer also offers a Globus Endpoint service (described here) for transfers from personal computers and other institutions.</p> <p>Transferring data into or out of the cluster is documented here.</p> <p>The login nodes SHOULD NOT be used for file transfers into and out of the cluster beyond extremely trivial cases. Their connections are four times slower and they are relied upon by all of your peers.</p> <p>Please use compute nodes for transfers WITHIN the cluster. For example, copying significant volumes of files from one file system to another, such as <code>/dcs05/a-place/</code> to <code>/dcs07/somewhere-else/</code>. Information about doing that can be found here</p> <ol> <li> <p>Note that X reverses the normal conception of client/server operation, which is that the remote computer is the \"server\". In X11, the \"server\" is the program receiving the keyboard and mouse inputs and displaying the output of remote \"client\" programs.\u00a0\u21a9</p> </li> </ol>","tags":["done","topic-overview"]},{"location":"access/file-transfer/","title":"File Transfer - Overview","text":"<p>Authoring Note</p> <p>This document was copied over from the old web site with few changes. Its information needs to be reviewed, links made to the globus and rclone documents and any others that are created as a result of creating an OVERVIEW document here with details in subordinate documents. Also, it would be nice to mention early on in this OVERVIEW that we have another document about copying data WITHIN the cluster.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#file-transfer-overview","title":"File Transfer - Overview","text":"<p>A number of options exist for transfering files to-and-fro between JHPCE and your local host. Which solution you chose, depends on your use case.</p> <p>If you are wanting to transfer files between JHPCE file systems, then you need to use different tools than are discussed here. Here is a sample batch job that you can use as a model to do such a transfer on a compute node using good rsync arguments.</p> <ul> <li>scp or sftp \u2014 file transfer via command line</li> <li>GUI for sftp \u2014 file transfer by drag and drop from your desktop</li> <li>GlobusOnline \u2014 fast file transfers between GlobusOnline endpoints</li> <li>Aspera  \u2014  very fast file transfer to and from Aspera servers</li> <li>OneDrive access with rclone  \u2014  Use \u201crclone\u201d to access your OneDrive directory (as well as other network drives (AWS buckets, Google Drive\u2026)</li> <li>Unison \u2014 keep directories synced between the cluster and your local computer</li> <li>Mount remote filesystems \u2014 directories at JHPCE mounted on your local host. IS THIS MATERIAL STILL ACCURATE IN 2024? Is this example SSHFS doc worth re-using?</li> <li>ftp (kind of\u2026)</li> </ul> <p>For transferring files to the cluster, you should use <code>jhpce-transfer01.jhsph.edu</code> rather than a login node when connecting. This is both significantly faster and more considerate to other users.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#scp-and-sftp","title":"<code>scp</code> and <code>sftp</code>","text":"<p>The <code>scp</code> and <code>sftp</code> command-line tools are the most common tools used for transferring data to and from the cluster.  The basic tradeoff is between speed (<code>scp</code> is faster) and flexibility (<code>sftp</code> is more flexible).  The <code>scp</code> and <code>sftp</code> commands are available from the Terminal on a MacOS or Linux based laptop/desktop, or from a <code>CMD</code> or <code>Powershell</code> prompt on recent Windows systems.</p> <p>Although both SCP and SFTP utilize the same SSH encryption during file transfer with the same general level of overhead, SCP is usually faster than SFTP at transferring files, especially on high latency networks.  SFTP should be used when you may need an interactive session on the cluster to navigate to a directory before transferring the files, whereas SCP should be used when you know the exact path of the file you want to transfer.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#scp","title":"<code>scp</code>","text":"<p>The <code>scp</code> command can be though of as a <code>network cp</code> command.  The command to transfer a file called <code>data.txt</code> from your local system to your home directory on the cluster would be:</p> <pre><code>scp LOCAL_PATH/data.txt USERID@jhpce-transfer01.jhsph.edu:REMOTE_PATH/REMOTE_TARGET_FILENAME\n</code></pre> <p>where the paths default to your current local directory and home directory on the remote. The target filename if omitted will be the local filename.</p> <p>If you want to copy a file from the cluster to your local laptop/desktop, you would reverse the arguments. For example to copy <code>data2.txt</code> from the cluster to a local file:</p> <pre><code>scp USERID@jhpce-transfer01.jhsph.edu:REMOTE_PATH/data2.txt LOCAL_PATH/LOCAL_TARGET_FILENAME\n</code></pre>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#sftp","title":"<code>sftp</code>","text":"<p>The <code>sftp</code> command is another means of transfering data to and from the cluster.  To use <code>sftp</code>, you would run the command:</p> <pre><code>sftp USERID@jhpce-transfer01.jhsph.edu\n</code></pre> <p>Once you\u2019ve connected, you\u2019ll be shown an <code>sftp&gt;</code> prompt.  From here you can use the shell command <code>ls</code> to get a directory listing, and and <code>cd</code> to change directories.  In addition to <code>ls</code> and <code>cd</code> you can use the <code>get</code> command to transfer a file from the cluster it to your local system, or the <code>put</code> command to transfer a file from your local system to the cluster.  Once you are done with <code>sftp</code>, you would type <code>exit</code> to end the session.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#guis","title":"GUIs","text":"<p>If you prefer drag and drop interface rather than using shell commands, then an application that presents a window for drag and drop is what you want. Depending on which OS you are using, we can recommend the following applications:</p> <p>macOS users might consider Filezilla. It   is an outstanding application that not only provides a GUI browser for FTP, SFTP, but it also allows you to browse WebDav, Amazon S3, and OpenStack Swift file systems. It is free to download and install.</p> <p>Our recommended application for Windows users accessing the JHPCE cluster is MobaXterm. You can also use https://en.wikipedia.org/wiki/WinSCP or Putty if you are already familiar with them.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#rclone","title":"Rclone","text":"<p>Rclone can be used to access network file resources, such as OneDrive, Google Drive, and AWS. See here for instructions on using it to connect to Hopkins OneDrive storage.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#aspera","title":"Aspera","text":"<p>Warning</p> <p>This might be obsolete information as of 20240220 -- that package is no longer visible at the path mentioned below.</p> <p>Aspera is a commercial product from IBM that allows file transfers that are reportedly 20 times faster than <code>ftp</code>. If you download data from the NCBI Aspera server or download/upload data from/to JHU CIDR on the Bayview campus, then you will use Aspera. The Aspera license does not allow us to install the client for our users. You must install it yourself. You may either download the linux client from the aspera site or else use the client that we already downloaded. If you prefer the latter, simply copy the installation script from here:</p> <pre><code>/jhpce/shared/jhpce/core/JHPCE_tools/1.0/packages/aspera-connect-3.6.2.117442-linux-64.sh\n</code></pre> <p>into your home directory, and then run the script:</p> <pre><code>bash aspera-connect-3.6.2.117442-linux-64.sh\n</code></pre> <p>This will install the <code>ascp</code> command under your home directory at  <code>~/.aspera/connect/bin</code>.  You can either add <code>~/.aspera/connect/bin</code> to your <code>PATH</code>, or use the full path to the <code>ascp</code> command to run it.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#unison","title":"Unison","text":"<p>Warning</p> <p>This might be obsolete information as of 20240220 -- the unison package is no longer visible at the path mentioned in Fiksel's documentation.</p> <p>Using Unison, you can keep data synchronized between a directory on the cluster and a directory on your local system.  Please see this excellent document written by Jacob Fiksel, one of our expert JHPCE cluster users: [https://github.com/jfiksel/cluster-example#how-to-use-unison-for-file-transfer-and-syncing] (https://github.com/jfiksel/cluster-example#how-to-use-unison-for-file-transfer-and-syncing)</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#globus","title":"Globus","text":"<p>We have a Globus endpoint. Please see this document.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#mounting-virtual-file-systems","title":"Mounting virtual file systems","text":"<p>A common use case occurs when a user has a pipeline that is periodicially emiting tab delimited files and the user wants to plot these files with a favorite plotting or analysis application that runs on their local host. In this case it is common to mount the remote file system on the local host via NSF or SMB.</p> <p>Unfortunately, given the size and hetergeneity of our user base (which spans the entire medical campus), this is not practical. Instead, we recommend that users create a virtual file system on their OSX machine with the MACFusion application. MacFusion is free and allows you to create a mount point on your local host that looks like just another directory in your local file system. So any applications and scripts on your local host can access the data in that mount point. From the user perspective, it acts just like an SMB or NSF mount point. Data is transferred back and forth via an encrypted link.</p> <p>Obsolete</p> <p>This might be obsolete information as of 20240220 - OSXFUSE is now named macFUSE and is hosted at a different location than what is described below.</p> <p>MacFusion requires the installation of an OSX kernel extenstion and some associated tools. OSXFuse provides the needed extension. OSXFuse implements a so called \u201d FileSystem in USErspace\u201d. This technology is described here. There exist FUSE kernel modules for most flavors of unix and linux.  The procedure for installing OSXFuse and MACFusion is described below.</p> <ul> <li>Downloaded OSXFUSE from sourceforge repository</li> <li>Install OSXFUSE</li> <li>Launch the OSXFUSE installer and perform a custom install.  Be sure to select \u201cMacFuse Compatibility Layer\u201d in the Custom Install screen. </li> <li>After installing the kernel extension it may, or may not, be necessary to reboot your mac.Screen Shot</li> <li>Download and install the Macfusion app from: http://macfusionapp.org.</li> <li>Startup MacFusion, and create an entry for <code>jhpce-transfer01.jhsph.edu</code> \u2014 enter your login and password.</li> <li>Select a mount point, e.g. <code>~/jhpce/myhome/</code></li> <li>Once the drive is mounted, you can cd to the directory in the shell or view it  in a window on your desktop. To do this you need to \u201cReveal\u201d the drive by pressing Command-R.  Once the directory is revealed, you can drag and drop files into the director in the usual way you drag and drop files into any directory on your mac.</li> </ul>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#ftp","title":"ftp","text":"<p>We don\u2019t have the ftp client installed on the cluster.  It is an older, less secure, unencrypted channel for transferring files. However if you are downloading files from an older site that does not support SFTP or one of the other more modern mechanisms, you have a couple of options for ftp.</p> <p>If you want to be able to interactively browse through the ftp site you can use the lynx text based browser command:</p> <pre><code>lynx ftp://USER@ftp.site.gov\n</code></pre> <p>Once connected, you can then use the arrow keys to move around the site, and to select a file to download or a directory to descend into.</p> <p>If you know the exact path to the file you want, you can use the \u201cwget\u201d command:</p> <pre><code>wget ftp://USER:PASSWORD@ftp.site.gov/path/to/file\n</code></pre> <p>All of these should be done from the <code>transfer</code> queue to make use of our high speed ScienceDMZ network connection.</p>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/file-transfer/#links","title":"Links","text":"<ul> <li>Setting up MobaXterm for File Transfers with the JHPCE Cluster</li> <li>Using rclone to Access OneDrive</li> </ul>","tags":["topic-overview","needs-major-revision","refers-to-old-website"]},{"location":"access/globus/","title":"Using Globus to transfer files","text":"<p>Globus is a \u201cdropbox-like\u201d service to enable data sharing between academic and research communities. By using Globus, you can easily transfer files between Globus endpoints, share data with collaborators, and easily transfer data between the JHPCE cluster and your local desktop or laptop.</p>"},{"location":"access/globus/#setting-up-an-account","title":"Setting up an account","text":"<p>The first step in using Globus is setting up and account on the Globus site.\u00a0 To start, go to https://www.globus.org/ and clicking on the \u201cLog in\u201d button in the upper right. You should now see the screen below. Select \u201cJohns Hopkins\u201d from the list of Organizations.</p> <p></p> <p>You will now be directed to the JHU Login Screen. Enter your JHED ID and Password:</p> <p>Once you enter your JHED information you will be sent to the main Globus window:</p> <p></p>"},{"location":"access/globus/#transferring-files","title":"Transferring Files","text":"<p>With Globus, you can transfer data between nodes (known as \u201cEndpoints\u201d) that are part of the Globus network. The endpoint for the JHPCE cluster is called <code>jhpce#globus01</code>. To connect to the JHPCE endpoints, enter <code>jhpce#globus01</code> in the \u201cCollections field, and select it from the list of results displayed.</p> <p></p> <p>Once you select the <code>jhpce#globus01</code> endpoint, you will be prompted to enter your JHPCE Login and Password:</p> <p></p> <p>And once you login you will be shown a flie list of your home directory on the JHCPE cluster.</p> <p></p> <p>In order to transfer data to and from your desktop, you will need to install the Globus Connect Personal package (https://www.globus.org/globus-connect-personal) on your desktop. To do this, click on the \u201cTwo-Panel\u201d icon at the top of your globus session, and then click on Install Globus Connect Personal.</p> <p></p> <p>On the next screen, follow the steps on the next page to download the appropriate software package for your system, generate a Globus Key, and create a Globu name for your desktop/laptop.\u00a0</p> <p>Once you install and start Globus Connect Personal, you will be able to easily transfer files between your desktop/laptop and the JHPCE cluster via the Globus web interface.</p>"},{"location":"access/globus/#sharing-data-with-others","title":"Sharing data with others","text":"<p>One of the benefits fo using Globus is that you can share data from the JHPCE cluster with outside collaborators. To do this navigate to the directory you wish to share, select it, and either Right-Click on it, or select \u201cShare\u201d from the menu on the right hand side of the screen. In this example, I\u2019m sharing the <code>$HOME/class-scripts/R-demo</code> directory from within\u00a0my home directory.</p> <p>Next, you will be prompted to provide a name for your share.\u00a0 In this example, I\u2019m calling it \u201cMyRDemo\u201d.\u00a0 Once you enter the name and Description, click \u201cCreate Share\u201d:</p> <p></p> <p>Next you will be shown the current access permissions, which should be just for your account to start with.\u00a0 To grant others permission to access your share, click on \u201cAdd Permissions \u2013 Share With\u201d</p> <p>You will now be able to select which users you wish to share your directory with.\u00a0 The person you are sharing with must have a Globus account, and will need to provide their Globus ID or email with you.\u00a0 Enter their Username or Email, click \u201cAdd\u201d, and then click \u201cAdd Permission\u201d.\u00a0 We strongly recommend that you only grant \u201cread\u201d permission. for your share.</p> <p>Alternatively. you can create an \u201canonymous share\u201d to share a directory with anyone on Globus. To do this select \u201call users\u201d. Again, we strongly recommend that you only grant \u201cread\u201d permission for your share.</p> <p></p> <p>Once your share is created, you can notify your collaborators that they can access your data by using the Share name you created (in this example it\u2019s \u201cMyRDemo\u201d), and can search for your share name.</p> <p></p>"},{"location":"access/mobaxterm/","title":"Mobaxterm Configuration","text":"<p>Mobaxterm is a Windows application that provides an ssh client, scp client and X11 server all in one program.\u00a0 It is a very convenient tool for accessing the JHPCE cluster and utilizing the many features of the cluster.\u00a0 There is some configuration that needs to be done though in order to effectively use Mobaxterm in the JHPCE environment.\u00a0 This FAQ will take you through the steps needed to configure Mobaxterm.\u00a0 Before your proceed you should have your Google Authenticator app available.</p> <p>The first thing you will need to do is download the MobaXterm program from their web site http://mobaxterm.mobatek.net/download-home-edition.html</p> <p>Be sure to use the \"Installer Edition\" instead of the \"Portable Edition\"</p> <p></p> <p>Once the program has been downloaded, install it as you would any other Windows program.</p> <p>Once the program is installed, start the MobaXterm program. You should see a screen like this:</p> <p></p> <p>From this screen, click on the \"Sessions\" icon </p> <p></p> <p>in the upper left corner.</p> <p>On the \"Session settings\" screen, click on \"SSH\"</p> <p></p> <p>Enter \"jhpce01.jhsph.edu\" as the \"Remote host\". Click on the \"Specify username\" checkbox, and enter your JHPCE username in the next field. Then click the \"OK\" button.</p> <p>When you click OK, you will initiate an SSH session to the JHPCE cluster. You will be prompted for your Google Authenticator \"Verification Code\", and then your password.</p> <p>Once you enter your password correctly, you will see a number of boxes pop up (usually 3) prompting for another Verification Code. Click \"Cancel on these boxes. You will then be prompted to save your password. In the lower left, check the box that says \"Do not ask this again\" and then click \"No\". (We will get rid of these annoying boxes in a couple of steps).</p> <p>mobaxterm5</p> <p>At this point you should be logged into the JHPCE cluster and sitting at a shell prompt.</p> <p>After you exit out of the JHPCE cluster, a \"jhpce01\" session will be saved as a \"Saved Sessions\".\u00a0 To login again, double-click on the jhpce01 \"Saved Session\", and you should then be prompted for \"Verification Code\" (which will come from Google Authenticator) and \"Password:\"</p>"},{"location":"access/mobaxterm/#optional-setting-up-ssh-keys-in-mobaxterm","title":"OPTIONAL -- Setting up SSH Keys in MobaXterm:","text":"<p>To make logging in more streamlined and avoid the pop-up windows when you login, you can create an SSH key pair in MobaXterm. Before starting you should login to the JHPCE cluster in MobaXterm using your Google Authenticator and Password. Once you are logged in:</p> <p>Click on \"Tools -&gt; MobaKeyGen\"</p> <p></p> <p>You should then see the \"MobaXterm SSH Key Generator\" Screen. Click on \"Generate\", and you will be prompted to move the mouse around to generate random data.</p> <p></p> <p>Move your mouse until the green bar fills up.</p> <p></p> <p>Once the green bar fills up, you should see a populated screen.</p> <p></p> <p>For security purposes, we strongly recommend you protect your key with a password.\u00a0 To do so, enter a password in the \"Key passphrase\" and \"Confirm passphrase\" boxes. Next, click \"Save private key\", and save the key to a know locationon your local laptop/desktop (such as you \"Documents directory).</p> <p>Now, at the top of the window you'll see the text version of your public key. Copy the contents of this output with your mouse, making sure to scroll all the way to the bottom of the text box.\u00a0 NOTE: to do Copy/Paste in MobaXterm, you should not use \\&lt;CTRL&gt;-C and \\&lt;CTRL&gt;-V. Instead, select the text you want to copy, then use the right mouse button to bring up the context menu, and select\u00a0 Copy or select Paste when you are pasting.</p> <p>Now, go back to the tab where your JHPCE ssh session is running. From your home directory, cd into the .ssh directory. In this directory, you will need to update the file <code>authorized_keys</code>. Edit this file with your text editor of choice (nano, vi, emacs) as shown below. If the file does not exist, you can also create the file with the command below.</p> <p></p> <p>Paste in the public key that you copied from your local session. Depending on your editor, the new key may only show up on one long line, or it may wrap to multiple lines. Save the \"authorized_keys\" file when you are done.</p> <p></p> <p>If you assigned a passphrase to your key (and you really should have) we need to make a couple of extra steps to allow the passphrase to be entered only once, instead of every time you start a new login session.</p> <ul> <li>First go to \"Settings-&gt;Configuration\" and go to the \"General\" tab and click on \"MobaXterm password management\"</li> <li>At the top of the window where it says \"Save sesison passwords\", you should click \"Ask\"</li> <li>Also be sure to check the \"Save SSH keys passphrase as well\" box</li> <li>Then click \"OK\"</li> </ul> <p></p> <ul> <li>Next on the \"Configuration Window\" go to the \"SSH\" tab, and at the    bottom of the screen check the \"Use internal SSH agent \"MobAgent\"</li> <li>Just below this checkbox, click the \"+\" sign on the right side of    the \"Load Following Keys\" screen, and navigate to your \"Private    Key\", and select it.</li> </ul> <p></p> <p>Then click OK. You will be prompted to restart Mobaxterm. Go ahead and restart it. When you MobaXterm restarts, you will be prompted to enter your passphrase for your private key.</p> <p>The final step will be to add your key to the JHPCE session in the MobaXerm application. On the left pane of MobaXterm, you should see a list of \"Saved sessions\", including a session for the \"jhpce01\" login node. Right-Click on the \"jhpce01\" session, and select \"Edit Session\". This will open a window that looks like:</p> <p></p> <ul> <li>Select the \"Use private key\" checkbox.</li> <li>The field next to the checkbox should populate with a path to your local private key. If it does not, or it is not the correct path, then click the blue icon on the right side of the field, and navigate to the location of your \"private key\" file.</li> </ul> <p></p> <ul> <li>Click OK to save your changes.</li> <li>Now, in the left pane of Saved Sessions, you should be able to double click on the \"jhpce01\" session, and a new tab should open up, and log you into the JHPCE cluster without having to enter a password or Google Authenticator PIN.</li> <li>Once you have verified that you can login, exit out of all of your SSH sessions, and close the MobaXterm app.\u00a0 Reopen the MobaXterm application, double click on the \"jhpce01\" session.\u00a0 As before, a new tab should open up, and log you into the JHPCE cluster without having to enter a password or Google Authenticator PIN. </li> </ul>"},{"location":"access/onedrive/","title":"Using rclone to access OneDrive","text":"<p>Below is an example of using rclone to access the OneDrive network resource on the JHPCE cluster.  The initial setup is a bit involved, but regular operation is fairly straightforward.</p>","tags":["needs-review"]},{"location":"access/onedrive/#one-time-configuration","title":"One-time Configuration","text":"<p>Before you start, you will need to have an X11 graphical environment set up either by using MobaXterm on a Windows system or Xquartz on a Mac.  To start, login to the cluster as normal, and then srun into a compute node with a 10G RAM request (srun \u2013pty \u2013x11 \u2013mem=10G bash ) .  Part of the rclone setup process will involve using a web browser to generate an authentication key, so once you\u2019ve logged into a compute node, run \u201cchromium-browser &amp;\u201c.  The ampersand at the end will cause Firefox to run run in the background.  You may see a steam of warning message about \u201clibGL errors\u201d, but these are because we are using X11 forwarding and not a local graphics card, and assuming the browser stars up acter a few seconds, those messages can be ignored.</p> <p></p> <p>Now, from your srun session, you will need to run \u201cmodule load go\u201d to load the \u201cgo\u201d module, and then run \u201crclone config\u201d to begin the rclone setup.</p> <p></p> <p>When prompted to \u201cmake a new remote\u201d, enter \u201cn\u201d for \u201cnew remote\u201d. When prompted for a name, enter something descriptive, like \u201cOneDrive\u201d. Next, you will be presented with a long list of storage types.</p> <p></p> <p>. . .</p> <p></p> <p>. . .</p> <p></p> <p>When prompted for the type of storage to use, enter \u201conedrive\u201d. When prompted for \u201cOauth Client ID\u201d, just hit enter. When prompted for \u201cOauth Client Secret\u201d, just hit enter. When prompted to \u201cEdit advanced config\u201d, just hit enter to use the default \u201cNo\u201d answer. When prompted to \u201cUse auto config?\u201d, enter \u201cy\u201d. At this point you\u2019ll see a URL with \u201chttp://127.0.0.1\u201d in the address, and get a message \u201cWaiting for code\u2026\u201d</p> <p></p> <p>Also at this point, the Firefox browser should open a new tab, and start going to the http://127.0.0.1 address, which should redirect you to the Microsoft login page.</p> <p></p> <p>From here, you should enter \u201cJHEDID@jh.edu\u201d. Where of course you specify your own JHEDID. You will then be sent to the familiar \u201cJohns Hopkins\u201d JHED Login screen, where you should enter your JHED password.</p> <p></p> <p>When prompted to \u201cSave your login\u201d, you should select \u201cDon\u2019t Save\u201d. You should then see in the Firefox display, a \u201cSuccess!\u201d message, and in your \u201csrun\u201d session, you should see the message \u201cGot code\u201d, and then a selection of OneDrive site options. You should select option \u201c1\u201d for \u201conedrive\u201d.</p> <p></p> <p>Next, you should see a message where you can select which drive to use. There should only be one drive, so select \u201c0\u201d. You will also get a confirmation message, and you should select \u201cy\u201d.</p> <p></p> <p>At this point a summary of the configuration will be displayed, and you should select \u201cy\u201d to accept the configuration. Finally, you can enter \u201cq\u201d to quit the config process, and you can also close the Firefox browser.</p> <p></p> <p>At this point your OneDrive connection has been configured, and you can start to access your OneDrive.</p>","tags":["needs-review"]},{"location":"access/onedrive/#regular-operation","title":"Regular Operation","text":"<p>To access your OneDrive, you\u2019ll use the \u201crclone\u201d command with various options. The most often used commands are \u201crclone lsd\u201d to list directories, \u201crclone ls\u201d to recursively list files (this can take a long time if you have a lot of files in OneDrive and you are listing the top lecel directory), and \u201crclone copy\u201d to copy data between the cluster and your OneDrive.</p> <p>An example of \u201crclone lsd\u201d is below. There are a couple of key items to note. First, the name of the argument following \u201clsd\u201d should be the same name your used for your OneDrive config. You can run \u201crclone listremotes\u201d to see the name you used. The second item to note is that the name of your remote must end in a colon.</p> <p><pre><code>[compute-113 /users/bob]$ rclone lsd OneDrive:\n          -1 2019-12-21 01:54:32         2 BoxMigration\n          -1 2014-11-05 17:23:32        25 Documents\n          -1 2014-11-05 17:22:55       173 HomeDir\n          -1 2021-01-19 13:12:36         2 JHPCE-Billing-Videos\n          -1 2015-12-03 11:37:36         1 Lustre\n          -1 2019-07-11 17:25:16        10 OLDVMs\n          -1 2014-11-05 15:15:11         1 Shared with Everyone\n          -1 2019-07-11 12:13:21         0 USB\n</code></pre> To see the files in a particular directory, you would use \u201crclone ls\u201d and supply a directory name after the colon.</p> <pre><code>[compute-113 /users/bob]$ rclone ls OneDrive:Documents\n    84842 FY2014Q3 JHPCE Charges.xlsx\n    89229 FY2014Q4 JHPCE Charges.xlsx\n    57580 Globus-compute-022.rtf\n    43103 HPSCC Expenses.xlsx\n   684491 JHPCE-Overview-2014.pdf\n  1061265 JHPCE-Overview-2014.pptx\n    71168 JHU-Intel-Test-Cluster-Access.xls\n. . .\n   410012 pg10031.txt\n  4454050 pg31100.txt\n  1418582 pg6400.txt\n      426 plot1.r\n       20 plot1.sh\n       83 test1.sh\n[compute-113 /users/bob]$ \n</code></pre> <p>Finally \u201crclone copy\u201d can be used to transfer files between your OneDrive and the JHPCE cluster.</p> <pre><code>[compute-113 /users/bob]$ ls pg6400.txt\nls: cannot access pg6400.txt: No such file or directory\n[compute-113 /users/bob]$ rclone copy OneDrive:Documents/pg6400.txt .\n[compute-113 /users/bob]$ ls -l pg6400.txt\n-rw-r--r-- 1 bob mmi 1418582 Nov  5  2014 pg6400.txt\n[compute-113 /users/bob]$ touch zzz-test.txt\n[compute-113 /users/bob]$ rclone copy zzz-test.txt OneDrive:Documents\n[compute-113 /users/bob]$ rclone ls OneDrive:Documents | tail\n  2806147 ge_presentation.pdf\n   320053 pdf.tgz\n  5589891 pg100.txt\n   410012 pg10031.txt\n  4454050 pg31100.txt\n  1418582 pg6400.txt\n      426 plot1.r\n       20 plot1.sh\n       83 test1.sh\n        0 zzz-test.txt\n</code></pre> <p>This should give you a good start on using \u201crclone\u201d to access your OneDrive. Please email \u201cbitsupport\u201d if you have any questions.</p>","tags":["needs-review"]},{"location":"access/ssh/","title":"SSH - Key Information","text":"","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#ssh-basics","title":"SSH Basics","text":"<p>Access to the JHPCE cluster requires the use of SSH.</p> <p>SSH stands for Secure SHell. SSH is actually a set of internet standard protocols. Programs implementing these protocols include both command line interface (CLI) tools and those with graphic user interfaces (GUI).  They all enable you to make secure, encrypted connections from one computer to the next.</p> <p>Depending on the kind of operating system your computer uses, you may or may not need to install SSH software.</p> <p>Apple Macs come with CLI SSH tools pre-installed. You use them by entering commands in the Terminal app.  You can install GUI apps from various vendors, but we will only discuss the CLI tools except for some file transfer GUI programs.</p> <p>On a Windows system you will need to install an ssh client.  We recommend the excellent GUI program MobaXterm. Here is a document describing how to use it. There are other programs, such as the PuTTY family of tools and WinSCP.</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#ssh-keys","title":"SSH Keys","text":"<p>SSH programs make use of something called public-key cryptography. Basically secure communications can be created by splitting a secret into to parts, and placing one part on each end of a link.</p> <p>This can be extended to an optional pair of files you can generate and distribute such that one is located on the JHPCE cluster and the other is on your computer. Or smartphone!</p> <p>This key pair of files is generated once, and protected with a passphrase. You place a copy of the \"public\" key file on JHPCE in a particular place with specific permissions. You keep a copy of the \"private\" key file on your personal device(s). Once you prove to a program (ssh-agent) running on your device that you know the passphrase to your private key file, it will thereafter provide your private key when you run an SSH command.</p> <p>Once configured properly, you can use SSH keys instead of your JHPCE password.</p> <p>Example document</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#just-yanked-in-here-from-our-knowledgebase-document","title":"Just yanked in here from our Knowledgebase document","text":"","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#login-howto","title":"Login howto","text":"<ul> <li>When ssh-ing into the cluster you will be prompted for 2 pieces of information. </li> <li>First you will be prompted for \u201cVerification Code:\u201d, and for this you will enter the 6 digit number from your authenticator app (we recommend Google authenticator).</li> <li>Next you will be prompted for \u201cPassword:\u201d; this you this will use a traditional password that only you know.  Note that when entering your Verification Code and Password, your cursor will not move when you type, so it will appear like nothing is happening.  + If you have never accessed the cluster before, and \u201cInitial Verification Code\u201d and \u201cInitial Password\u201d will be provided to you during your JHPCE Cluster Orientation session.</li> <li>If you only get prompted for \u201cPassword\u201d and not \u201cVerification Code\u201d, you are likely using the wrong login ID.  Your login ID is provided to you during the JHPCE orientation; it is not the same as your JHED ID.</li> <li>If you have entered your credentials correctly, you will see a login banner for the JHPCE cluster, and you will be sitting at a shell prompt on the login node.  However, if you are prompted for \u201cVerification Code:\u201d again, you likely mistyped either your password or Verification Code, and you should wait until your Google Authenticator code changes (it changes every 30 seconds) and you will need to try logging in again.</li> <li>If you have tried several times to login, and you try to ssh again and get a \u201cConnection Refused\u201d message, you will need to wait 30 minutes before trying to login again.</li> <li> <p>If you continue to have login issues, please contact bitsupport@lists.jhu.edu.</p> </li> <li> <p>2 Factor Authentication</p> </li> </ul>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#unix-based-machines-linux-and-mac-osx","title":"Unix-based machines (linux and mac osx)","text":"<p>(((((((IN 2024 DO WE NEED TO SPECIFY A DIFFERENT (STRONGER) ENCRYPTION TYPE???))))))) + First, on your local laptop/desktop, open a terminal and <code>cd</code> into your home directory  and invoke the ssh key generator:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <ul> <li>You will be prompted for a passphrase.  For security reasons, we require that you use a passphrase to protect your key.  This prevents someone who gets access to your private key file from being able to use it!!!!! For the other questions, you can select the default values.</li> <li>The key generator will put two files in the .ssh subdirectory of your home directory, typically  <code>~/.ssh/id_rsa</code> and  <code>~/.ssh/id_rsa.pub</code>. </li> <li>You should only ever have to run the ssh key generator once on your local host.  If you have already configured passwordless login and you run the key generator a second time, it will overwrite your previous public and private key files. This will break all password-less logins that you set up with your previous keys.</li> <li>Next you want to copy your public key file to the remote host and append it to the authorized_keys file in the <code>.ssh</code> subdirectory of your home directory on the remote host. </li> <li>If there is no <code>~/.ssh</code> directory in the remote host, you will need to login to the remote host and create one. Note, attempting to connect to another host, like <code>ssh github.com</code>, will create one if it isn't there already. </li> <li>You can perform the copy and append operations in one line as follows;</li> </ul> <p><pre><code>cat ~/.ssh/id_rsa.pub | ssh &lt;your_userid&gt;@jhpce01.jhsph.edu 'cat &gt;&gt; ~/.ssh/authorized_keys'\n</code></pre> + Where you replace <code>&lt;your_userid&gt;</code> with your JHPCE userid and where you enter your JHPCE password when you are prompted for it by ssh. + To test that everything is working you should be able to log into the remote host from your local host with the following command<code>ssh &lt;your_userid&gt;@jhpce01.jhsph.edu</code>. + When you start ssh, you will be prompted for the passphrase that you used to protect your key.  To avoid having to enter your passphrase every time you use ssh, you can use the <code>ssh-agent</code> program.  To use <code>ssh-agent</code>, run</p> <p><pre><code>ssh-add\nssh-agent\n</code></pre> The ssh-agent will remain active for as long as your desktop or laptop is up and running.  If you reboot your desktop/laptop, you will need to rerun the ssh-add and ssh-agent commands.</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#loging-into-nodes","title":"Loging into nodes","text":"<p>Authoring Note</p> <p>(((This isn't true if you use <code>srun</code>. Is it even true in JHPCE 3.0 anyway Also, do we want to be suggesting that people log into compute nodes?)))</p> <p>Logging into a cluster node from a login node requires keypairs.   If your private key file is in <code>.ssh/</code> then it should work, since the public key file is in your <code>authorized_keys</code> file. If you do not want the same private key file to be used to log into nodes as the one used to log into the login node, then repeat create a new public private key / public key pair on the cluster.  Append the public key to your<code>authorized_key</code> file. Note, when appending, add the public key, do not overwrite the existing file.</p> <p>Many users set up an alias for the ssh command so they don\u2019t have to type as much to log into the remote host.  You can do this by adding the following line to your <code>~/.bashrc</code>, <code>alias hpc='ssh -X &lt;your_userid&gt;@jhpce01.jhsph.edu'</code>.</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#windows-machines","title":"Windows machines","text":"<p>If your desktop/laptop runs Microsoft Windows then you first need to install MobaXterm on your windows machine. If you are using MobaXterm, please use the steps at the bottom of our MobaXterm configuration page.</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#permissions-on-ssh-files","title":"Permissions on SSH Files","text":"<p>SSH is very strict about the permissions found on your files on the remote end of a connection. These files are found on JHPCE in your home directory inside the directory <code>.ssh</code>  Because this directory's name begins with a period, it is not listed when you use the <code>ls</code> program.</p> <p>The primary symptom of there being a file permissions problem is that ssh is still asking for a password when you think it should not.</p> <p>These rules are normally found to be broken on the remote side of a connection, but the permissions on your computer also matter.</p> <p>ALL OF THESE FILES NEED TO BE OWNED BY YOUR ACCOUNT.</p> <p>This table shows you two forms of the chmod command arguments needed to force permissions to be acceptable by SSH. The most convenient notation used by chmod is an octal (base-8) number. The most readable notation is a comma-separated combination of letters.</p> <p>These two commands are equivalent:</p> <ul> <li><code>chmod 700 $HOME/.ssh</code></li> <li><code>chmod u+rwx,g-rwx,o-rwx $HOME/.ssh</code></li> </ul> File/Directory Octal Human Readable Note $HOME 755 or tighter g-w,o-w Not writable by group or other $HOME/.ssh 700 u+rwx,g-rwx,o-rwx No access by group or other $HOME/.ssh/authorized_keys 600 u+rw,g-rwx,o-rwx Authorized keys file $HOME/.ssh/config 600 u+rw,g-rwx,o-rwx Config file $HOME/.ssh/id_* 600 u+rw,g-rwx,o-rwx Private key files $HOME/.ssh/id_*.pub 644 u+rw,g+r,g-wx,o+r,o-wx Public key files","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#ssh-and-x11","title":"SSH and X11","text":"<p>See our document on X11 for instructions on making ssh work to support X11 displays.</p>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/ssh/#mac-specific-configuration","title":"Mac-specific Configuration","text":"<p>!!! \"Under construction\"     Needs refinement. Some mention should be made in the x11 doc.</p> <p>In your ~/.ssh/config file you may find some of these options useful.</p> Key macOS settings<pre><code>UseKeychain yes\nXAuthLocation /opt/X11/bin/xauth\nAddKeysToAgent yes\nIdentityFile ~/.ssh/id_ecdsa\nIdentityFile ~/.ssh/id_rsa\nForwardAgent yes\n</code></pre> Keeping connections alive<pre><code># These values of 15 and 30 mean that my client ssh program will send a\n# message to the server every 15 seconds, and not decide that a remote\n# server is unresponsive until (15*30)=450 seconds\n\nServerAliveInterval 15\nServerAliveCountMax 30\n# IDK whether I included this for a good reason or not. Seems like I\n# would want the connection to die by lack of response to either method\n# Was it because I could not specify how long TCPKeepAlive waited?\nTCPKeepAlive no\n</code></pre> Per-host definitions for convenience<pre><code>Host jhpce01 j1 jhpce01.jhsph.edu\n    Hostname jhpce01.jhsph.edu\n    User your-cluster-username\n    ForwardX11 yes\n    IdentityFile ~/.ssh/id_ecdsa.jhpce\n</code></pre>","tags":["in-progress","refers-to-old-website","ssh"]},{"location":"access/x11/","title":"X11/The X Window System","text":"","tags":["in-progress","refers-to-old-website"]},{"location":"access/x11/#what-is-it","title":"What is it?","text":"<p>The X Window System (aka X11, aka X) is the primary GUI system for UNIX computers. X allows a program (an X \"client\") running on a UNIX computer in the cluster to be displayed on a remote computer (running an X \"server\"<sup>1</sup>) over the network. To use it, your local computer needs to have X server software installed on it.</p> <p>SSH provides support for tunnelling X11 over an encrypted connection. You may need to tell SSH that you want that service, by, for example, adding the -X flag to an ssh command in a macOS Terminal.</p> <p>X11 has been (slowly) being replaced by Wayland. Wayland has been the future for many years, but is becoming common. However this is usually invisible to users. Mentioned here in case you see references to Wayland where you expect to see X11.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"access/x11/#how-do-you-get-it","title":"How do you get it?","text":"<ul> <li>macOS users need to install XQuartz from xquartz.org.</li> <li>Windows users need to use a program like MobaXterm. We have a page describing installing and using MobaXterm.</li> <li>Linux laptop or desktop users are already using X as their windowing system.</li> </ul>","tags":["in-progress","refers-to-old-website"]},{"location":"access/x11/#example-x11-programs","title":"Example X11 Programs","text":"<p>xterm, SAS, RStudio, xclock, thunar.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"access/x11/#configuring-ssh-to-support-x11","title":"Configuring SSH To Support X11","text":"<p>Authoringnote</p> <p>The SSH document points to this one for ALL of the information needed to use X11. So this section needs to be complete. Pointing to the MobaXterm document of course, for Windows users.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"access/x11/#troubleshooting-x-connections","title":"Troubleshooting X Connections","text":"<p>Authoringnote</p> <p>This section should include pointers to FAQ items or vice versa. We shouldn't rewrite the same information twice!!!</p> <ul> <li>When -X ?</li> <li> <p>When -Y ?</p> </li> <li> <p>printenv DISPLAY or echo $DISPLAY</p> </li> <li> <p>[user@login31 ~]$ srun --pty --x11 bash srun: error: No DISPLAY variable set, cannot setup x11 forwarding.</p> </li> <li> <p>How do Windows users configure ForwardX11Timeout ?</p> </li> </ul> <p>Is <code>ForwardX11Timeout 0</code> better in 2024 than specifying a particular time in hours?</p> <p>https://jhpce.jhu.edu/question/my-x11-forwarding-stops-working-after-20-minutes/</p> <ol> <li> <p>Note that X reverses the normal conception of client/server operation, which is that the remote computer is the \"server\". In X11, the \"server\" is the program receiving the keyboard and mouse inputs and displaying the output of remote \"client\" programs.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","refers-to-old-website"]},{"location":"csub/csub-overview/","title":"C-SUB Overview","text":"<p>The CMS SUBcluster (C-SUB) makes use of some of the resources of an existing High Performance Computing cluster called JHPCE.</p>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#motivation-to-create-the-facility","title":"Motivation to create the facility","text":"<p>The Centers for Medicare and Medicaid Services (CMS) is part of the U.S. Department of Health and Human Services. It provides important data for researchers of patients, their conditions, and the American health care system.</p> <p>Acquiring and managing sensitive information from the Federal government is time-consuming, and requires on-going administrative and information technology support by groups with specific expertise.</p> <p>To facilitate research, an effort has been made to create an infrastructure that can provide those resources, as well as a computational facility to store and analyze the data.</p> <p>Researchers can leverage this existing infrastructure to more quickly and efficiently start and conduct their work.</p> <p>The Health Analytics Research Platform (HARP) was created to implement this vision. It is a collaboration of existing personnel across multiple organizations. Its leaders provide funding and guidance to an IT group which created a computing facility named the C-SUB.</p>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#health-policy-management-hpm-component-of-c-sub","title":"Health Policy &amp; Management (HPM) Component of C-SUB","text":"<p>The Health Analytics Research Platform (HARP) provides data services to HBHI- affiliated and HEADS Center-affiliated investigators to facilitate research collaborations advancing HBHI\u2019s strategic pillars.</p> <ul> <li>HPM</li> <li>HEADS/HARP Director: Dan Polsky </li> <li>HEADS/HARP Deputy Director: Matt Eisenberg </li> <li>CMS Data Expert: Frank Xu</li> </ul> <p>Please send C-SUB data-specific requests to support@harp-csub.freshdesk.com</p> <ul> <li>exporting files out of the C-SUB</li> <li>data inventory \u2013 current and desired additions or updates</li> </ul> <p>Health Analytics Research Platform (HARP)</p> <p>Hopkins Business of Health Initiative (HBHI)</p> <p>Hopkins Economics of Alzheimer Disease &amp; Services (HEADS)</p>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#differences-between-jhpce-c-sub-clusters","title":"Differences between JHPCE &amp; C-SUB Clusters","text":"<p>The CMS subcluster (C-SUB) makes use of some of the resources of the original JHPCE cluster. For example, the scientific software such as STATA and SAS.</p> <p>However, in many other ways it operates differently than the rest of the cluster in order to keep CMS data from escaping.</p> <p>Please keep that in mind when reading JHPCE documentation or asking for help via the bitsupport &amp; bithelp mailing lists. Mention that you are a C-SUB user.</p>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#commonalities","title":"Commonalities","text":"<ul> <li>SLURM job scheduler</li> <li>Almost all of the software</li> <li>use of modules</li> <li>except for programs which run out of containers like RStudio Server</li> </ul>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#differences","title":"Differences","text":"<ul> <li>Different user accounts</li> <li>Different SLURM server, partitions and nodes</li> <li>File transfer in and out is VERY different</li> <li>Home directory locations</li> <li>Common file sharing areas among DUA members (e.g. /users/55548/shared/)</li> </ul>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#getting-help","title":"Getting Help","text":"","tags":["in-progress","topic-overview"]},{"location":"csub/csub-overview/#-this-web-site-use-the-search-field","title":"- This web site - use the search field","text":"<ul> <li> <p>the orientation slides</p> </li> <li> <p>bitsupport@lists.johnshopkins.edu</p> </li> <li>System issues (accessing software, general questions)</li> <li> <p>Monitored by the systems administrators</p> </li> <li> <p>bithelp@lists.johnshopkins.edu</p> </li> <li>Complex application issues (R/SAS/python...)</li> <li> <p>Monitored by volunteer power users</p> </li> <li> <p>support@harp-csub.freshdesk.com</p> </li> <li> <p>Data layout questions, export &amp; update requests</p> </li> <li> <p>https://jhpce-app02.jhsph.edu (only visible from Hopkins networks)</p> </li> <li> <p>password resets, one-time password tokens</p> </li> <li> <p>Others in your lab</p> </li> <li>Web Search \u2013 Google your error message</li> </ul>","tags":["in-progress","topic-overview"]},{"location":"csub/csub-stub/","title":"stub page for the \"C-SUB\" topic","text":"<p>This is a stub page for the \"C-SUB\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"csub/csub-stub.md\"</p>"},{"location":"csub/new-csub-users/","title":"THIS DOCUMENT NEEDS TO BE REWRITTEN. DOESN'T YET CONTAIN MUCH C-SUB INFO.","text":"","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#registration","title":"Registration","text":"","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#for-new-users","title":"For New Users","text":"<p>If you are prospective user, please start by getting the approval of a PI who has provided us with a budget number. (If you and they are both new to JHPCE, please ask them to visit this page.)  Then request a user account by filling out this form.  Once we have received your form, the JHPCE management team will contact you to schedule your attendance at the next JHPCE Orientation.  All new users are required to attend a 1 1/2 hour orientation session. Orientations are generally scheduled every other week.</p>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#orientation-slides","title":"Orientation Slides","text":"<p>You can download the orientation slides at JHPCE-Overview</p>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#generally-expected-knowledge","title":"Generally expected knowledge","text":"<ul> <li>Anything I/O intensive should be done on the compute nodes rather than the jhpce01 login node.</li> <li>Anything more than a quick <code>ls</code> including: copying large files, recursively changing permissions, creating or extracting tar or zip archives, running a <code>find</code> should be done in a session on a compute node.</li> <li>Data transfers of files larger about 1GB should be done through <code>jhpce-transfer01.jhsph.edu</code> rather than a login node.</li> <li>Try to avoid having directories with more than 100 files in them. </li> <li>Try to avoid storing programs and scripts in data directories like <code>DCL*</code>.</li> <li>Most storage on the cluster is raided but not backed up. </li> <li>We do back up home directories and a few other select directories on the DCS and DCL systems for groups that have requested backups.  We do have additional backup storage capacity available for a small fee.</li> <li>Make use of your 1 TB of fastscratch storage for IO intensive job</li> <li>Please remember that DCS and DCL stand for \u201cDirt Cheap Storage\u201d and   \u201cDirt Cheap Lustre\u201d, and were designed with cost-effectiveness as a   primary driving factor over performance.</li> <li>Sharing data can be done in several ways on the cluster: i.) traditional Unix file permissions and groups and ii.) Access Control Lists (ACLs).</li> <li>Sharing files with external collaborators can be done via Globus.</li> </ul>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#new-c-sub-user-orientation","title":"New C-SUB user orientation","text":"","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#what-to-do-before-the-c-sub-jhpce-orientation-session","title":"What to do BEFORE the C-SUB JHPCE Orientation Session","text":"<p>There is a lot of material for us to cover and you to absorb. It is vital for your success that you complete a number of steps PRIOR to attending the\u00a0Orientation Session for the CMS Subcluster of the JHPCE (pronounced by its letters J-H-P-C-E) cluster.</p> <ul> <li>Download a copy of the slides from the Orientation     from:JHPCE-Overview-CMS.pdf.</li> <li>If you have never used a Linux or Unix system before, we strongly     recommend going through the Unix Command     Line     tutorial. The cluster is entirely Linux based. Our Orientation is     about using the cluster, not using Linux. This tutorial should     only take 30 minutes or so to go through.</li> <li> <p>In order to access the JHPCE cluster and make use of the     applications on the cluster, you may need to install additional     software on your smart phone and laptop.</p> </li> <li> <p>Install the 2 Factor Authentication program. \u00a0The JHPCE cluster     makes use of \"Google Authenticator\" to provide enhanced security.     \u00a0You can choose to either install an app on your smartphone or, if     you do not have an Apple or Android based smart phone, you can     install an extension to the Google Chrome browser.\u00a0 Prior to the     Orientation Session, you will only need to download the     GoogleAuthenticator app on your smart phone, or install the Authy     Chrome extension. We will be configuring Google Authenticator     during the Orientation Session. Please     see\u00a0https://jhpce.jhu.edu/knowledge-base/authentication/2-factor-authentication/#otp     for instructions.</p> </li> <li>Install required client software.\u00a0 You may need to install a     couple of programs on your laptop or desktop in order to access the     JHPCE Cluster.\u00a0 You will need 1) an SSH client for logging in, 2) an     SFTP client for transferring files to and from the cluster, and 3)     an X11 client for displaying graphics back from the JHPCE cluster.     \u00a0The SSH client is a requirement -- the SFTP and X11 clients are     preferable but optional.<ul> <li>Microsoft Windows We have found that the easiest program to use for accessing the     JHPCE cluster is MobaXterm as it combines the functionality of     all 3 software packages (SSH, SFTP, and X11) in 1 program.\u00a0     Install MobaXterm by following the first few steps of     https://jhpce.jhu.edu/knowledge-base/mobaxterm-configuration/     .\u00a0 Alternatively, if you already use an SSH client, (such as     putty     or Cygwin) and an SCP client\u00a0 (such as     WinSCP),     you can continue using that software.</li> <li>Apple Macintosh There are built in command line tools for ssh and scp that     can be run from a Terminal window. The Terminal program can be     found in \"Applications -&gt; Utilities\". From a Terminal window,     you would type:\\     <code>ssh &lt;username&gt;@jhpcecms01.jhsph.edu</code>and then login with     the login id and the password we provided to you.- In order to     run graphical programs on the cluster and have them displayed on     your Mac, you will need to install XQuartz from     http://xquartz.macosforge.org/landing/.- Optionally, you can     also install a GUI based SFTP program such as     \"Filezilla\". One note about     Filezilla -- if you download the package from the default link     on SourceForge, you may be be blocked by your MalWare/Virus     Scanner, or prompted to install Potentially Unwanted Programs     (PUPs) during installation.\u00a0 We recommend you follow the     alternative download link     here to     download a clean copy of the program.</li> </ul> </li> </ul>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"csub/new-csub-users/#best-practices-passwords-and-authentication","title":"Best practices: passwords and authentication","text":"<ul> <li>Do not share your password with ANYONE.</li> <li>Choose a \"strong\" password.</li> <li>Use the \"kpasswd\" command to choose a new password. It requires your new password to have three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</li> <li>It would be best if your password was unique and not the same     password you use on other systems.</li> <li>If you believe your password or your computer have been compromised     please reset your password using a different device. Visit     https://jhpce-app02.jhsph.edu/\u00a0 to reset your password.\u00a0 This web     site is only available on campus, so if you are outside of the     school network, you will need login to the JHU VPN first. You will     log into that page with your JHED ID and password.</li> <li>Hopkins staff will *NEVER* send you an email message asking for     your password or login credentials</li> <li>NEVER give out your password and login ID to anyone in an email     message or on a web page.</li> </ul>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"files/acl/","title":"ACL - Access Control Lists","text":"<p>Traditional Unix file and group permissions can be used to share access to files and directories.  However, there can be times when more fine-grained control of shared access is needed. To accomplish this, Access Control Lists (ACLs) can be used. They add to the normal permissions.</p> <p>The Wikipedia has a fairly good description of normal UNIX file and group permissions, including the symbolic and numeric notation schemes.</p>","tags":["in-progress"]},{"location":"files/acl/#caution","title":"Caution","text":"<p>JHPCE uses two kinds of file systems on its large storage servers: ZFS and Lustre.</p> <p>You need to use a different pair of ACL commands for each type.</p> <p>As of March 2024, the only Lustre file systems are those which begin with the path <code>/dcl02/</code></p> <p>As of March 2024, all of the files originally found on the Lustre file server named DCL01 have been copied off to live on other, ZFS-using file servers. But the name /dcl01 has been preserved, for convenience.</p>","tags":["in-progress"]},{"location":"files/acl/#overview","title":"Overview","text":"<p>Some common notes that are applicable to both types of ACL commands:</p> <ul> <li>ACLs can be used to give either read or read/write access to a file or directory. </li> <li>ACLs should use the security notion of \u201cleast privilege\u201d, meaning that ACLs should give only the needed access and nothing more. +  When setting up an ACL for a user access on a file or directory that is several layer deep in the directory structure, a <code>READ-EXECUTE</code> ACL will need to be set on all levels above the file or directory you are sharing. For example, if you are setting an ACL on the directory <code>/users/bob/mydata/project1/shared</code>, you would also need to set a <code>READ-EXECUTE</code> ACL on <code>/users/bob/mydata/project1</code>, <code>/users/bob/mydata</code>, and <code>/users/bob</code>.</li> <li>Default ACLs can be set on a directory, and this ACL will be inherited into the directory structure as new files and directories are create.</li> <li>However, existing files and directories that exist beneath a directory that an ACL is being set on will not be affected by a new ACL. If you need to propagate an ACL into an exiting directory tree, you will need to use the <code>recursive</code> option to the ACL command.</li> <li>With default ACLs, the individual user\u2019s <code>umask</code> setting is important in assuring that new files and directories that get created have the correct permissions set. The <code>umask</code> setting will take precedence over the ACL, so it must be more permissive than the ACL. For example, if you want to set a default group ACL where the group has write access, you need to make sure that your umask is set to <code>0002</code> rather than <code>0022</code>, as the \u201c2\u201d in the group umask bit will prevent the group write capability.</li> <li>ACL commands should be run on a compute node and not a login node. Long-running recursive ACL commands on large directory trees may also be done via a submitted batch job script.</li> </ul> <p>Example useage:</p> <pre><code>[alice@compute-123 ~]$ mkdir test1\n[alice@compute-123 ~]$ umask\n0022\n[alice@compute-123 ~]$ nfs4_setfacl -a A:gfdi:hpscc@cm.cluster:RWX test1\n[alice@compute-123 ~]$ touch test1/f1\n[alice@compute-123 ~]$ nfs4_getfacl test1/f1\n\n# file: test1/f1\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA:g:hpscc@cm.cluster:rtcy\nA::EVERYONE@:rtcy\n[alice@compute-123 ~]$ umask 0002\n[alice@compute-123 ~]$ touch test1/f2\n[alice@compute-123 ~]$ nfs4_getfacl test1/f2\n\n# file: test1/f2\nA::OWNER@:rwatTcCy\nA::GROUP@:rwatcy\nA:g:hpscc@cm.cluster:rwatcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#basic-commands","title":"Basic commands","text":"<ul> <li>There are 2 commands for dealing with ACLs on <code>/users</code>, <code>/dcs04</code>, <code>/dcs05</code>, <code>/dcs06</code>, and <code>/dcs07</code>. </li> <li>The <code>nfs4_getfacl</code> command will display current ACL setting for a file or directory</li> <li>The <code>nfs4_setfacl</code> command is used to modify ACLs.  </li> <li>With ACLs you can grant either read-only access, or read-write access on a directory or file to specific users.</li> <li>The simplest permissions to use in ACLs are R for read access, W for write access, and X for execute and directory access. For detailed description of fine-grained permissions that can be set, please see (https://www.osc.edu/book/export/html/4523).</li> <li>User ACLs By default, one\u2019s home directory is only accessible to the owner, and the ACL should reflect this. For example, for the user alice, the ACL on their home directory would look like:</li> </ul> <pre><code>[alice@compute-123 ~]$ pwd\n/users/alice\n[alice@compute-123 ~]$ nfs4_getfacl .\n# file: .\nA::OWNER@:rwaDxtTcCy\nA::GROUP@:tcy\nA::EVERYONE@:tcy\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#read-permisions","title":"Read permisions","text":"<ul> <li>Now, if alice wanted to grant read-only access to their home   directory to the user bob, they would use the <code>nfs4_setfacl</code>   command:</li> </ul> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RX .\n[alice@compute-123 ~]$ getfacl .\n# file: .\nA::OWNER@:rwaDxtTcCy\nA::bob@cm.cluster:xtcy\nA::GROUP@:tcy\nA::EVERYONE@:tcy\nAt this point, bob would be able to access alice\u2019s home directory. Now suppose there is a file that alice wants to let bob update. Alice could use ACLs to grant write access to a particular file:\n\n[alice@compute-123 ~]$ ls -l shared-data.txt\n-rw-r--r-- 1 alice users 79691776 Feb  2 07:06 shared-data.txt\n[alice@compute-123 ~]$ nfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RWX shared-data.txt \n[alice@compute-123 ~]$ nfs4_getfacl shared-data.txt\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#write-permissions","title":"Write permissions","text":"<ul> <li>Now suppose alice wanted to create a directory that bob could write to. </li> <li>Alice could create a directory, and grant bob write access to it:</li> </ul> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\nNow the user bob can copy or save files in the \u201cshared\u201d directory.\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#inherited-defaults","title":"Inherited Defaults","text":"<p>To set an inherited \u201cdefault\u201d ACL that will allow bob access on all new files and directories that get saved into <code>/users/alice/shared</code>, you would need to first give bob the normal ACL permissions, then add a second set of ACLs that will be inherited using the <code>fdi</code> option to the <code>nfs4_setacl</code> command.</p> <p>One issue we\u2019ve seen is that a <code>@USER</code> and <code>@GROUP</code> ACL need to be explicitly set for permissions to be properly set:</p> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:bob@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:OWNER@:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:GROUP@:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\nA:fdi:OWNER@:rwaDxtTcCy\nA:fdi:bob@cm.cluster:rwaDxtcy\nA:fdi:GROUP@:rwaDxtcy\nA:fdi:EVERYONE@:tcy\n</code></pre> <p>To set a Group ACL, you need to add the <code>g</code> option to the <code>nfs4_getfacl</code> command.</p> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:g:hpscc@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA:g:hpscc@cm.cluster:rwaDxtcy\nA::EVERYONE@:rtcy\nTo set a Group Inherited ACL, you need to add the \u201cgfdi\u201d option to the \u201cnfs4_getfacl\u201d command. With this inherited group ACL set, all new files and directories will inherit the group settings.\n\n[alice@compute-123 ~]$ nfs4_setfacl -a A:gfdi:swdev@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:OWNER@:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:GROUP@:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\n\n# file: shared\nA::OWNER@:rwaDxtTcCy\nA::GROUP@:rwaDxtcy\nA:g:hpscc@cm.cluster:rwaDxtcy\nA::EVERYONE@:rxtcy\nA:fdi:OWNER@:rwaDxtTcCy\nA:fdi:GROUP@:rwaDxtcy\nA:fdig:swdev@cm.cluster:rwaDxtcy\nA:fdi:EVERYONE@:tcy\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#removing-an-acl","title":"Removing an ACL","text":"<p>If you want to remove and ACL, you can use the <code>-x</code> option to <code>nfs4_setfacl</code>. Please note that you need to use the full ACL, and not the <code>RWX</code> shortcuts.</p> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -x A::bob@cm.cluster:rwaxtcy shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#directories-in-lustre-file-systems","title":"Directories in Lustre file systems","text":"<p>There are two commands for dealing with ACLs on the JHPCE cluster for directories in Lustre file systems, which start with <code>/dcl</code>.  The <code>getfacl</code> command will display current ACL setting for a file or directory, and the <code>setfacl</code> command is used to modify ACLs.  With ACLs you can grant either read-only access, or read-write access on a directory or file to specific users.</p> <p>Let\u2019s say there is a directory <code>/dcl01/project/data/alice</code> that alice owns. The <code>getfacl</code> command could be used to see the current ACL set on the directory:</p> <pre><code>[alice@compute-123 ]$ pwd\n/dcl01/project/data/alice\n[alice@compute-123 ]$ getfacl .\n# file: .\n# owner: alice\n# group: users\nuser::rwx\ngroup::---\nother::---\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#read","title":"Read","text":"<p>Now, if alice wanted to grant read-only access to <code>/dcl01/project/data/alice</code> to the user bob, they would use the <code>setfacl</code> command:</p> <pre><code>[alice@compute-123 ]$ setfacl -m user:bob:rx .\n[alice@compute-123 ]$ getfacl .\n# file: .\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#write","title":"Write","text":"<p>Now suppose there is a file that alice wants to let bob update. Alice could use ACLs to grant write access to a particular file:</p> <pre><code>[alice@compute-123 ]$ ls -l shared-data.txt\n-rw-r--r-- 1 alice users 79691776 Feb  2 07:06 shared-data.txt\n[alice@compute-123 ]$ getfacl shared-data.txt\n# file: shared-data.txt\n# owner: alice\n# group: users\nuser::rw-\ngroup::r--\nother::r--\n[alice@compute-123 ]$ setfacl -m user:bob:rw shared-data.txt \n[alice@compute-123 ]$ getfacl shared-data.txt\n# file: shared-data.txt\n# owner: alice\n# group: users\nuser::rw-\nuser:bob:rw-\ngroup::r--\nmask::rwx\nother::r--\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#creating-a-common-dir","title":"Creating a common dir","text":"<p>Now suppose alice wanted to create a directory that bob could write to. Alice could create a directory, and grant bob write access to it:</p> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ]$ setfacl -m user:bob:rwx shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:rwx\ngroup::r-x\nmask::rwx\nother::r-x\nNow the user bob can copy or save files in the \u201cshared\u201d directory.\n</code></pre>","tags":["in-progress"]},{"location":"files/acl/#default-acls","title":"Default ACLs","text":"<p>To set an inherited default ACL that will allow bob access on all new files and directories that get saved into <code>shared</code>, you would need to use the <code>-d</code> option to the <code>setacl</code> command:</p> <pre><code>[[alice@compute-123 ~]$ setfacl -d -m user:bob:rwx shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:rwx\ngroup::r-x\nmask::rwx\nother::r-x\ndefault:user::rwx\ndefault:user:bob:rwx\ndefault:group::--x\ndefault:mask::rwx\ndefault:other::r-x\nIf you want to remove and ACL, you can use the \u201c-x\u201d option to setfacl.\n\n[alice@compute-123 ]$ setfacl -x user:bob shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\ngroup::r-x\nmask::r-x\nother::r-x\n</code></pre>","tags":["in-progress"]},{"location":"files/archive-files/","title":"Archive Files","text":"<p>JHPCE users store petabytes of data on our storage servers in millions of files. When </p>","tags":["needs-to-be-written","jeffrey"]},{"location":"files/archive-files/#archive-commands","title":"Archive commands","text":"<p>cpio tar zip bzip2</p>","tags":["needs-to-be-written","jeffrey"]},{"location":"files/archive-files/#compression","title":"Compression","text":"<p>Most archive commands have optional arguments enabling the compression or decompression of files being added to or extracted from archives.</p> <p>Some data compresses more than others. Plain text files compress very well. Binary files holding data read out of lab instruments or images compress less well or are already compressed as a result of the file format used.</p> <p>JHPCE uses ZFS file systems with compression enabled for home directories and data stores like /dcs05.</p> <p>Therefore you do not need to spend time or CPU cycles compressing files or archives already stored in the cluster.  That may be a worthwhile activity for reducing the size of files or archives before importing them into the cluster. Usually that is the case when the path from the data source to JHPCE contains links with limited bandwidth.</p>","tags":["needs-to-be-written","jeffrey"]},{"location":"files/archive-files/#clean-up-before-creating-an-archive","title":"Clean up before creating an archive.","text":"<p>Command to delete unwanted files matching patterns</p> <p>touch -t 201909111200 911-noon; find . -newer 911-noon -print</p> <p>find . ( ( -type d -name Library -prune ) -o ( -type d -name .dropbox -prune ) ) -o -newer 911 -depth 4 -print # ignore directories named Library and .dropbox, limit how deep to search</p> <p>find ~/Library -name com.microsoft.rdc* -print0 | xargs -0 ls -ltd # -print0 and -0 for xargs allows searches to deal with PATH NAMES CONTAINING SPACES</p> <p>find /usr/bin/ -exec file {} \\;|grep Perl # find Perl scripts</p>","tags":["needs-to-be-written","jeffrey"]},{"location":"files/copying-files/","title":"Copying Files Within The Cluster","text":"<p>JHPCE users store petabytes of data on our storage servers. Some of it accumulates as work is done. Much is imported from outside the cluster as source material for research. Moving around large numbers of files can be done with a variety of methods.  Some of them give you more confidence in the results than others. Some are misleadingly quiet despite failing or producing a result that isn't the same as the source location.</p> <p>The <code>cp</code> command is a good example of a familiar tool that can produce unexpected results. It supports recursion through the <code>-R</code> flag. Depending on the UNIX version, <code>cp</code> may or may not treat symbolic links or hard-linked files or sparse files or device files the way you expect.</p> <p>All tools have limitations, most of which we, thankfully, don't encounter in day-to-day use. However, when manipulating large numbers of files and files in directory trees many levels deep, these issues can begin to surface. For example, consider what happens if a program has a limitation where it cannot handle file paths longer than 1024 characters. You would be unlikely to experience a problem using that program to copy files around unless they are <code>/stored/in/directory/trees/with/many/entries/YYYY-MM-DD-MM-SS/long-filename-with-details-embedded-in-its-name.dat</code>  Such paths are common in the sciences and become more common as time passes and data accumulates.</p> <p>When transferring files into and out of the cluster, please use the transfer node. The various tools for that work are described in this document.</p> <p>Please use compute nodes when copying more than a few files inside the cluster.</p>","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#archive-tools-moving-data-through-a-pipe","title":"Archive tools moving data through a pipe","text":"<p>Rsync is usually the best method. But rsync wasn't always available in the past, or it didn't support copying one or another attribute that more basic tools did. Kinds of file permissions, data forks, etc.</p> <p>One method that can be quick and effective is to use an archive program like tar to create an archive file which is passed through an input/output pipe to the same program running in another directory that extracts files into that location. This technique can use available system memory as buffer space, leading to smoother flows of data as disk reads and writes occur with optimal amounts of bytes.</p> <p>This example copies the named directory some-directory from the current working directory to another location. Because tar by default works with blocks of data 512 bytes in size, a higher efficiency is achieved by telling it to create larger blocks to reduce the overhead of doing input/output requests in such small sizes.</p> <p><pre><code>tar cbf 20480 - some-directory | (cd /destination/place; tar -xbf 20480 -)\n</code></pre> This example extends the technique to copying the data off of your current host to another host: <pre><code>tar cbf 20480 - some-directory | ssh myaccount@another-host (cd /destination/place; tar -xbf 20480 -)\"\n</code></pre></p>","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#rsync","title":"Rsync","text":"<p>Rsync is a powerful command for copying large numbers of files between a SOURCE (aka SRC) and a DESTINATION (aka DEST), within a single computer or between computers. </p> <p>Note</p> <p>A key benefit of <code>rsync</code> is that it will copy only the material needed to make DEST match SRC. Most other programs, such as <code>cp</code> or <code>scp</code>, will always copy over all of SRC to DEST.</p> <p>There are MANY arguments which control how the copying will occur. So you can do things like specify files to exclude, create a log of the actions taken, and provide statistics when done.</p> <p>The most common flag used with <code>rsync</code> is <code>-a</code> for \"archive\"</p>","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#source-and-destination","title":"SOURCE and DESTINATION","text":"<p>Rsync is very flexible.</p> <p>SRC and DEST can be paths or hostnames with colons and paths or a combination. rsync will use ssh to send files between hosts, so you can even specify a different username for one of the SRC and DEST.</p> <p>Multiple items can be listed as SRC material. Of course there can only be one DESTINATION</p> Local copying: <code>rsync [OPTION...] SRC... DEST</code> Access via remote shell: Pull:              <code>rsync [OPTION...] [USER@]HOST:SRC... [DEST]</code> Push:              <code>rsync [OPTION...] SRC... [USER@]HOST:DEST</code>","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#mind-the-trailing-slash","title":"Mind the trailing slash","text":"<p>rsync ultimately needs you to specify a SOURCE location and a DESTINATION location.</p> <p>If the SOURCE represents a directory then adding a forward slash to it will cause the contents of the directory to be copied into DESTINATION. If there is no trailing forward slash, then the SOURCE  directory will be copied into DESTINATION.</p> Example If directory /some/source/place contains files a, b, and c <code>rsync -a /some/source/place/  /a/destination/location/</code> will result in the contents of /a/destination/location/ being a, b, and c. If the command was instead  <code>rsync -a /some/source/place  /a/destination/location/</code> will result in the contents being instead /a/destination/location/place","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#rsync-examples","title":"Rsync Examples","text":"","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#rsync-flags-you-may-want-to-use","title":"Rsync Flags You May Want To Use","text":"<p>Most flags have two forms you can choose between, a short one consisting of a single character, or a readable form preceeded by two hyphens, and typically followed by an equals sign and a value. <pre><code>-H preserve hard links\n-x don't cross filesystem boundaries\n--archive, -a            archive mode; equals -rlptgoD (no -H,-A,-X)\n--dry-run, -n            perform a trial run with no changes made\n--atimes -U preserve access (use) times\n--crtimes, -N preserve create times (newness)\n--times, -t preserve modification times\n--acls, -A preserve ACLs (implies --perms)\n--xattrs, -X preserve extended attributes\n--numeric-ids don't map uid/gid values by user/group name\n--human-readable, -h     output numbers in a human-readable format\n--ignore-existing  (don't overwrite existing)\n--delete-after  (wait until end to process deletes)\n--max-delete=NUM         don't delete more than NUM files (FOR SAFETY) \n--itemize-changes, -i    output a change-summary for all updates \n--itemize-changes has complex output. see man page string \"has a cryptic output\" which I've saved as shell script rsync-itemize\n--list-only              list the files instead of copying them\n--log-file=FILE\n--size-only              skip files that match in size \n--sparse                 turn sequences of nulls into sparse blocks\n--ignore-times, -I       don't skip files that match size and time\n--info=FLAGS             fine-grained informational verbosity\n--chmod=CHMOD            affect file and/or directory permissions\n--usermap=STRING         custom username mapping \n--groupmap=STRING        custom groupname mapping (STRING is not simply\n--chown=USER:GROUP       simple username/groupname mapping \n--exclude-from={'list.txt'}\n--exclude={'*.txt','dir3','dir4'}\n--checksum -c           compare checksums\n</code></pre></p>","tags":["in-progress","jeffrey"]},{"location":"files/copying-files/#example-batch-job","title":"Example batch job","text":"<p>Is shown here.</p>","tags":["in-progress","jeffrey"]},{"location":"files/data-security/","title":"Data Security","text":"<p>This is a stub page for the \"Managing Files\" topic.</p> <p>Authoring Note</p> <p>What should users keep in mind about providing the appropriate amount of protection to their data files? (Also, should we have a document that points to outside guides to data management lifecycles? Case Western Reserve has good documentation.)</p>","tags":["needs-to-be-written"]},{"location":"files/files-overview/","title":"MANAGING FILES","text":"<p>We have previously addressed these issues as if they were independent. I believe it would help users increase their efficiency if they were seen as a set of related issues under this topic: managing files.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#overview","title":"Overview","text":"<p>General info about where files of different types live (should live). Pointers to existing info over in the Storage topic. Reminder about what is backed up and what is not.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#sharing-files-with-other-users","title":"Sharing files with other users","text":"","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#normal-unix-file-ownership-and-permissions","title":"Normal UNIX file ownership and permissions","text":"<p>Brief overview of how things work and how to inspect.</p> <p>UNIX groups.</p> <p>If nothing else, a pointer to other web pages (ours or on the Internet).</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#home-directory","title":"Home Directory","text":"<p>What are the default settings and why are they important?</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#project-storage","title":"Project storage","text":"<p>What are the default settings and why are they important?</p> <p>What are common variations?</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#using-acls-to-allow-increased-access","title":"Using ACL's to allow increased access","text":"<p>Existing document goes here, although that needs revision. Among other things, to be more clear about how commands to provide inherited are needed in addition to the initial commands providing the desired access.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#transferring-files-into-and-out-of-the-cluster","title":"Transferring files into and out of the cluster","text":"<p>Our existing information gets folded in here.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#accessing-data-from-compute-nodes","title":"Accessing data from compute nodes","text":"<p>Should you stage your data to fastscratch?</p> <p>Should you stage the data locally? Space considerations. (DO NOT FILL UP /TMP). Mention sbcast?</p> <p>There's probably other things we could usefully say about this subject.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#copying-data-around-within-cluster","title":"Copying Data Around Within Cluster","text":"<p>Alternatives to copying (sharing via ACL, using symbolic links).</p> <p>Do it on compute nodes.</p> <p>Example batch jobs for doing so.</p> <p>Rsync argument recommendations.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#best-practices","title":"Best Practices","text":"<p>Should users bother with compressing? No, because of underlying ZFS compression.</p> <p>But archiving files can be a good idea. Speeds up directory access (less metadata in directories). Simplifies transferring. Mention checksumming with md5sum.</p> <p>Check beforehand that there will be sufficient space in destination before trying to import data or copy it around.</p> <p>Suggestions about backing up your critical files, such as using versioning systems.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-overview/#data-protection-policies","title":"Data Protection Policies","text":"<p>What do users need to know about cluster and university policies to protect certain classes of information? Within their own files and with respect to files copied out of databases like marketscan.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"files/files-stub/","title":"stub page for the \"Managing Files\" topic","text":"<p>This is a stub page for the \"Managing Files\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"files/files-stub.md\"</p>"},{"location":"files/sharing-files/","title":"Sharing Files","text":"<p>How do you safely collaborate with others in a UNIX environment in an on-going basis?</p> <p>There are different locations where you can store files for the long term. The correct location to store files depends on several factors, including their size, how private they are, whether other users should be able to read or write them, and whether they should remain for your group after you have gone to another organization.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#unix-ownership-permissions","title":"UNIX Ownership &amp; Permissions","text":"<p>You must understand the basics of file ownership and permissions to successfully share files. Here are some tutorials to read:</p> <ul> <li>https://docs.nersc.gov/filesystems/unix-file-permissions/</li> <li>https://www.tutorialspoint.com/unix/unix-file-permission.htm</li> <li>https://www.redhat.com/sysadmin/linux-file-permissions-explained</li> <li>https://kb.iu.edu/d/abdb</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#important-concepts","title":"Important Concepts","text":"<p>Out of the basic information on this topic, these are some particular details you should understand. </p> umask umask is a variable which controls the permissions assigned to files and directories that you create. You have a default umask, which can be changed for future logins if you change your <code>.bashrc</code> file. You can change the current umask at any time before you do run some commands. re-use of permission bits There are nine basic permission bits for files and directories. Three each for owner, group, and other. As UNIX developed and new capabilities were needed, the authors added one more bit (used for setuid and setgid), then started adding multiple meanings to some bits. This is shown by the letter (and its capitalization) used to represent it in the output of the <code>ls -l</code> command. read and execute bits on directories The role of the read and execute bits on directories is somewhat different than that on files.  A directory is essentially a special kind of file, containing details about its contents. So you need to be able to read the directory in order to list its contents. On a directory, the execute bit means \"search\" or \"traverse\" Users must have appropriate permissions on ALL of the parts of a path needed to reach a final file or directory. They don't need to be able to modify all of the parent directories, but they have to be able to descend through the tree of directories. /dcs07/somegroup/data/src/compile.py An example absolute path composed of four directories and a file. You cannot read the file unless the four directories and the file have appropriate permissions and group memberships. You can only modify the file if you have write permission on it. You can create other files in the same directory only if the <code>src</code> directory has the necessary writable bit enabled.","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#unix-commands-to-know","title":"UNIX Commands To Know","text":"<p>You can learn more about these commands by reading their manual page. You can run the command <code>man command-name</code> to read it locally. Tips for reading manual pages can be found in this PDF document.</p> <p>Only some of the arguments needed are shown in this table.</p> Command Description Notes ls -l List file details Lists both files &amp; dirs ls -ld List dir details <code>-d</code> option says not to list contents of dir umask Display or change umask See footnote<sup>1</sup> id Display your user &amp; grp Specify another user to see their info chmod Change permissions use <code>-R</code> option for recursion chgrp Change grp of file or dir use <code>-R</code> option for recursion chown Change owner of file or dir Ask sysadmins to change ownership newgrp - Create new shell with different group the hyphen option is useful sg Execute cmd with different grp mkdir Make a directory You can make a tree of new dirs with <code>-p newdir1/subdir1/subdir2</code> rmdir Remove an empty directory Use <code>/bin/rm -rf</code> to delete non-empty dirs <p>(To keep table size down, we used abbrieviations: dir for directories, grp for groups, cmd for command.)</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#access-control-lists-acls","title":"Access Control Lists - ACLs","text":"<p>Sometimes you want to give access to an individual or a group in ways that the traditional UNIX permissions and ownership model do not allow. You can use an ACL to grant those permissions, including </p> <p>See this document for details on how to do this.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#where-to-share","title":"Where to share?","text":"","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#project-space","title":"Project Space","text":"<p>The best place for long-term sharing of files, and the only place to share any significant volume of files, is in file systems created for research groups who purchase allocations of one of our storage servers.</p> <p>If your need is temporary or you cannot purchase your own space, you might be able to secure permission from an existing owner to use their space. They will need to create a subdirectory within their allocation for you and change the permissions of the directories above that to allow you and your group to see into those directories. We are happy to create a new UNIX group so you can use it in your permissions scheme. Send the request with the desired group name and a list of member usernames to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#home-directory","title":"Home Directory","text":"<p>Everyone has a home directory. By default this is a private space. Things like SSH can break if the permissions on the home directory are set incorrectly.</p> <p>You don't want to open up your home directory to everyone in the cluster by making your home directory group or world readable or writable. By default all users belong to the same group. It's not just that people you like and trust can see your files -- so can hackers if they get access to anyone's account. Your configuration files reveal details about your account, your accounts elsewhere and, if writable, allow hackers to set traps so they can become you. Then possibly access your home or other computers.</p> <p>If you must use your home directory, please use ACLs to give specific access to specific people. We have written an ACL document to guide you. We are happy to create a new UNIX group so you can use it in your permissions scheme. Send the request with the desired group name and a list of member usernames to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#scratch-space","title":"Scratch Space","text":"<p>If you need to briefly share some files with someone and they are too large to send via email (say 5MB), you might consider placing them in shared scratch space. This is not recommended, but we want you to be informed about this choice before you make it.</p> <p>If you and your recipient are members of some common group other than the default, it is best to make your temporary directory group readable and searchable but remove access for \"other\".</p> /tmp An acceptable place for a quick one-time exchange of small files which don't contain protected info IF YOU ARE CAREFUL. This directory is world-readable and writable. A prime consideration is whether there is enough room in /tmp for your purpose. The operating system and other users depend on there ALWAYS being enough free space in this file system. Delete your files ASAP after the exchange. /fastscratch An acceptable place for exchanging larger files, but not for sharing over times longer than, say, a week. Everyone has a 1TB quota on this fast file system, but there is only 22TB of space. This space is meant to enable fast access to data read or written by jobs on the compute nodes. Files older than 30 days are deleted automatically. This document describes this file system in more detail.","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#group-writable-directories","title":"Group Writable Directories","text":"<p>Authoring Note</p> <p>Most remaining work on this page lies below this point. Above here what is missing is mainly some example command output.</p> <p>Every file or directory has an owner and a group. Every user has a primary group, and can belong to one or more secondary groups.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#group-sticky-bit","title":"Group sticky bit","text":"<p>When you want to make a directory tree group writable but also configure the SGID bit on directories, you cannot use a simple recursive chmod command. Because that will apply the same permission to both files and directories.</p> <p>These commands are examples of how you can recursively set correct permissions. They use the <code>find</code> command to print path names to directories or files, then pass those to xargs to execute the given commands upon.  </p> <pre><code>cd into-the-top-of-the-directory-tree\nfind . -type d | xargs chmod u+rwx,g+rwx,g+s,o-rwx\nfind . -type f | xargs chmod u+rwx,g+rw,o-rwx\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#creating-a-group-writable-directory","title":"Creating A Group Writable Directory","text":"<p>Raw Material Here</p> <p>The following needs to be rewritten. It was sent as email to a user.</p> <p>The list command, ls, is crucial to figuring out what is possible, where. The long argument, -l, will show you ownership and permission information, as seen in my previous email. Sometimes you also want to use the -d argument to tell ls that you want to see the information about a directory and not what is inside it.</p> <p>So using ls -l  on /users/shared/55548/ will show you that many subdirectories are not group writable. Your failed efforts involve trying to create files or directories inside of directories that are group readable and searchable and sticky but not writable.</p> <p>Every directory in a path to the destination needs to have suitable permissions. Readability: every directory in the path /users/55548/shared/MA_switching needs to be readable by for you to be able to list its contents. Writability: only the last directory in the path needs to be writable for you to be able to modify its contents.</p> <pre><code>jhpcecms01:/users/55548/shared# ls -l\ndrwxrwsrwx  7 c-jlevy33-55548  c-55548  7 Dec  5 11:10 biosim_leverage/\ndrwxr-s---  3 c-eblasco1-55548 c-55548  3 Feb  1 14:00 c-eblasco1-55548/\ndrwxr-s---  7 c-tbrow261-10201 c-55548  9 Dec 20 12:17 Elixhauser/\ndrwxr-s---  3 c-ttotoja1-55548 c-55548  3 Feb 16 16:09 forOwen/\ndrwxrwxr-x 16 c-jxu123-10201   c-55548 22 Jul 15  2023 from-yoda/\ndrwxrwsrwx  7 c-jlevy33-55548  c-55548  7 Feb  7 10:54 hipfracture_adrd/\ndrwxr-s---  7 c-jlevy33-55548  c-55548  7 Dec 20 14:03 levy_partd/\ndrwxr-s---  2 c-aliu63-55548   c-55548  6 Feb 19 16:39 MA_partB/\ndrwxr-s---  2 c-aliu63-55548   c-55548  5 Dec 15 14:03 MA_switching/\ndrwxr-s--- 10 c-rwu32-55548    c-55548 13 Nov  8 10:31 mltss_duals/\ndrwxrws---  4 c-yyang279-55548 c-55548  4 Nov 30 10:13 shen_yang/\ndrwxr-s---  2 c-ykuang6-55548  c-55548  4 Jan 30 12:13 test/\ndrwxr-s---  2 c-jlevy33-55548  c-55548  3 Aug 29 15:19 toofull/\n</code></pre> <p>Only the owner of a directory can change its permissions. So in this case c-aliu63-55548 needs to change the permissions on MA_switching to make it group writable. This command run by them would change the directory: <code>chmod g+w /users/55548/shared/MA_Switching</code> This command run by them would change the directory and everything within it using the recursive flag: <code>chmod -R g+w /users/55548/shared/MA_Switching</code></p> <p>Why are files and directories created with the permissions that they get? A key factor here is the \u201cumask\u201d setting in the user\u2019s environment when files and directories are being created. The command \u201cumask -S\u201d will show you the permissions that will be used for new files and directories. Here you see an example of my inspecting the contents of my umask setting and then changing it to make it such that when I create a file it will be group-writable. If you want such a setting to be used in the future for all of your logins, you would add a line to the bottom of your .bashrc file in your home directory.</p> <p>jhpcecms01:/users/55548/shared# umask -S u=rwx,g=rx,o=rx jhpcecms01:/users/55548/shared# umask u=rwx,g=rwx,o=rx jhpcecms01:/users/55548/shared# umask -S u=rwx,g=rwx,o=rx</p> <p>Many of the people modifying files in the shared directory may want to adjust the permissions of existing files and directories, and change their umask values. If they want directory trees to be group writable and new files or directories to be the same. Or some other set of permissions. Not everything within the shared directory needs to be shared with every other user of the DUA.</p> <ol> <li> <p>There are 2 versions, one built into bash and a standalone one /usr/bin/umask. The latter has a <code>-S</code> option which displays permissions in a more readable fashion. The built-in only works with octal digits. To see the umask manual page which supports -S, use the command <code>man 1p umask</code> To use this version of the umask command, you need to specify the full path to it (/usr/bin/umask) instead of <code>umask</code>.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","jeffrey"]},{"location":"forms/new-user/","title":"New user","text":"Loading\u2026"},{"location":"gpu/gpu-overview/","title":"GPU Critical Knowledge","text":"","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#overview-or-everything-in-one-document","title":"Overview or Everything in One Document??","text":"<p>Should this be an overview? JRT thinks so, because he envisions a growing number of GPU-related documents instead of one massively long document. </p> <p>Therefore should it be moved out of \"User Guides\" to be a topic inside of \"Software\"?</p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#migrate-previous-web-site-document","title":"MIGRATE PREVIOUS WEB SITE DOCUMENT","text":"<p>Probably put some of it here, some of it into other document(s)</p> <p>https://jhpce.jhu.edu/knowledge-base/gpus-on-the-jhpce-cluster</p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#our-gpu-nodes","title":"Our GPU Nodes","text":"<p>Differentiate between public and PI partitions.</p> <p>We have a table showing their resources, and partition names.</p> <p>This command will show you the current use of GPU resources: <code>slurmpic -g</code></p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#using-them","title":"Using them","text":"<p>Dos and don'ts.</p> <p>Is it all interactive? If not, provide some example batch job files.</p> <p>Mark has written material and sent it to various people. Combine that with perhaps info from this Tensorflow sample document</p> <p>Look for other resources out there containing advice about using GPUs in a SLURM context.</p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#existing-docs-at-other-sites","title":"Existing docs at other sites","text":"<p>Warning</p> <p>These other clusters have different software and policies. Look for useful information but don't expect what they say/do to work here.</p> <p>USC: USC documentation</p> <p>New Mexico State Univ: their page</p> <p>UMich section showing relevant SLURM directives for GPU use</p> <p>Yale has CUDA, tensorflow and miniconda modules while we do not. Useful? PyTorch install instructions.</p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#application-specific-advice","title":"Application-specific advice","text":"<p>Authoring Note</p> <p>JRT added this info here but believes it should be migrated out of the \"GPU Overview\" document to somewhere else.</p>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#alphafold","title":"Alphafold","text":"<pre><code>$ module load alphafold/4.3.1\n$ srun --pty --x11 --mem=100G --cpus-per-task=8 --partition gpu --gpus=1 bash\n[compute-123]$ module load alphafold\n(alphafold-gpu) [compute-123]$ run_alphafold.sh -d /legacy/alphafold/data -f ./test.fasta -o . -t 2020-05-14 -n 8\n</code></pre>","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"gpu/gpu-overview/#tensorflow","title":"Tensorflow","text":"","tags":["topic-overview","needs-to-be-written","refers-to-old-website","gpu","mark"]},{"location":"help/cost-calculator/","title":"Cost Calculator","text":"<p>Should this document be renamed to something more general, like Billing or The Cost of Computing?</p> <p>PULL MATERIAL IN FROM EXISTING WEB SITE POLICIES PAGE</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#why","title":"Why?","text":"","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#billing-process","title":"Billing Process","text":"","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#how-do-we-calculate-charges","title":"How do we calculate charges?","text":"<p>Requested not used, except for duration.</p> <p>CPU requested</p> <p>RAM requested</p> <p>Job duration</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#if-something-changes","title":"If something changes","text":"<p>Who to contact with what information?</p> <ul> <li>People leave</li> <li>People work for different PIs</li> <li>People work for multiple PIs</li> <li>PIs leave</li> </ul>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#cost-calculator_1","title":"Cost Calculator","text":"<p>Are we going to create a form? Or just give some examples?</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#seeing-your-past-usage","title":"Seeing Your Past Usage","text":"<p>The sacct command is used to view information about completed jobs. Here is our sacct document with tips for using that command.</p>","tags":["needs-to-be-written","mark"]},{"location":"help/external/","title":"External Guides","text":"<p>The World Wide Web has been of incalculable benefit to humanity. We owe Sir Tim Berners-Lee an enormous debt of gratitude.</p> <p>Tip</p> <p>If you have suggestions for this page, please send the link(s) and your reasons for nominating it/them to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#caveat-emptor","title":"CAVEAT EMPTOR","text":"<p>You need to consider whether what you're reading on an external web site applies to JHPCE. Policies and practices differ between clusters, as does available software and their versions.  Don't copy-and-paste blindly. Nonetheless, there is a wealth of information out there that you can apply to your work.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#slurm-web-sites-of-note","title":"SLURM Web Sites of Note","text":"<p>We are accumulating this information on this page.</p> <p>We also have a document with links to not only the vendor's documentation for each common command, but also pages we've written with advice and examples for some important commands.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#user-tutorials","title":"User tutorials","text":"<ul> <li>R/Bioconductor-powered Team Data Science - this site has a wealth of information for improving your data science and teamwork skills. A real gem for people new to the field!</li> <li>Lieber Institute RStats club</li> <li>Leo Collado Torres YouTube Channel including JHPCE playlist - note that the JHPCE playlist starts off with videos featuring the Sung Grid Engine job scheduler, which has been replaced by SLURM in 2023. The data science contents of the SGE-containing videos may still be of interest.</li> <li>Lieber Institute module config and Lieber Institute module source code - Lieber has generously built and maintains many software modules for JHPCE users</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"help/external/#jhpce-tutorials","title":"JHPCE Tutorials","text":"<ul> <li>JHPCE 3.0 Orientation Slides -- SLURM scheduler &amp; Rocky 9</li> <li>C-SUB JHPCE Orientation Slides 2023</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"help/external/#obsolete","title":"Obsolete","text":"<p>JHPCE 2.0 Orientation Slides -- 2023 -- Sun Grid Engine</p>","tags":["in-progress","jeffrey"]},{"location":"help/faq/","title":"FAQ","text":"<p>There is a dedicated SLURM FAQ document.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#why-does-bash-report-that-it-cant-find-the-module-command","title":"Why does bash report that it can\u2019t find the module command?","text":"Click to expand answer <p>If you receive a message like</p> <pre><code>bash: module: command not found\n</code></pre> <p>The module is a shell function that is declared in <code>/etc/bashrc</code>. It is always a good idea for <code>/etc/bashrc</code> to be sourced immediately in you <code>~/.bashrc</code>.  Edit your <code>.bashrc</code> file so that the first thing it does is o execute the system bashrc file, i.e. your <code>.bashrc</code> file should start with the following lines:</p> <pre><code>if [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq/#my-script-is-giving-odd-error-messages-about-r-or-m","title":"My script is giving odd error messages about <code>\\r</code> or <code>^M</code>.","text":"Click to expand answer <p>Windows and Unix use different characters to indicate a new line.  If you have uploaded your script from a Windows machine, it may have the Windows newline characters.  These need to be replaced by the Unix newline characters.  To do this, you can run the \u201cdos2unix\u201d command on your script <code>dos2unix myscript.sh</code>. This will strip out all of the Windows newlines and replace them with the Unix newlines.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#what-is-the-safe-desktop","title":"What is the SAFE desktop?","text":"Click to expand answer <p>The SAFE desktop is a virtual Windows computer that you can use to run scientific software and access JHPCE. For more information see this item.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#ssh-bad-owner-or-permissions","title":"SSH \"Bad owner or permissions\"","text":"Click to expand answer <p>If you receive a message like \"Bad owner or permissions on ~/.ssh/config\" or continue to have to provide your password when ssh'ing to JHPCE when you think you have configured things to not need one, your file owner or permissions may be incorrect. See this document for answers.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#im-getting-x11-errors-when-using-rstudio-with-putty-and-xming","title":"I\u2019m getting X11 errors when using rstudio with Putty and Xming","text":"<p>Obsolete</p> <p>As of 20240220 vcxsrv is not installed on the cluster. This FAQ item needs to be reviewed and probably removed. JRT</p> <p>We\u2019ve had issues reported by users of Putty with Xming. One solution we\u2019ve found is to use the (vcxsrv))[https://sourceforge.net/projects/vcxsrv/] instead Xming newlines.  (This commmand is on all nodes.)</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#how-can-i-add-packages-into-emacs-on-the-cluster","title":"How can I add packages into emacs on the cluster?","text":"<p>Make sure to <code>module load emacs</code> to get a later version of emacs. Then one needs to edit their <code>.emacs</code> file in their home director to include the package repos     <pre><code>(require 'use-package)\n(add-to-list 'package-archives '(\"gnu\" . \"http://elpa.gnu.org/packages/\") t)\n(add-to-list 'package-archives '(\"melpa\" . \"http://melpa.org/packages/\") t)\n</code></pre>     After restaring emacs then one can do <code>M-x package-list-packages</code> and follow the GUI.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#xauth-error-messages-from-macos-sierra-when-using-x11-forwarding-in-ssh","title":"Xauth error messages from MacOS Sierra when using X11 forwarding in SSH","text":"<p>With the upgrade to MacOS Sierra, the \u201c-X\u201d option to ssh to enable X11 forwarding may not work.  If you receive the message: <code>untrusted X11 forwarding setup failed: xauth key data not generated</code> , you can resolve the issue by add the line <code>ForwardX11Trusted yes</code> to your <code>~/.ssh/config</code> file on your Mac. You may still see the warning: Warning: <code>No xauth data; using fake authentication data for X11 forwarding.</code> To eliminate this warning, add the line <code>XAuthLocation /usr/X11/bin/xauth</code> to your <code>~/.ssh/config</code> file on your Mac.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#when-running-sas-an-error-dialog-pops-up-about-remote-browser","title":"When running SAS, an error dialog pops up about Remote Browser","text":"<p>When running SAS, you may need to specify options to indicate which browser to use when displaying either help or graphical output. We recommend using the Chromium browser.</p> <p>See our SAS usage document about how to resolve this issue.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#im-on-a-mac-and-the-c-command-to-interrupt-an-ssh-session-isnt-working","title":"I\u2019m on a Mac, and the <code>~C</code> command to interrupt an ssh session isn\u2019t working","text":"<p>It used to, but I upgraded MacOS and now it does not work.*  Newer versions of MacOS have disabled by default the ability to send an SSH Escape with <code>~C</code> (~ Shift+C).  To reenable this, on you Mac, you need to set the <code>EnableEscapeCommandline</code> option.  You can do this by either running <code>ssh -o EnableEscapeCommandline=yes . . .</code> or by editing your <code>~/.ssh/config</code> file, and at the top of that file add the line:</p> <pre><code>EnableEscapeCommandline=yes\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq/#how-do-i-get-the-rstudio-program-to-work-on-the-cluster","title":"How do I get the Rstudio program to work on the cluster?","text":"<p>See our core R support document about this and other R usage.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#my-x11-forwarding-stops-working-after-20-minutes","title":"My X11 forwarding stops working after 20 minutes","text":"<p>This error comes from the <code>ForwardX11Timeout</code> variable, which is set by default to 20 minutes.  To avoid this issue, a larger timeout can be supplied on the command line to, say, 336 hours (2 weeks):</p> <pre><code>$ ssh -X username@jhpce01.jhsph.edu -o ForwardX11Timeout=336h\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq/#how-do-i-copy-a-large-directory-structure-from-one-place-to-another","title":"How do I copy a large directory structure from one place to another.","text":"<p>Please do not copy or move anything except a small set of files on the login nodes.</p> <p>As an example, to copy a directory tree from <code>/home/bst/bob/src</code> to <code>/dcs07/bob/dst</code>, first, create a cluster script, let\u2019s call it <code>copy-job</code>, that contains the line <code>rsync -avzh /home/bst/bob/src/ /dcs07/bob/dst/</code>. Next, submit this script as a batch job to the cluster. An example SLURM batch job can be found here.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#my-app-is-complaining-that-it-cant-find-a-shared-library-eg-libgfortranso1","title":"My app is complaining that it can\u2019t find a shared library, e.g. <code>libgfortran.so.1</code>","text":"<p>Nine times out of ten, the allegedly missing library is there. The problem is that your application is looking for the version of the library that is compatible with the old system software. It will not help to point your application to the new libraries. They are more than likely to be incompatible with the new system. The correct solution is to reinstall your software. If the problem persists after the reinstallation, then please contact us and we will install standard libraries that are actually missing.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#ssh-gave-a-scary-warning-remote-host-identification-has-changed","title":"ssh gave a scary warning: <code>REMOTE HOST IDENTIFICATION HAS CHANGED</code>","text":"<p>Go into the <code>~/.ssh</code> directory of your laptop/desktop and edit the known_hosts file.  Search for the line that starts with the host that you ssh\u2019d to. Delete that line (it is probably a long line that wraps). Then try again</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/#why-arent-slurm-commands-or-r-or-matlab-or-available-to-my-cron-job","title":"Why aren\u2019t SLURM commands, or R, or matlab, or\u2026 available to my cron job?","text":"<p>Authoring note</p> <p>This info needs to be re-written for upgraded cluster. And moved to the SLURM FAQ. Should the <code>scrontab</code> command be mentioned?</p> <p><code>cron</code> jobs are not launched from a login shell, but the module commands and the JHPCE default environment is initialized automatically only when you log in. Consequently, in a cron job, you have to do the initialization yourself. Do this by wrapping your cron job in a bash script that initializes the module command and then loads the default modules. You bash shell script should start with the following lines:</p> <pre><code>#!/bin/bash\n\n# Source the global bashrc\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\nmodule load JHPCE_DEFAULT_ENV\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq/#ive-deleted-files-but-my-quota-hasnt-changed","title":"I've deleted files but my quota hasn't changed","text":"<p>See this document.</p>","tags":["needs-review","in-progress"]},{"location":"help/general-advice/","title":"General Tips/Requests","text":"","tags":["needs-major-revision","refers-to-old-website"]},{"location":"help/general-advice/#the-cluster-is-a-shared-resource","title":"The cluster is a shared resource","text":"<p>Generally expected knowledge + Take care not to use too many resources on the login nodes. Anything CPU, RAM, or input/output intensive should be done on compute nodes rather than one of the login nodes. + Anything more than a quick <code>ls</code> including: copying large files, recursively changing permissions, creating or extracting tar or zip archives, running a <code>find</code> should be done in a session on a compute node. + Data transfers of files larger about 1GB should be done through <code>jhpce-transfer01.jhsph.edu</code> rather than a login node. + Try to avoid having directories with more than 100 files in them.  + Try to avoid storing programs and scripts in data directories like <code>DCL*</code>. + Most storage on the cluster is raided but not backed up.  + We do back up home directories and a few other select directories on the DCS and DCL systems for groups that have requested backups.  We do have additional backup storage capacity available for a small fee. + Make use of your 1 TB of fastscratch storage for IO intensive job + Please remember that DCS and DCL stand for \u201cDirt Cheap Storage\u201d and   \u201cDirt Cheap Lustre\u201d, and were designed with cost-effectiveness as a   primary driving factor over performance. + Sharing data can be done in several ways on the cluster: i.) traditional Unix file permissions and groups and ii.) Access Control Lists (ACLs). + Sharing files with external collaborators can be done via Globus.</p>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"help/general-advice/#best-practices-passwords-and-authentication","title":"Best practices: passwords and authentication","text":"<ul> <li>Do not share your password with ANYONE. You are responsible for the use of your account.</li> <li>Choose a \"strong\" password.</li> <li>Use the \"kpasswd\" command to choose a new password. It requires your new password to have three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</li> <li>It would be best if your password was unique and not the same     password you use on other systems.</li> <li>If you believe your password or your computer have been compromised     please alert us via email to bitsupport@lists.jh.edu. Then reset your password using a different device. Visit     https://jhpce-app02.jhsph.edu/\u00a0 to reset your password.\u00a0 This web     site is only available on campus, so if you are outside of the     school network, you will need login to the JHU VPN first. You will     log into that page with your JHED ID and password.</li> <li>Hopkins staff will *NEVER* send you an email message asking for     your password or login credentials</li> <li>NEVER give out your password and login ID to anyone in an email     message or on a web page.</li> </ul> <p>If you have never used a Linux or Unix system before, we strongly     recommend going through the Unix Command     Line     tutorial. The cluster is entirely Linux based. Our Orientation is     about using the cluster, not using Linux. This tutorial should     only take 30 minutes or so to go through.</p>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"help/general-advice/#new-c-sub-user-orientation","title":"New C-SUB user orientation","text":"","tags":["needs-major-revision","refers-to-old-website"]},{"location":"help/general-advice/#what-to-do-before-the-c-sub-jhpce-orientation-session","title":"What to do BEFORE the C-SUB JHPCE Orientation Session","text":"<p>There is a lot of material for us to cover and you to absorb. It is vital for your success that you complete a number of steps PRIOR to attending the\u00a0Orientation Session for the CMS Subcluster of the JHPCE (pronounced by its letters J-H-P-C-E) cluster.</p> <ul> <li>Download a copy of the slides from the Orientation     from:JHPCE-Overview-CMS.pdf.</li> <li></li> <li> <p>In order to access the JHPCE cluster and make use of the     applications on the cluster, you may need to install additional     software on your smart phone and laptop.</p> </li> <li> <p>Install the 2 Factor Authentication program. \u00a0The JHPCE cluster     makes use of \"Google Authenticator\" to provide enhanced security.     \u00a0You can choose to either install an app on your smartphone or, if     you do not have an Apple or Android based smart phone, you can     install an extension to the Google Chrome browser.\u00a0 Prior to the     Orientation Session, you will only need to download the     GoogleAuthenticator app on your smart phone, or install the Authy     Chrome extension. We will be configuring Google Authenticator     during the Orientation Session. Please     see\u00a0https://jhpce.jhu.edu/knowledge-base/authentication/2-factor-authentication/#otp     for instructions.</p> </li> <li>Install required client software.\u00a0 You may need to install a     couple of programs on your laptop or desktop in order to access the     JHPCE Cluster.\u00a0 You will need 1) an SSH client for logging in, 2) an     SFTP client for transferring files to and from the cluster, and 3)     an X11 client for displaying graphics back from the JHPCE cluster.     \u00a0The SSH client is a requirement -- the SFTP and X11 clients are     preferable but optional.<ul> <li>Microsoft Windows We have found that the easiest program to use for accessing the     JHPCE cluster is MobaXterm as it combines the functionality of     all 3 software packages (SSH, SFTP, and X11) in 1 program.\u00a0     Install MobaXterm by following the first few steps of     https://jhpce.jhu.edu/knowledge-base/mobaxterm-configuration/     .\u00a0 Alternatively, if you already use an SSH client, (such as     putty     or Cygwin) and an SCP client\u00a0 (such as     WinSCP),     you can continue using that software.</li> <li>Apple Macintosh There are built in command line tools for ssh and scp that     can be run from a Terminal window. The Terminal program can be     found in \"Applications -&gt; Utilities\". From a Terminal window,     you would type:\\     <code>ssh &lt;username&gt;@jhpcecms01.jhsph.edu</code>and then login with     the login id and the password we provided to you.- In order to     run graphical programs on the cluster and have them displayed on     your Mac, you will need to install XQuartz from     http://xquartz.macosforge.org/landing/.- Optionally, you can     also install a GUI based SFTP program such as     \"Filezilla\". One note about     Filezilla -- if you download the package from the default link     on SourceForge, you may be be blocked by your MalWare/Virus     Scanner, or prompted to install Potentially Unwanted Programs     (PUPs) during installation.\u00a0 We recommend you follow the     alternative download link     here to     download a clean copy of the program.</li> </ul> </li> </ul>","tags":["needs-major-revision","refers-to-old-website"]},{"location":"help/good-query/","title":"Helpful Hints For Asking Questions or Reporting Problems","text":"<p>This page describes how to write a good help request email.</p>","tags":["done"]},{"location":"help/good-query/#a-good-request","title":"A Good Request","text":"<ol> <li> <p>Write a descriptive summary.</p> <ul> <li>Put in a short summary into the Subject.</li> <li>Which cluster are you on? We support two -- JHPCE and C-SUB. We assume JHPCE but specify \"C-SUB\" if that is where you are working.</li> </ul> </li> <li> <p>Expand on this in a first paragraph. Try to answer the following questions:</p> <ul> <li>Please give us your user name on the cluster.</li> <li>What are you trying to achieve?</li> <li>When did the problem start?</li> <li>Did it work before or is this the first time you are trying to do this?</li> <li>Which steps did you attempt to achieve this?</li> </ul> </li> <li>Additional advice:<ul> <li>Put enough details in the details section.</li> <li>Send us screenshot images of what you did -- often we can notice little details you might not have thought to mention.</li> <li>Please give us the exact commands you type into your console.</li> <li>What are the symptoms/is the error message</li> <li>Have you tried to google for the error message?</li> <li>Never put your password into the ticket.</li> <li>In the case that you handle person-related data of patients/study participants, never write any of this information into the ticket or subsequent email.</li> </ul> </li> </ol>","tags":["done"]},{"location":"help/good-query/#specific-questions-for-common-issues","title":"Specific questions for common issues","text":"","tags":["done"]},{"location":"help/good-query/#problems-connecting-to-the-cluster","title":"Problems Connecting to the Cluster","text":"<ul> <li>From which machine/IP do you try to connect (ifconfig on Linux/Mac, ipconfig on Windows)?</li> <li>Did it work before?</li> <li>What is your cluster user name? This is NOT your JHED ID.</li> <li>Please send us the output of <code>ssh-add -l</code> and add <code>-vvv</code> to the SSH command that fails for you.</li> <li>What is the response of the server?</li> </ul>","tags":["done"]},{"location":"help/good-query/#problems-submitting-jobs","title":"Problems Submitting Jobs","text":"<ul> <li>Please give us the directory that you ran things in.</li> <li>Please send us the submission script that you have problems with.</li> <li>If the job was submitted, Slurm will give you a job ID. We will need this ID.</li> <li>Please send us the output of scontrol show job  or sacct --long -j  of your job.","tags":["done"]},{"location":"help/help-basics/","title":"Help","text":""},{"location":"help/help-basics/#ways-to-seek-help-and-support","title":"Ways to Seek Help and Support","text":"Search this website: There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Search/Email for \"bithelp\" advice: <code>bithelp@lists.johnshopkins.edu</code>. This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users, and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. You should check to see if your question has been asked and answered before in the list archives. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff vi \"bitsupport\": <code>bitsupport@lists.johnshopkins.edu</code>. This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty. You should check to see if your question has been asked and answered before in the list archives. Please don't hesitate to reach out to us if you have any questions or concerns. <p>Tip</p> <p>When contacting us, please provide enough information so we can promptly diagnose the problem or answer your question.</p> <p>Here is a page describing the information we need.</p>"},{"location":"joinus/intro/","title":"Introduction","text":""},{"location":"joinus/intro/#overview-for-new-users-or-pis","title":"Overview for new users or PIs","text":"<p>The JHPCE Cluster operates as a non-profit service center run out of the  Biostats department of the Bloomberg JHU Bloomberg School of Public Health. As  such we do charge fairly nominal fees for compute and storage useage, and all users on the cluster need to have a sponsoring PI that will fund their usage.</p>"},{"location":"joinus/intro/#information-for-new-pis","title":"Information for New PIs","text":"<p>The JHPCE cluster is a Linux based environment, and is great for running loosely coupled parallelizable jobs, or programs needing large amounts of  CPUs, RAM, or disk space.  We have a number of genomics and statistical tools installed on the cluster, but most of the analysis is done using programs  written in languages like R, python, Stata, or SAS.</p> <p>As menetioned above, we do operate as a non-profit JHU Service Center, and do charge for compute and storage usage on the cluster.  </p> Compute Charges Our fees for compute time are roughly 1 penny per hour for a job using 1 core  and 5GB of RAM.  Costs scale linearly with time and cpu+mem usage, so a job  running for 24 hours that used 8 cores and 40GB of RAM would cost about $2.00.  Storage Charges    Costs for storage are broken into home directory storage and project storage space.       1. **Home directory Storage:** All users are given a personal home directory with a 100GB quota.  For home directory space, we charge $0.45 per GB per year, so this cost would max out at $45 for a year if a user used their entire 100GB of space.      2. Project Storage: If you need several TB of space for storing large amounts of data, you can purchase an allocation on one of our large storage arrays.  Every 12 months or so we purchase a new large storage array for the JHPCE cluster, and sell allocations on that array. The cost for an allocation will be based on the actual cost of the storage, but has been decreasing over time.  Our latest storage build worked out to be about $30 per TB per year.  There is typically a 10TB minimum buyin for new storage purchases.  <p>If that sounds like a good fit for your lab, then the first step in accessing the cluster would be for you or someone on your team to complete the New Project form  This will help us gather the relevant contact and financial information for your lab.</p> <p>Once the New Project form is completed, we\u2019ll add your group as a Sponsoring Organization on the New Users form at which point the members of your team that will be performing the analysis can sign up for an account.  All new users are required to attend one of our JHPCE Orientation Sessions, held every other Wednesday afternoon, during which their account will be set up on the cluster.</p> <p>JRT didn't put the c-sub info in this document. (He was assuming that c-sub users would have their own page.) In any case, maybe some of the c-sub material should be made a part of this page.</p>"},{"location":"joinus/intro/#information-for-new-users","title":"Information for New Users","text":"<p>If you are prospective user, please start by getting the approval of a PI who has established a Project with the JHPCE cluster. If you and they are both new to JHPCE, please ask ask your PI to visit this page. Once your PI has a Project with JHPCE, you can request an account by filling out the New Users form  Once you complete your form, you'll receive an introductory email, which will include a link to sign up for an upcoming orientation session.</p>"},{"location":"joinus/intro/#orientation","title":"Orientation","text":"<p>We do require that all new users on the cluster attend a JHPCE Orientation Session.  The Orientation is typically held every other Wednesday afternoon, and lasts for about 2 hours.  Prior to attending the orientation, you should review the What to deo before attenting the orientation session page.  The slides for the orientation can be found Here</p>"},{"location":"joinus/new-node/","title":"How to buy nodes for your research","text":"<p>PULL SOME MATERIAL IN FROM EXISTING WEB SITE POLICIES PAGE</p> <p>This is the document for you. The cluster only exists because PIs bought nodes and collaborated. With more nodes, more research could be done, and faster.</p> <p>There are various times when people might consider buying nodes.</p> <ul> <li>HEY, MAYBE I SHOULD URGE MY ADVISOR TO BUY A NODE SO OUR RESEARCH CAN PROCEED FASTER</li> <li>DOES IT REALLY COST THAT MUCH TO GET A NODE?</li> <li>I'M GOING INTO A BUDGET MEETING, MAYBE I SHOULD PLAN TO BUY A NODE?</li> <li>I'M WRITING A GRANT PROPOSAL, MAYBE I SHOULD INCLUDE MONEY FOR BUYING NODES OR STORAGE!!</li> </ul>","tags":["needs-to-be-written"]},{"location":"joinus/new-node/#process","title":"Process","text":"<p>How to go about it.</p>","tags":["needs-to-be-written"]},{"location":"joinus/new-node/#benefits","title":"Benefits","text":"","tags":["needs-to-be-written"]},{"location":"joinus/new-node/#how-your-jobs-are-prioritized","title":"How Your Jobs Are Prioritized","text":"<p>We can pull nodes out of shared. We can give jobs in PI partitions higher priority than those in the shared partition.</p> <ul> <li>how do we help you get your work done preferentially despite the node being in the shared partition</li> <li>about how nodes need to be shared with everyone else. What expectations do we want to set? Do we want to explicitly start talking to the community about buying nodes that are managed but normally private rather than being normally shared?</li> </ul>","tags":["needs-to-be-written"]},{"location":"joinus/new-node/#example-costs-and-configurations","title":"Example costs and configurations","text":"<p>how to characterize what kinds of balances favor what kinds of workloads? Do we even go there unless we are going to start hosting co-located private-by-default nodes?</p> <p>compute node examples. provide a few recent price points. Any value in mentioning these variations?</p> <ul> <li>favors CPUs?</li> <li>favors RAM?</li> <li>local scratch storage space??? we could make a UNIX-group-controlled local scratch for only PI members to use instead of the all-purpose, all-user /scratch concept.</li> </ul> <p>GPU node example cost and configuration (warning about lead times for GPU components??)</p>","tags":["needs-to-be-written"]},{"location":"joinus/new-pi/","title":"Are you a Primary Investigator wanting to join JHPCE?","text":"<p>PULL MATERIAL IN FROM EXISTING WEB SITE POLICIES PAGE</p>","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#do-we-need-a-c-sub-version","title":"Do we need a C-SUB Version?","text":"<p>If so it should live in the \"csub\" directory with all of the other csub-specific files.</p>","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#benfitssales-pitch","title":"Benfits/sales pitch","text":"","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#how-to-register","title":"How to register","text":"<p>If you are a Principal Investigator registering a new project or organization, please fill out this orm. If you have never registered a project and budget number with the JHPCE, we request that you contact the director of the JHPCE to arrange a 1/2 hour orientation (either in person or via telephone).</p>","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#becoming-a-stakeholder","title":"Becoming a stakeholder","text":"<p>You can buy one or more nodes. See this document.</p>","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#please-cite-jhpce-in-your-papers","title":"Please cite JHPCE in your papers","text":"","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-pi/#how-to-budget-for-future-jhpce-expenses","title":"How to budget for future JHPCE expenses?","text":"<p>Maybe some of our PIs can tell us whether there is anything PIs should know from this angle.</p>","tags":["needs-to-be-written","refers-to-old-website"]},{"location":"joinus/new-stg/","title":"Do you need more storage space?","text":"<p>This is the document for you.</p> <p>A short overview document.</p>","tags":["needs-to-be-written"]},{"location":"joinus/new-stg/#maybe-include-a-pointer-to-a-form-with-which-to-request-an-allocation","title":"Maybe include a pointer to a form with which to request an allocation,","text":"<p>(but we want them to first read more details, not just fill out the form wrong. </p>","tags":["needs-to-be-written"]},{"location":"joinus/new-stg/#save-the-details-for-this-other-document","title":"Save the details for this other document:","text":"<p>Point to this document located within the storage topic.</p>","tags":["needs-to-be-written"]},{"location":"joinus/new-users/","title":"New user form","text":"<p>Please complete the form below to request an account on the JHPCE cluster. Once we have received your form, you'll receive an email message with some introductory information and a link to sign up for one of our upcoming orientation sessions.  The sessions are generally held every 2 weeks.</p> <p><sub>(Note: the form is on our old web site and will be transitioned soon)</sub></p> <p>Your browser does not support iframes.</p>","tags":["refers-to-old-website"]},{"location":"orient/first-login/","title":"First Login","text":"<p>This is a stub page for the \"First Login\" topic.</p> <p>Warning</p> <p>Document needs to be written.</p> <p>Possible example of what can go here is this page.</p>","tags":["needs-to-be-written"]},{"location":"orient/orient-stub/","title":"Orient stub","text":"<p>This is a stub page for the \"Orientation\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"orient/stub.md\"</p>"},{"location":"orient/orientation-overview/","title":"All about orientation","text":"","tags":["needs-to-be-written","topic-overview"]},{"location":"orient/orientation-overview/#apply-for-an-account","title":"Apply for an account","text":"","tags":["needs-to-be-written","topic-overview"]},{"location":"orient/orientation-overview/#sign-up-for-a-session","title":"Sign up for a session","text":"<p>Add link here.</p>","tags":["needs-to-be-written","topic-overview"]},{"location":"orient/orientation-overview/#documentation-used-in-orientation","title":"Documentation used in orientation","text":"<p>You can download the orientation slides at JHPCE-Overview</p> <p>(Those files live in docs/orient/images/)</p>","tags":["needs-to-be-written","topic-overview"]},{"location":"ourtools/features/","title":"WEB SITE TOOLS/ENABLED FEATURES WORTH KNOWING HOW TO USE","text":"<p>Let's create a visually appealing web site using some of these features!! Use a WYSIWYG editor like MacDown and \"mkdocs serve\" to quickly edit.</p> <p>Please consult the Tips and Conventions page for authoring guidance and tips.</p> <p>Jeffrey likes:</p> <ul> <li>admonitions -- a lot!!</li> <li>highlighting text</li> <li>keyboard meta keys (like Ctrl)</li> <li>Details (collapsed blocks of text)</li> </ul> <p>Running the mkdocs package installed via python allows you to develop web pages on your local computer. See the recipe</p> <p>There is another document containing wishlist items that we might want to enable/configure.</p>"},{"location":"ourtools/features/#critical-reference-material","title":"Critical reference material","text":"<p>Look here for information about these and other features!!! Just keep in mind that ones marked \"insiders\" are not available for our use. Materials for MkDocs reference section.</p> <p>It isn't clear how much caution one should use in consulting MkDocs documents and people's solutions for it. JRT thinks that Material for MkDocs differs enough that one should definitely keep in mind whether your google search has turned up something about MkDocs.</p>"},{"location":"ourtools/features/#diagramming-with-mermaid","title":"Diagramming with Mermaid","text":"<p>mkdocs.yml contains code enabling the use of a JavaScript tool called Mermaid. If you want your local <code>mkdocs serve</code> program to be able to display it, you need to <code>pip install mkdocs-mermaid2-plugin</code> (see recipe at bottom of this page)</p> <p>Material for MkDocs Diagrams documentation</p> <p>Diagram syntax from the mermaid people</p> <p>A live editor at the mer-people site!!! You can copy the resulting code to your buffer or save the image as (png,svg).</p> <pre><code>graph LR\n   A[Your computer] --&gt; B[Login nodes]\n   B[Login nodes] --&gt; C[(Compute nodes)]</code></pre> <p>Supported types: flowchart (aka graph), sequenceDiagram, stateDiagram-v2,classDiagram, erDiagram</p> <p>JRT finds these types interesting: timeline, user journey</p> <p>Numerous others, including pie, bar and line charts (xychart-beta)</p> Only some types officially supported by Material for MkDocs <p>Besides the diagram types listed above, Mermaid.js provides support for pie charts, gantt charts, user journeys, git graphs and requirement diagrams, all of which are not officially supported by Material for MkDocs. Those diagrams should still work as advertised by Mermaid.js, but we don't consider them a good choice, mostly as they don't work well on mobile. While all Mermaid.js features should work out-of-the-box, Material for MkDocs will currently only adjust the fonts and colors for flowcharts, sequence diagrams, class diagrams, state diagrams and entity relationship diagrams.</p>"},{"location":"ourtools/features/#details","title":"Details","text":"<p>Like an admonition but makes pages more readable by collapsing content. Documentation here and also explained in the detail below.</p> Psst: Click To Expand <p>You can have it be open by default, too. (Add a + after the opening ?+?+?)</p> Syntax to use <p>Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class keyword (like \"tip\" or \"warning\") or classes (separated with spaces) and the summary contained in double quotes. Content is placed below the header and must be indented with FOUR SPACES.</p> <p>Another detail can be nested inside by adding another blank line and another detail header line and content block. But this header line needs to start with the word \"multiple\" So ??? multiple class \"Title\"</p>"},{"location":"ourtools/features/#frontmatter-in-document-files","title":"Frontmatter (in document files)","text":"<p>Tags are the primary use of frontmatter I think we should use at this point. This may not be a complete list of directives that one can optionally add within a document. But the basics are that you can add to the top of the document a stanza to set the title of the document, a description of it, a status indicator such as new or deprecated. See this page for how to define an icon for the page.</p> <p>Apparently multiple frontmatter elements, like both tags and a page title, need to exist together inside one pair of three dashed lines. See example:</p> <pre><code>---\ntags:\n  - a tag\ntitle: some docs need explicit titles set b/c they can't be correctly guessed\n---\n</code></pre>"},{"location":"ourtools/features/#tags","title":"Tags","text":"<p>An example of frontmatter is the code to add tags to documents. 20240211 I tested adding a tag and it works. I also specified in the nav section a page for Material for MkDocs to automatically list tags and the pages they are found on.</p> <p>A lot of optional tag-related settings/capabilities seem to be reserved for paying sponsers as of 20240201. See this page. Can users search for documents by tags in the search field?</p> <p>The tags I envision using at the outset are shown below, so we can try to use them to figure out which pages need attention and possibly who is assigned to finish it.</p> <p>The tag \"needs-improvement-later\" could be added to a page which has \"done\" to indicate that what we have is able to be published but needs more work.</p> <p>The tag \"contains-refs-to-old-site\" came to me because some pages, such as the R page, contain screenshots which contain the URL of the old web site.</p> <p>The tag \"last-revised-YYYYMMDD\" could be used on a page which also has \"done\" or \"needs-improvement-later\" so you can tell that information by looking at the tags page. As opposed to having to go look at the repository. The wishlist document mentions adding a plugin or extension which allows the last-revised date to be automatically generated and listed at the bottom of each page.</p> <p>Place lines like this at the very top of the document, before the document title, to add the tags mentioned. Tags are strings but I am hoping to avoid spaces or underscores. (Underscores suck b/c they require the shift key. And you can't always see them depenind on how text renders.)</p> <p>Code blocks are numbered by default (given settings in mkdocs.yml). See this section for instructions on adding titles to code blocks and disabling line numbering.</p> <pre><code>---\ntags:\n  - done\n  - needs-review\n  - in-progress\n  - needs-improvement-later\n  - contains-refs-to-old-site\n  - last-revised-20240210\n  - jeffrey\n  - mark\n  - jiong\n  - adi\n  - brian\n---\n</code></pre>"},{"location":"ourtools/features/#internal-links","title":"Internal links","text":"<p>From this mkdocs page JRT learned that you can specify anchor points to document sections by knowing that they are converted to lowercase and white space is replaced by dashes. So this very section, named \"Internal links\" can be specified as a link to \"features.md#internal-links\"</p>"},{"location":"ourtools/features/#keyboard-meta-keys","title":"Keyboard meta keys","text":"<p>(enabled by the pymdownx.keys extension)</p> <p>keyboard meta keys displayed e.g. Ctrl by using two plus characters, the keyword like \"ctrl\", then two more plus characters</p> <p>+ + Ctrl + +</p> <p>You can create a meta-key sequence by surrounding the sequence with plus symbols but only using single plus symbols in between the keywords.</p> <p>Ctrl+Alt+Del</p> <p>+ + ctrl + alt + delete + +</p> <p>A backtick ` is called a grave, by the way. As in the French \"accent grave\" </p> <p>Example keywords are: dblquote, cmd, ctrl, esc, tab, del, arrow-up, pipe, windows. All of the symbols you could desire are listed here!!!! https://facelessuser.github.io/pymdown-extensions/extensions/keys/ </p> <p>To create the vertical pipe or bar symbol, you can use + + bar + +</p> <p>You can create a key with any wording you want + + \" Your Wording Here \" + + to create Your Wording Here</p>"},{"location":"ourtools/features/#abbreviations","title":"Abbreviations","text":"<p>Abbreviations can be defined by using a syntax similar to URLs and footnotes, starting with an asterisk immediately followed by the term to be associated in square brackets.</p> <p>This code creates the following sentence. You can hover over \"HTML\" and see the definition appear.</p> <pre><code>The HTML specification is maintained by the W3C.\n*[HTML]: Hyper Text Markup Language\n</code></pre> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"ourtools/features/#glossary","title":"Glossary","text":"<p>There's a way to create a document which is automatically updated when people define abbrieviations. See the wishlist document for details.</p>"},{"location":"ourtools/features/#definition-list","title":"Definition List","text":"<p>You can create an indented block of text using a colon followed by FOUR space characters.</p> <p>Example code and result: <pre><code>`a sample term to define`\n:    The definition you are seeking. (But not the droids.)\n</code></pre></p> <code>a sample term to define</code> The definition you are seeking. (But not the droids.)"},{"location":"ourtools/features/#admonitions","title":"Admonitions","text":"<p>These are sweet! We should use them frequently. But be aware that they are not rendered correctly in MacDown.app. This is where it is good to be running \"mkdocs serve\" on your local machine.</p> <p>About admonitions</p> <p>You add an admonition by</p> <ol> <li>starting a line with three explanation marks, a space, and a keyword (called a \"type qualifier\") such as note, danger, example, info, tip, warning. Here is a list. Certain colors are used for known keywords. If you use your own word or phrase, the color is maybe out of your control.</li> <li>on the next line(s) start with FOUR spaces</li> </ol> <p>Note</p> <p>Some text in a note.</p> <p>Example</p> <p>admonitions allow setting off info inside colored boxes, e.g. note,tip,warning,danger,example. https://squidfunk.github.io/mkdocs-material/reference/admonitions/#usage</p> <p>All lines indented four spaces are included in your admonition, including fenced code blocks.</p>"},{"location":"ourtools/features/#open-urls-in-new-tabs","title":"Open URLs in new tabs","text":"<p>(Adi has configured the server to always open URLs in new tabs.)</p> <p>JRT thinks there might be plugins which make this easier than what you have to do othwerwise, which is to use HTML instead of the Markdown notation. In normal HTML you add a space and a string to the end of the URL: <code>target=\"_blank\"</code></p> <pre><code>&lt;a href=\"https://squidfunk.github.io/mkdocs-material/reference/admonitions/\" target=\"_blank\"&gt;About admonitions&lt;/a&gt;\n</code></pre>"},{"location":"ourtools/features/#footnotes","title":"Footnotes","text":"<p>This<sup>1</sup> is a reference to the feature's description.</p> <p>You add a footnote by entering</p> <p><code>[^1]</code></p> <p>in the midst of your text. Anywhere in the document you write the footnote by placing at the start of a line the corresponding numbered entry using the same syntax but adding a colon and a space character after the closing square brace.</p> <p><code>[^1]: Wording of footnote</code></p>"},{"location":"ourtools/features/#data-tables","title":"Data tables","text":"<p>Tables are easily constructed out of vertical pipe symbols |, hyphens - and text. Optional colons can be used to align column contents.</p> <p>They don't render unless you also include the line of hyphens under the line containing the column titles.</p> <p>A simple table is created with these characters:</p> <p>| Column1 Title | Column2 Title |</p> <p>| ---------- | ---------- |</p> <p>| Contents C1R1 | Contents C2R1 |</p> <p>| Contents C2R1 | Contents C2R2 |</p> <p>Table Tips:</p> <ul> <li>Number of pipe symbols per line must match.</li> <li>Number of hyphens in the second line do not have to match any column width.</li> <li>Alignment is done by placing a colon to the left, right, or on both sides of the hyphens in your dividing line.<ul> <li>First column aligned left, second aligned right:  | :---------- | ----------: | </li> <li>First column no alignment, second aligned center:  | ---------- | :----------: | </li> </ul> </li> </ul>"},{"location":"ourtools/features/#sortable-tables","title":"Sortable tables","text":"<p>This is now implemented.</p> <p>https://squidfunk.github.io/mkdocs-material/reference/data-tables/#sortable-tables</p>"},{"location":"ourtools/features/#import-csv-or-excel-file","title":"Import CSV or Excel file","text":"<p>See https://timvink.github.io/mkdocs-table-reader-plugin/</p>"},{"location":"ourtools/features/#icons-and-emojois-examples-kinds-of-check-marks","title":"ICONS and EMOJOIS: Examples: Kinds of check marks","text":"<p>JRT found that these three kinds of check marks are examples of \"icons\" and \"emojis\".</p> Example Markdown text <code>:material-check:</code> <code>:material-close:</code> <code>:material-check-all:</code> <p>They would not render until I added these three lines to mkdocs.yml: <pre><code>  - attr_list\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n</code></pre></p> <p>See this page for more information and links to the \"icon sets\" as well as other pages that display a bazillion emojis: https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/</p> <p> works if you enter <code>:smile:</code></p> <p>STILL UNKNOWN - CAN ANYONE FIGURE IT OUT AND UPDATE THIS PAGE???</p> <p>But these other things didn't work. For some of them I downloaded an .svg file for them and placed them in both a top-level .icons/ directory and a docs/.icons/ directory. So read more if you want to be able to put all kinds of cool symbols in our pages. DO YOU NEED TO HAVE .ICONS DIRECTORIES AT ALL?</p> <p>:page-facing-up:</p> <p>:mdiDatabaseSettingsOutline:</p> <p>:mdiOrderAlphabeticalAscending:</p> <p>:fa-regular fa-envelope:</p>"},{"location":"ourtools/features/#fenced-code-blocks","title":"Fenced code blocks","text":"<p>Note that you can set off a block of text using three preceding and three following backtick characters.</p> <p>There are MANY options for code blocks. All kinds of syntaxes can be used to mean different things. Here is the main document for code blocks.</p>"},{"location":"ourtools/features/#code-block-line-numbers-highlighting","title":"code block line numbers &amp; highlighting","text":"<p>For example, line numbers are enabled by default. You can disable them for a specific code block by adding after the beginning three back ticks a space, then a <code>linenums=\"0\"</code> If you have multiple code blocks and want the line numbers to continue in second and later blocks, you can replace that 0 with a specific number.</p> <p>You can highlight specific line numbers within the block by adding after the beginning three back ticks a space, then a <code>hl_lines=\"2 3\"</code></p>"},{"location":"ourtools/features/#code-block-titles-including-other-files","title":"code block titles &amp; including other files","text":"<p>Add a title by following leading 3 backticks with a space and <code>title=\".browserslistrc\"</code></p> <p>When Snippets is enabled, content from other files (including source files) can be embedded by using the <code>--8&lt;--</code> notation directly from within a code block to pull in a file via a relative path, in this case <code>includes/sample-bashrc</code></p> sample bashrc<pre><code>\n</code></pre>"},{"location":"ourtools/features/#code-block-formatting-by-programming-language","title":"code block formatting by programming language","text":"<p>!!! note:     The language keywords are not the same as in github -- there is overlap but also differences. See this list for the  language keywords for the Pygments Python syntax highlighter used by Material for MkDocs.</p> <p>Text written before learning about the above:</p> <p>Like in Github you can specify a programming language keyword immediately following the leading three backticks to cause the text to be formatted in that language's notation. </p> <p>There is a directory full of examples you can simply click on.</p> <p>Useful keywords include Awk, checksums, DNS Zone, Jupyter Notebook, Python, R, Regular Expression, Rich Text Format, SAS, Shell, sed, SSH Config/filenames, stata, YAML</p> <p>A python example</p> <pre><code>def fn():\npass\n</code></pre>"},{"location":"ourtools/features/#highlighting-text","title":"Highlighting text","text":"<p>https://squidfunk.github.io/mkdocs-material/reference/code-blocks/</p> <p>https://squidfunk.github.io/mkdocs-material/setup/extensions/python-markdown-extensions/#highlight</p> <p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p> <p>I could not figure out how to quote plain text correctly so it would be displayed instead of rendered. Here I'm going to use the key symbols to defeat that challenge.</p> <p>Highlight a passage by starting with</p> <p>{ = =</p> <p>and end with </p> <p>= = }</p> <p>Delete text by starting with</p> <p>{ - - </p> <p>and ending with</p> <p>- - }</p> <p>Add Replacement text by starting with</p> <p>{ + +</p> <p>and ending with</p> <p>+ + }</p> <p>This can also be combined using tildes and greater-than characters. See the markdown source code onea single.</p> <p>and comments can be added inline by starting with</p> <p>{ &gt; &gt;</p> <p>and ending with</p> <p>&lt; &lt; }</p>"},{"location":"ourtools/features/#recipe-for-running-mkdocs-locally","title":"Recipe for Running Mkdocs Locally","text":"<p>As of 2024029 these steps are needed to build a local Material for MkDocs server that will run a browser at <code>http://127.0.0.1:8000/</code></p> <p>Warning</p> <p>Because our <code>mkdocs.yml</code> contains some certain material, JRT thinks, <code>mkdocs serve</code> spits out errors if it doesn't find Git supporting files/directories. But you're supposed to be able to create stand-alone web pages outside of Git so it would be nice to understand the interdependency.</p> <pre><code>cd ~/Documents/GitHub/\n\ngit clone https://github.com/jhpce-jhu/jhpce_mkdocs\nor you can use the ssh-key method:\ngit clone git@github.com:jhpce-jhu/jhpce_mkdocs\n\ncd jhpce_mkdocs\n\npip3 install mkdocs-material\npip3 install mkdocs-git-revision-date-localized-plugin\npip3 install mkdocs-open-in-new-tab\npip3 install mkdocs-mermaid2-plugin\nmkdocs build\nmkdocs serve\n</code></pre>"},{"location":"ourtools/features/#errors-you-might-run-into-running-mkdocs-locally","title":"Errors You Might Run Into Running Mkdocs Locally","text":"<p>It can sometimes be useful to Ctrl+C the mkdocs serve program and restart it. Usually this involves significant changes to <code>mkdocs.yml</code> and those will stop over time. However when in doubt give it a try.</p> <p>JRT added some instructions to the mkdocs.yml file causing warnings to be issued. JRT has found them very useful. <pre><code># https://www.mkdocs.org/user-guide/configuration/#validation\nvalidation:\n  omitted_files: warn\n  absolute_links: warn\n  unrecognized_links: warn\n</code></pre></p> <p>The <code>mkdocs serve</code> program will spew out a number of warnings and error messages as you change files. Most of them are important but a few of them are going to recur and are harmless. For example warnings about files in the docs/ tree which are not mentioned in the nav bar. Such files are only going to grow in number. Perhaps there is a way to exclude known cases?</p> This error means you have an error in frontmatter YML code somewhere<pre><code>TypeError: '&lt;' not supported between instances of 'NoneType' and 'str'\nERROR   -  [14:46:28] An error happened during the rebuild. The server will appear\n           stuck until build errors are resolved.\n</code></pre>"},{"location":"ourtools/features/#macdown-wysiwyg-editor","title":"MacDown WYSIWYG Editor","text":"<p>Jeffrey has found that the free MacDown editor is VERY HELPFUL in authoring. It is a MarkDown editor for the Macintosh which displays the source code in one pane and the rendered document in another pane opposite.</p> <p>It renders most of Material for MkDocs material correctly, but not all. This is another reason why it is useful to run \"mkdocs serve\" locally, so you can see in a web browser the results of your edits.</p> <p>There are preferences that are worth enabling. This page lists the MarkDown elements which are enabled and disabled by default. This page discusses extended syntax and the site has other interesting MarkDown reference information.</p> <p>One handy feature: If you copy a web URL, highlight some text in your source code, then click on the link insert symbol in the toolbar, it will automatically paste in the URl as it inserts the square brackets and parentheses to create a URL.</p> <p>Does automatic pattern matching for syntax. Includes a command line program to use to open documents (Jeffrey configured his Mac using Get Info on an .md file to use MacDown by default so he can say in Terminal \"open file.md\" and it opens up the GUI). Supports a variety of themes for those who like dark mode.</p> <p>One flaw in this program is that sometimes the source code pane is blank when you open existing documents. The solution is to quit the program and launch it again. The fast workaround is to grow and shrink the document width and the dividing line between the source and rendered document panes.</p> <p>You can install it by download via the web or with Homebrew with <code>brew install --cask macdown</code> Stats on its page indicate some popularity.</p> <ol> <li> <p>https://squidfunk.github.io/mkdocs-material/reference/footnotes/#adding-footnote-references\u00a0\u21a9</p> </li> </ol>"},{"location":"ourtools/tags/","title":"Tagged Files","text":"<p>Tags and the files they are mentioned in are listed here, automatically generated by Material for MkDocs.</p> <p>While the web site is under development, they guide site authors to documents which need help and notify users how to approach the information found on tagged pages.</p> <p>After the web site content stabilizes, the tags that remain will primarily consist of keywords.</p>"},{"location":"ourtools/tags/#adi","title":"adi","text":"<ul> <li>GUI Applications</li> </ul>"},{"location":"ourtools/tags/#done","title":"done","text":"<ul> <li>Access Overview</li> <li>Submitting Good Queries</li> <li>Self Service MFA, Password Requests</li> <li>Quality of Service (QOS)</li> <li>sacct useful command examples</li> <li>scontrol useful command examples</li> <li>When Will My Job Start?</li> <li>Fastscratch</li> <li>Quotas</li> </ul>"},{"location":"ourtools/tags/#gpu","title":"gpu","text":"<ul> <li>GPU</li> </ul>"},{"location":"ourtools/tags/#in-progress","title":"in-progress","text":"<ul> <li>SSH</li> <li>X11</li> <li>What Is The C-SUB?</li> <li>ACL</li> <li>Copying within cluster</li> <li>Sharing Files</li> <li>External Guides</li> <li>FAQ</li> <li>Tips &amp; Conventions</li> <li>GUI Applications</li> <li>Crafting Jobs</li> <li>Monitoring Your Jobs</li> <li>Example User Guides</li> <li>Backups &amp; Restores</li> <li>R Basics</li> <li>SAS</li> </ul>"},{"location":"ourtools/tags/#jeffrey","title":"jeffrey","text":"<ul> <li>Archive Files</li> <li>Copying within cluster</li> <li>Sharing Files</li> <li>External Guides</li> </ul>"},{"location":"ourtools/tags/#jiong","title":"jiong","text":"<ul> <li>Conda Environment</li> <li>Modules</li> <li>Python Packages</li> <li>R Packages</li> <li>SAS</li> <li>Singularity</li> <li>stub page for the \"Software\" topic</li> </ul>"},{"location":"ourtools/tags/#mark","title":"mark","text":"<ul> <li>GPU</li> <li>Cost Calculator</li> <li>Containers</li> </ul>"},{"location":"ourtools/tags/#needs-major-revision","title":"needs-major-revision","text":"<ul> <li>File Transfer - Overview</li> <li>New User</li> <li>General Tips/Requests</li> <li>Storage Overview</li> <li>Helpful GUI Programs</li> </ul>"},{"location":"ourtools/tags/#needs-review","title":"needs-review","text":"<ul> <li>OneDrive via rclone</li> <li>FAQ</li> <li>Modules</li> </ul>"},{"location":"ourtools/tags/#needs-to-be-written","title":"needs-to-be-written","text":"<ul> <li>Staff</li> <li>Archive Files</li> <li>Data Security</li> <li>Files Overview</li> <li>GPU</li> <li>Cost Calculator</li> <li>New node?</li> <li>New PI information and form</li> <li>Need more space?</li> <li>First Login</li> <li>Orientation Overview</li> <li>Adding Your Own Python and R Libraries</li> <li>Conda Environment</li> <li>Containers</li> <li>Jupyter</li> <li>Python Packages</li> <li>R Packages</li> <li>Singularity</li> <li>Overview (Outline)</li> <li>stub page for the \"Software\" topic</li> <li>VS Code</li> </ul>"},{"location":"ourtools/tags/#refers-to-old-website","title":"refers-to-old-website","text":"<ul> <li>Joint HPC Exchange</li> <li>JHPCE Model</li> <li>File Transfer - Overview</li> <li>SSH</li> <li>X11</li> <li>New User</li> <li>GPU</li> <li>General Tips/Requests</li> <li>New PI information and form</li> <li>New user form</li> <li>R Basics</li> </ul>"},{"location":"ourtools/tags/#slurm","title":"slurm","text":"<ul> <li>Crafting Jobs</li> <li>Monitoring Your Jobs</li> <li>Partitions</li> <li>Quality of Service (QOS)</li> <li>SLURM FAQ</li> <li>sacct useful command examples</li> <li>sacctmgr useful command examples</li> <li>scontrol useful command examples</li> <li>Example User Guides</li> <li>When Will My Job Start?</li> </ul>"},{"location":"ourtools/tags/#ssh","title":"ssh","text":"<ul> <li>SSH</li> </ul>"},{"location":"ourtools/tags/#topic-overview","title":"topic-overview","text":"<ul> <li>Access Overview</li> <li>File Transfer - Overview</li> <li>What Is The C-SUB?</li> <li>Files Overview</li> <li>GPU</li> <li>Orientation Overview</li> <li>Storage Overview</li> <li>Overview (Outline)</li> </ul>"},{"location":"ourtools/tips-conventions/","title":"Tips and Conventions","text":"<p>We aim to create a great web site for our users. Consistency contributes to that result. Here are some conventions and time-saving tips.</p> <p>PLEASE read through the Features page for ideas about ways to present information in the best manner. Materials for MkDocs has so many tools for creating attractive and useful pages!!!</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#conventionschecklist","title":"Conventions/Checklist","text":"<p>When editing pages on our site, please keep these things in mind.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#update-tags","title":"Update tags","text":"<p>as needed to reflect current document status. Example <code>needs-to-be-written</code> becomes <code>in-progress</code> as the document is fleshed out and becomes somewhat useful to users.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#remove-authoring-notes","title":"Remove authoring notes","text":"<p>as their guidance is fulfilled by your modifying the document.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#write-information-once-then-refer-to-it","title":"Write information once, then refer to it","text":"<p>Instead of placing versions of the same information in multiple places, put one authoritative version in the right document and then refer to it in other documents where needed. Example: In the FAQ, the answer to a question/issue may be a simple \"See this document\"</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#refer-to-specific-locations-within-documents","title":"Refer to specific locations within documents","text":"<p>Jeffrey enabled \"permalink\" so each section of each document can have its own URL.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#tips","title":"Tips","text":"","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#linking-to-documents-within-web-site","title":"Linking to documents within web site:","text":"<ul> <li>Because documents are divided up between directories by topic, any references you make to them need to use correct relative paths.</li> <li>Links need to include the \".md\" file name suffixes. These are not shown in the URL on the web site, but are required for links to be make correctly.</li> </ul>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#symbolic-link-image-files-which-change","title":"Symbolic Link Image Files Which Change","text":"<p>We have some documents, such as Orientation PDF, which are updated. During the updates their file names often change in order to embed date versioning info.</p> <p>Instead of embedding links in our web pages to the actual PDF file, and having to find and update all of the links every time the file name changes, Jeffrey has found that creating symbolic links with well-chosen static file names to the variable file names allows the links to remain constant.</p> Make a target named latest-orient.pdf<pre><code>cd docs/orient/images\nln -s JHPCE-Overview-CMS-2023-12-2.pdf latest-orient.pdf\n</code></pre>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#blank-lines-are-required-for-some-features-to-work","title":"Blank lines are required for some features to work:","text":"<p>Some elements, such as lists like this one, rely on there being a blank line above the first item. Same is true for admonitions and details.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#frontmatter-indentation","title":"Frontmatter Indentation:","text":"<p>Items in YAML at the top of many pages has to be indented according to YAML rules, or things break.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#indentation-for-block-content","title":"Indentation for block content:","text":"<p>FOUR spaces is what you need to put in front of each paragraph you want to be included in something like an admonition or detail. FOUR, no more, no less. Four spaces is also needed to nest list items.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#inspect-example-documents","title":"Inspect example documents","text":"<p>if you want to see how something was done in practice. The features page contains many such examples.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#line-numbers-in-code-blocks","title":"Line numbers in code blocks","text":"<p>They had been enabled by default until 20240310, when Jeffrey disabled them. When enabled, you got rid of them for a cod block by adding <code>linenums=\"0\"</code> after the opening three backticks and a space, i.e. ` ` ` Space linenums=\"0\"</p> <p>Now that they have been disabled, if you want line numbering, you should add <code>linenums=\"1\"</code></p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#add-a-title-too","title":"Add a title, too","text":"<p>to code blocks by adding <code>\"title words\"</code> after the opening three backticks and a space, i.e. ` ` ` Space \" My title \" Space linenums=\"0\"</p>","tags":["in-progress"]},{"location":"ourtools/wishlist/","title":"Features we might/should implement","text":""},{"location":"ourtools/wishlist/#places-with-resources-to-investigate","title":"Places with resources to investigate","text":"<p>This best-of-mkdocs is updated regularly and has categorized MkDocs plugins and other solutions generated from github star rankings.</p> <p>This page has a number of recommendations that I find interesting. As always, one has to consider whether something is the best-in-class. The PDF export link below for example hasn't been updated in years. Is it still the right choice? Maybe. Mermaid graph generation plugin, PDF export, page redirects, exclude and exclude-from-search, Jupyter notebook</p>"},{"location":"ourtools/wishlist/#multi-line-column-headers","title":"Multi-line column headers","text":"<p>Maybe this is possible. Would be nice to be able to use more words-per-column without forcing the table to be pushed out and require the use of a scrollbar.</p>"},{"location":"ourtools/wishlist/#absolute-paths","title":"Absolute paths","text":"<p>So references to other documents could be made to a single absolute path. Right now you have to use relative paths, which is just prone to more errors. </p>"},{"location":"ourtools/wishlist/#sortable-tables","title":"Sortable tables","text":"<p>(Adi has enabled) They are possible. Might require an extension or plugin.</p>"},{"location":"ourtools/wishlist/#announcement-bar-in-header","title":"Announcement bar in header","text":"<p>For things like planned outages. See Github issue 777.</p>"},{"location":"ourtools/wishlist/#open-urls-in-new-tabs","title":"Open URLs in new tabs","text":"<p>(Adi has enabled) I think there might be plugins which make this easier than what you have to do othwerwise, which is to use HTML instead of the Markdown notation. In normal HTML you add a space and a string to the end of the URL: <code>target=\"_blank\"</code></p> <pre><code>&lt;a href=\"https://squidfunk.github.io/mkdocs-material/reference/admonitions/\" target=\"_blank\"&gt;About admonitions&lt;/a&gt;\n</code></pre>"},{"location":"ourtools/wishlist/#maybe-a-user-contribution-capability","title":"Maybe a user-contribution capability","text":"<p>For topic headings like SOFTWARE where users have recipes and tuning advice. Implemented how? git pull requests against any page on the site? Is there a way to limit to a subsection?</p>"},{"location":"ourtools/wishlist/#search-enhancements","title":"Search enhancements","text":"<pre><code>plugins:\n  - search\n</code></pre> <p>Search is a plugin. There are many plugins out there for MkDocs. A different set, I suppose, for Material for MkDocs. Some overlap, some exclusive.</p> <p>Note that search is enabled by default but if you add options it HAS TO BE LISTED in the plugins stanza.</p> <p>There are A LOT of extras you can do. Someone explore later if desired.</p> <p>https://squidfunk.github.io/mkdocs-material/setup/setting-up-site-search/</p>"},{"location":"ourtools/wishlist/#blog","title":"BLOG","text":"<p>A blog writable only by site administrators. As a tool for making announcements. <pre><code>plugins:\n  - blog\n</code></pre> Blog is a plugin. https://squidfunk.github.io/mkdocs-material/setup/setting-up-a-blog/ Be aware that you cannot use features preceeded by \"Insiders\" This is enabled by adding a line to the plugins stanza</p> <p><code>- blog</code></p>"},{"location":"ourtools/wishlist/#last-modified-dates-on-documents","title":"last-modified dates on documents","text":"<p>This might be provided by multiple plugins out in the world. The one that might be the most common is \"git-revision-date-localized\"</p>"},{"location":"ourtools/wishlist/#glossary","title":"Glossary","text":"<p>There's a way to create a document which is automatically updated when people define abbrieviations.</p>"},{"location":"ourtools/wishlist/#tabs","title":"Tabs","text":"<p>Jeffrey doesn't quite understand where we would use these yet. But they're very neat. tabs documentation Here is an example of a pair of tabs inside an admonition.</p>"},{"location":"portal/datacatalog/","title":"Database Catalog","text":"<p>The JHPCE Data Catalog provides a way to browse and search an online collection of data sets generated at JHU as well as publicly available ones.</p> <p>The goals of the JHPCE Data Catalog are:</p> <ul> <li>Provide a central place for JHU generated data</li> <li>Facilitate the collaboration between different JHU Departments</li> <li>Help JHU researchers idenfity data sets that relevant to their work</li> </ul> <p>The catalog does not store the data sets but rather provides an index of available ones to help researchers identify data sets pertinent to their work.</p> <p>Warning</p> <p>This is being developed. Please send suggestions about functional matters to bitsupport. After further refinement we will welcome contributions to the database itself.</p> <p>Access requires a JHED credential and a computer that is on campus or using the VPN.</p> <p>https://jhpce-app02.jhsph.edu/</p>"},{"location":"portal/portal-stub/","title":"stub page for the \"Web Portal\" topic","text":"<p>This is a stub page for the \"Web Portal\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"portal/portal-stub.md\"</p> <p>https://jhpce-app02.jhsph.edu/</p>"},{"location":"portal/web-apps/","title":"JHPCE Web Enabled User Apps","text":"<p>Our web portal has several sections. You will need to log with your JHED ID and password. This web site is only available on campus, so if you are outside of the school network, you will need login to the JHU VPN first.</p> <p></p> <ul> <li> <p> JupyterLab</p> <p></p> <p>JupyterLab is the latest web-based interactive development environment for notebooks, code, and data. Its flexible interface allows users to configure and arrange workflows in data science, scientific computing, computational journalism, and machine learning. A modular design invites extensions to expand and enrich functionality. </p> </li> <li> <p> RStudio</p> <p></p> <p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.</p> </li> <li> <p> Visual Studio Code</p> <p></p> <p>Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages and runtimes (such as C++, C#, Java, Python, PHP, Go, .NET). You can visit the link below for additional setup and usage details (this link site is only available on campus, so if you are outside of the school network, you will need login to the JHU VPN first.)</p> <p> Access Instructions</p> </li> </ul> <p>Authoring Note</p> <p>What do users need to know about using this service in practice?</p> <p>How long do sessions run if you don't connect to them? Do they automatically time out?</p> <p>Do users need to do anything to clean up if they change their mind or cannot connect to it or things freeze up? (<code>squeue --me</code> and <code>scancel jobid</code>?)</p> <p>Can you disconnect from and then reconnect to any of these apps?</p>","tags":["in-progress","adi"]},{"location":"portal/web-reset/","title":"JHPCE User Account Tools","text":"<p>Our web portal has several sections.</p> <p>The JHPCE User Account Tools section provide the means to:</p> <ul> <li>Reset your password</li> <li>Request Authenticator Code (\"One Time Password\")</li> <li>Update Contact Information</li> </ul> <p>Warning</p> <p>As of 20240306, C-SUB users cannot use these services, as they are not yet included in an underlying database.</p> <p>Note</p> <p>Your JHPCE cluster username and password are NOT the same as your JHED ID and password. These are maintained by different groups and do not change in one place when changed in the other.</p> <p>Password complexity requirements</p> <p>After you get logged in, use the \"kpasswd\" command to choose a new password. You will need to use a password with three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</p>","tags":["done"]},{"location":"slurm/crafting-jobs/","title":"Crafting SLURM Jobs","text":"<p>Authoring Note</p> <p>This document is accumulating information which might best be split into several more-focused documents. There are many aspects of creating jobs.</p> <p>There are a variety of techniques one can use to initiate SLURM jobs to accomplish various tasks. Here we will initially accumulate pointers to documentation written by others for their clusters. Later we will write concrete examples of our own.</p> <p>In the directories under <code>/jhpce/shared/jhpce/slurm/</code> you will find files used during orientation, some accumulated documents and batch examples.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#sbatch-srun-and-salloc","title":"Sbatch, srun and salloc","text":"<p>There are three commands used to request resources from SLURM. You will find all three discussed in the linked documentation.</p> <p>Here at JHPCE we have been using <code>srun</code> primarily as way to start interactive sessions. However it can also be used inside of <code>sbatch</code> scripts. See web site below for examples.</p> <p>Be careful using <code>salloc</code> that you don't leave allocated resources unused.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#sbatch-rules","title":"Sbatch Rules","text":"<ol> <li>First characters in batch file need to be: <code>#!/bin/bash</code> (although you can use an interpreter other than bash)</li> <li><code>#SBATCH</code> directives need to appear as the first characters on their lines.</li> <li><code>#SBATCH</code> directives need to appear before any shell commands.</li> <li>You can put comments after # symbols.</li> <li>You may need to add a <code>wait</code> command at the bottom to ensure that processes spawned earlier complete before the script does.</li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#slurm-directive-order-of-precendence","title":"SLURM Directive Order of Precendence","text":"<p>For now see these two PDF pages from an orientation document.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#job-environment","title":"Job Environment","text":"<p>Authoringnote</p> <p>This probably deserves its own document. There are a variety of things to describe here, some subtle. Options to pass or not pass parts of environment used to dispatch job into the job. Should people use the <code>-l</code> arg to bash? <code>#SBATCH --chdir=</code></p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#inputoutput-considerations","title":"Input/output Considerations","text":"<p>Authoringnote</p> <p>Use the same terms as used in the storage overview. We want to be consistent. It helps users, and helps us make links between related articles.</p> <ul> <li>home directory</li> <li>local compute node /tmp</li> <li>fastscratch</li> <li>project storage</li> </ul> <p>Some programs have specific variables you can set to indicate where files should be created.</p> <p>SAS has \"WORK\" -- is this set to something in the module load process?</p> <p>R has \"TEMPDIR\" which defaults to /tmp.</p> <pre><code>You could use your 1TB of fastscratch space for this. So your SLURM script could use commands like:\n\nmodule load conda_R\nexport TEMPDIR=$MYSCRATCH\nR CMD BATCH myprog.R\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#dependent-jobs","title":"Dependent jobs","text":"<p>You can configure jobs to run in order with some conditional control. See this part of the sbatch manual page.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#heterogeneous-job-support","title":"Heterogeneous Job Support","text":"<p>Each component of such jobs has virtually all job options available including partition, account and QOS (Quality Of Service). See this vendor document.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#examples-from-elsewhere","title":"Examples from Elsewhere","text":"","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#workflow","title":"Workflow","text":"<p>This cluster has some good material about workflows.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#nersc-examples","title":"NERSC Examples","text":"<p>Good examples.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#usc-examples","title":"USC Examples","text":"<p>Good examples of basic different types of batch jobs</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#running-multiple-jobs-from-one-script","title":"Running Multiple Jobs From One Script","text":"<p>Using srun inside of sbatch scripts, in serial and parallel. Remember to include the <code>wait</code> bash command at the end of your batch file so the job doesn't end before all of the tasks inside of it.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#using-signals-to-clean-upcheckpointing","title":"Using signals to clean up/checkpointing","text":"<p>It's a Good Thing to save the state of your computation so that you can pick up where you left off if your job ends earlier than expected.  We should provide some links to existing documentation people have written about how to implement checkpointing.</p> <p>It's also a Good Thing to clean up after yourself, by, for example, deleting files created in /tmp by your job.</p> <p>If a job is cancelled or killed because it exceeds its time limit (maybe memory too?), SLURM sends two signals some time apart. Normally the first is a TERM signal, later a KILL signal. You can dispatch jobs with instructions to send them specific signals a specified number of seconds before the KILL signal is sent.</p> <p>You can modify your batch jobs so they do Good Things when they receive the first signal.</p> <p>See the sbatch manual page's explanation for the <code>--signal</code> argument.</p> <p>Pay attention to which process(es) are sent signals. The batch job, all of the job steps, ...</p> <p>Here is a blog post which discusses this in some detail.</p> <p>That post refers to this one which was updated after the post was written, so there might be newer info than was incorporated in the post.</p> <p>This stackoverflow answer seems to take a different approach. This is an advanced topic and will require some care and perhaps experimentation to verify your solution.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#example-batch-jobs","title":"Example Batch Jobs","text":"","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#copying-data-within-cluster","title":"Copying data within cluster","text":"<p>Here is a sample batch job. More information about this topic is accumulating here.</p> Click to expand <p>Content. How hard will it be to do code blocks? <pre><code>#!/bin/bash\n\n#SBATCH -p shared\n#SBATCH --mem=10G\n#SBATCH --job-name=cp-files\n#SBATCH --time=15-0\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --output=cp-files-%j.out    # file to collect standard output\n#SBATCH --error=cp-files-%j.err # file to collect standard output\n#SBATCH --mail-user=my-email-address@jhu.edu\n#SBATCH --mail-type=BEGIN,FAIL,END\n\ndate\ncd /dcs04/sample/path/\n\necho \"I am running on compute node:\"\nhostname\n\necho \"In directory:\"\npwd\n\necho \"The files found in this directory are:\"\n/bin/ls\n\n# args are meant to try to prevent files from being deleted in destination\nrsyncargs=\"-h --progress --sparse --numeric-ids --one-file-system --stats --ignore-existing --max-delete=0\"\n\necho \"about to try to rsync\"\n\nrsync -a $rsyncargs directory-to-be-copied /dcs05/destination/path/\n\ndate\necho \"done\"\n</code></pre></p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#copying-data-intoout-of-cluster","title":"Copying data into/out of cluster","text":"<p>We have a transfer node which is a SLURM client.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#running-a-job-on-every-node","title":"Running A Job On Every Node","text":"<p>This is put here as a tool for system administrators needing to do maintenance where a SLURM job is appropriate. Maybe the technique will be useful for someone for a more limited case.</p> Click to expand <pre><code>#!/bin/bash\n#\n# JPHCE - dispatch-to-everynode - Dispatch a job to each node which is responding\n#\n#       FAILS TO WORK IF YOU DON'T SPECIFY A BUNCH OF PARTITIONS\n#       BECAUSE SHARED IS USED. Following worked at this time\n#       #SBATCH --partition=shared,cee,transfer,sysadmin,sas,gpu,bstgpu,neuron\n#\n# You need to specify a batch file at the minimum\n# You can specify additional arguments\n# TODO: Nice to be able to specify a partition to sinfo if desired\n#--------------------------------------------------------------------------\n#--------------------------------------------------------------------------\n# Date          Modification                                       Initials\n#--------------------------------------------------------------------------\n# 20231222      Created, added standard comment section.                JRT\n#--------------------------------------------------------------------------\n\nusage()\n{\necho \"Usage: $0 [directives..] batchfile \"\necho \"Usage:   Specify at least a job file\"\necho \"Usage:   Good idea to include in your batch file --output=/dev/null\"\nexit 1\n}\n\nif [ $# -lt 1 ]; then\n        usage\nelse\n        for i in `sinfo -N -r | awk '{print $1}' | sort -u | grep -v NODELIST`\n        do\n                echo $i\n                sbatch --nodelist=${i} \"$@\"\n        done\nfi\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/","title":"Monitoring SLURM Jobs","text":"<p>Has my job ended? Why did my job fail? How much memory did my sample job use so I can use memory efficiently in for future similar jobs?</p> <p>There are a number of ways in which you can learn the answers to these questions.</p> <p>Making sure your jobs request the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#email-notification","title":"Email Notification","text":"<p>You can direct SLURM to send you email when various things happen with your job. These directives can be given on the command line, in batch job scripts, and set in your SLURM defaults file. You can even modify running jobs to set or change their notification settings (see the scontrol tips page).</p> <p>Warning</p> <p>Take care not to cause a storm of outgoing email from our cluster!!! This will lead to our server being blacklisted by Hopkins and/or other mail administrators. Then NO ONE will get email until we can convince them that it won't happen again.</p> <p>By default email notifications are sent for entire job arrays, not individual tasks. Be VERY careful if you change that behavior.</p> <p>The mail arguments are shown in the sbatch manual page.</p> Specify your email address<pre><code>--mail-user=&lt;email-address&gt;\n</code></pre> Specify notification events<pre><code>--mail-type=&lt;list-of-types&gt;  # comma-separated\n</code></pre> Here are the main types: <ul> <li>NONE, BEGIN, END, FAIL, INVALID_DEPEND</li> <li>ALL (equivalent to BEGIN, END, FAIL, INVALID_DEPEND, REQUEUE, and STAGE_OUT)</li> <li>TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit)</li> </ul>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#output-and-error-files","title":"Output and error files","text":"<p>By default your job will store an output file named \"slurm-%j.out\" where the \"%j\" is replaced by the job ID containing job output and errors in the same directory in which your job ran. You can direct SLURM to put the file(s) elsewhere and change their names.</p> <p>For job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the array index. </p> <p>If you add echo statements in your job batch file then you can inspect the job output/error files for clues as to its status.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#scontrol-show-job","title":"scontrol show job","text":"<p>The <code>scontrol</code> command has many uses. Before a job ends you can get detailed information with <code>scontrol</code>.</p> <p>Yay</p> <p>We have written a document describing frequent uses of scontrol. See this page.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#squeue","title":"squeue","text":"<p>Display information about pending and running jobs.</p> <p>https://slurm.schedmd.com/archive/slurm-22.05.9/squeue.html</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#sstat","title":"sstat","text":"<p>Display process statistics of a running job/step. Some of the advice given for <code>sacct</code> (below) applies to <code>sstat</code>.</p> <p>https://slurm.schedmd.com/archive/slurm-22.05.9/sstat.html</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#sacct","title":"sacct","text":"<p>Display accounting data for running and completed jobs in the Slurm database.</p> <p>Yay</p> <p>We have written a document describing key elements of using sacct. See this page.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#suggestions","title":"Suggestions","text":"<p>The following was sent to a user who was having SAS jobs fail with errors indicating a problem with space or quota.</p> <p>Instrumenting your job means adding commands to it to gather information. Running df commands to check the size and capacity of all or specific file systems. (df -h -x tmpfs might be a helpful variant). As mentioned in the orientation material, the sstat command can gather information for running jobs (as opposed to sacct which is oriented towards completed jobs). Some of the output fields available via sstat and sacct relate to memory usage while others can reveal disk input/output volumes.</p> <p>Because your batch job script might not be able to run sstat\u2019s during SAS\u2019s execution, you may need to run the information-gathering commands interactively or via a second batch job. (I mean your batch job could run commands before or after invoking SAS, unless they put the SAS program into the background they won\u2019t be able to run the commands while SAS is running. I don\u2019t know what happens if one tries to run a program in the background (by putting an &amp; after it, like one can run thunar for example) inside of a SLURM batch job.</p> <p>Your gathering might involve the sleep command in between invocations. I would suggest also running the date command so you have timestamps.</p> <p>You could put the sleep, date, sstat commands inside of a while (true) loop. Then cancel the loop with a control c or scancel depending on whether they are running interactive or batch. Redirecting the output to text files would be useful.</p> <p>So your information-gathering efforts should probably start by running some sacct commands to inspect the parameters of previous failed jobs, and perhaps even the ones that succeeded to see if any differences appear.</p> <p>The -e flag is available to both sstat and sacct to show the fields available. The sets overlap but also differ.</p>","tags":["in-progress","slurm"]},{"location":"slurm/partitions/","title":"Partitions","text":"<p>A partition is a logical collections of nodes that comprise different hardware resources and limits to help meet the wide variety of jobs that get scheduled on the cluster. Nodes can belong to more than one partition.</p> <p>There are several types of partitions:</p> <ul> <li>General access (e.g. shared, interactive, gpu, transfer)</li> <li>Application only (e.g. sas)</li> <li>GPU (equipped with GPU cards)</li> <li>PI-owned (for use only by members of the PI's group)</li> </ul>","tags":["slurm"]},{"location":"slurm/partitions/#pi-partitions","title":"PI Partitions","text":"<p>JHPCE exists because Primary Investigators worked together to create a cluster. They share their resources via public partitions (see below).</p> <p>Only submit jobs to these partitions if you are a member of the Primary Investigator's research groups or have been given explicit permission to do so. If you are in doubt, ask before submitting. Jobs from non-group members will be killed and repeated abuse will lead to repercussions.  </p>","tags":["slurm"]},{"location":"slurm/partitions/#public-partitions","title":"Public Partitions","text":"<p>Partitions shared, interactive, gpu, sas and transfer are considered public and available to all.</p> <p>Specific use</p> <p>Only jobs which require the use of GPU cards should be submitted to the gpu partition.</p> <p>Only jobs which require the use of the SAS application should be submitted to the sas partition.</p> <p>Only jobs related to transferring data into or out of the cluster should be submitted to the transfer partition.</p> <p>The public partitions provide low-priority access to unused capacity throughout the cluster. Capacity on the shared queue is provided on a strictly \u201cas-available\u201d basis and serves two purposes.</p> <p>First it provides surge capacity to stakeholders who temporarily need more compute capacity than they own, and second, it gives provides non-stakeholders access to computing capacity.</p> <p>Scheduling polices attempt to harvest unused capacity as efficiently as possible while mitigating the impact on the rights of stakeholders to use their resources. The JHPCE service center does not guarantee that stakeholders will provide sufficient excess capacity to meet non-stakeholders needs, however in practice the cluster is rarely operating at full capacity so there is usually ample computing capacity on the shared queue.</p>","tags":["slurm"]},{"location":"slurm/partitions/#getting-info-about-partitions","title":"Getting Info About Partitions","text":"<p>Our command <code>slurmpic</code> shows information about partitions, including the member nodes, their current utilization, and some summary statistics.<sup>1</sup> By default it displays the shared partition. Specific partitions can be displayed using <code>slurmpic -p partitionname</code>. All of the nodes in all of the GPU partitions can be displayed with <code>slurmpic -g</code>. Run <code>slurmpic -h</code> to see usage notes.</p> <p>The best way to see the configuration of a partition is with the scontrol command. (Vendor's scontrol manual page. Our local scontrol tips page.) <pre><code>scontrol show partition partitionname\n</code></pre></p> <p>The command <code>sinfo</code> shows information about all of the partitions. It has many options, so you can also use it to see information about nodes. (Note: Partitions which require group membership to submit to are only visible via <code>sinfo</code> to members of those groups. Because the local command <code>slurmpic</code> uses <code>sinfo</code> to retrieve information, the output of <code>slurmpic -a</code> (show all nodes) will omit those private PI partitions' nodes.)</p>","tags":["slurm"]},{"location":"slurm/partitions/#cpu-partitions","title":"CPU Partitions","text":"","tags":["slurm"]},{"location":"slurm/partitions/#public-cpu-partitions","title":"Public CPU Partitions","text":"<p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Core RAM Time Notes/Use shared public 100 1TB (1d/90d) DEFAULT interactive public 2 20gb (1d/90d) Small but accessible gpu public (none) (none) (1d/90d) Only for GPU jobs sas application (none) (none) (none/90d) Licensed for SAS transfer public no (none) (none) (none/90d) <p>To reduce table width, column names are terse.</p>","tags":["slurm"]},{"location":"slurm/partitions/#pi-cpu-partitions","title":"PI CPU Partitions","text":"<p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Core RAM Time Notes/Use bader PI (none) (none) (none/90d) bluejay PI (none) (none) (none/90d) UNIX group cancergen PI (none) (none) (none/90d) caracol PI (none) (none) (none/90d) UNIX group cee PI (none) (none) (none/90d) cegs2 PI (none) (none) (none/90d) chatterjee PI (none) (none) (none/90d) echodac PI (none) (none) (none/90d) gwas PI (none) (none) (none/90d) not yet defined hl PI (none) (none) (none/90d) hpm PI (none) (none) (none/90d) not yet defined hongkai PI (none) (none) (none/90d) mommee PI (none) (none) (none/90d) stanley PI (none) (none) (none/90d) UNIX group sysadmin admin (none) (none) (none/90d) For system testing","tags":["slurm"]},{"location":"slurm/partitions/#gpu-partitions","title":"GPU Partitions","text":"<p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Requires Approval Core RAM GPU Time Notes/Use gpu public no (none) (none) (none) (1d/90d) bestgpu PI yes (none) (none) (none) (none/90d) caracol PI yes (none) (none) (none) (none/90d) neuron PI yes (none) (none) (none) (none/90d) <ol> <li> <p>Note that the statistics displayed are for that partition, not the whole cluster. Also, memory and CPU use of nodes that are DOWN or in DRAIN are not included in the stats.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/qos/","title":"Quality of Service","text":"<p>Systems administrators can define resource limits called QOS and assign them to a variety of objects, mainly users and job partitions. We use them to share resources equitably and to allow exceptions.</p> <p>For example, we have a QOS named <code>shared-default</code> which is normally set to allow a user to use 100 CPU cores and 1TB of RAM at any one time in the <code>shared</code> partition.  These values were chose to represent roughly 20% of those available in that partition. When Primary Investigators who own nodes need to remove them from the shared partition for their own use, that QOS' definition can be changed to a lower value to maintain the 20% goal.</p> <p>Vendor QOS documentation about them can be found here. There are also entries in the manual pages for various SLURM commands about QOS<sup>1</sup>.</p> <p>QOS definitions for users and partitions are stored in a database.</p> <p>We have a document containing useful QOS-related commands. Those commands include ones which allow you to see the value of a QOS like <code>interactive-default</code></p> <p>See our currently defined QOS in a readable format:</p> <p><code>sacctmgr show qos format=Name%20,Priority,Flags%30,MaxWall,MaxTRESPU%20,MaxJobsPU,MaxSubmitPU,MaxTRESPA%25</code></p>","tags":["done","slurm"]},{"location":"slurm/qos/#partition-qos","title":"Partition QOS","text":"<p>Job partitions like <code>shared</code> have two QOS-related attributes:</p> <ol> <li> <p>Qos - If present, this specifies the QOS which by default applies to all jobs submitted. The <code>interactive</code> partition has <code>QoS=interactive-default</code></p> </li> <li> <p>AllowQoS - By default, this value is set to ALL. If set to a comma-separated list, then only those can be used or requested by users. The <code>interactive</code> partition has <code>AllowQos=normal,interactive-default</code></p> </li> </ol> <p>You can see the configuration of a partition with the scontrol command. (Vendor's scontrol manual page. Our local scontrol tips page.) Here is a command which will show you the QOS attributes on an exmple partition named partitionname</p> <pre><code>scontrol show partition partitionname | grep -i qos\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/qos/#user-qos","title":"User QOS","text":"<p>User accounts have two QOS-related attributes:</p> <ol> <li> <p>Qos - Our users (currently) all have a QOS value of \"normal\", (which is inherited from their parent account, named \"jhpce\").  The \"normal\" QOS  (currently) has no limits defined for it.</p> </li> <li> <p>Allowed Qos - By default, this value is empty. If set to a comma-separated list, then the user can choose to submit jobs using one of them.  Jobs request a QOS using the \"--qos=\" option to the sbatch, salloc, and srun commands. (If the partition does not allow a QOS to be used, then your job will be rejected.)</p> </li> </ol> <p>See your own user database values: <code>sacctmgr show user withassoc where name=your-cluster-username</code></p> <p>The SLURM FAQ includes an entry about the error you receive if you ask for more RAM in a single job than is allowed. </p> <p>If you submit jobs which together ask for more memory than you are allowed to use at one time, then ones that \"fit\" inside the limit will run and the remaining jobs will be waiting in PENDING state. </p> <p>The \"Reason\" code shown in the output of <code>squeue --me -t pd</code> (\"show me my pending jobs\") will be <code>QOSMaxMemoryPerUser</code>for jobs waiting for your running jobs to be using less than the RAM limit defined in the QOS that is impacting you.  The Reason will be <code>QOSMaxCpuPerUserLimit</code> for jobs waiting for your core usage to drop below that which is allowed.</p>","tags":["done","slurm"]},{"location":"slurm/qos/#how-we-are-using-qos-to-date","title":"HOW WE ARE USING QOS TO DATE...","text":"<p>This is a summary of how QOS is configured at JHPCE.</p> <p>Our users all have a QOS value of \"normal\", which is inherited from their parent account, named \"jhpce\"</p> <p>(The account jhpce, of the organization jhpce, in the cluster jhpce3 is the parent of all of the users.)</p> <p>The \"normal\" QOS  (currently) has no limits defined for it. (It is the default QOS defined in SLURM. We didn't create it.)</p> <p>A fundamental element of our current practice is that the user\u2019s default QOS entries in the database are \"normal\".</p> <p>We\u2019ve defined additional ones and, for some users, changed the Allowed QoS field in their user entries to list ones that they can optionally use in addition to \"normal\". </p> <p>Those additional ones are being applied</p> <ol> <li>via per-partition QOS= definitions in /etc/slurm/partitions.conf     (e.g. QOS \"interactive-default\", \"shared-default\"), and </li> <li>via users specifying additional ones via per-job     directives (b/c we granted them access to them and     told them that they needed to use those directives).     (Users do not have to specify optional QOS on every     job thenceforward. Users can pick and choose what QOS to use for each job.</li> </ol> <ol> <li> <p>Capitalization doesn't matter when specifying QOS in commands.\u00a0\u21a9</p> </li> </ol>","tags":["done","slurm"]},{"location":"slurm/slurm-commands-ref/","title":"SLURM COMMANDS","text":"<p>Here are links to online copies of the manual pages for commands. If we've written a page with advice about using the command, use the (LOCAL TIPS) link.</p>"},{"location":"slurm/slurm-commands-ref/#locally-written-tools","title":"Locally Written Tools","text":"<ul> <li>slurmpic: Essential program for getting cluster status info. Use -h option to see essential usage details. (no man page yet)</li> <li>smem: Displays memory used by your currently running jobs. If given a jobid number, it will display info about the memory usage of that job. (no man page yet)</li> <li>memory reporting script - puts per-user output daily into directories under /jhpce/shared/jhpce/jhpce-log/</li> </ul>"},{"location":"slurm/slurm-commands-ref/#contributed-programs-weve-installed","title":"Contributed Programs We've Installed","text":"<ul> <li>seff: Display efficiency of CPU and RAM usage of a completed job. (no man page yet)</li> <li>slurm-mail: Tool used to add details to mail sent to you. Not something you can modify. Listed for completeness.</li> </ul>"},{"location":"slurm/slurm-commands-ref/#provided-with-slurm","title":"Provided with Slurm","text":"<p>All of the manual pages are here, including those for the configuration files found in /etc/slurm/</p>"},{"location":"slurm/slurm-commands-ref/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li>salloc: request an interactive job allocation</li> <li>sbatch: submit a batch script to Slurm</li> <li>srun: launch one or more tasks of an application across requested resources</li> </ul>"},{"location":"slurm/slurm-commands-ref/#information-about-cluster-and-jobs","title":"Information about cluster and jobs","text":"<p>Do not run slurmpic, squeue, sacct or other Slurm client commands that send remote procedure calls to slurmctld, the main SLURM control and scheduling daemon, from loops in shell scripts or other programs. Ensure that programs limit calls to slurmctld to the minimum necessary for the information you are trying to gather.</p> <ul> <li>sacct: (LOCAL TIPS): display accounting data for jobs in the Slurm database</li> <li>sattach: attach to a running job step</li> <li>scontrol: (LOCAL TIPS): display (or modify when permitted) the status of Slurm entities (jobs, nodes, partitions, reservations)</li> <li>sinfo: display node and partition information</li> <li>sprio: (LOCAL TIPS): display the factors that comprise a job's scheduling priority</li> <li>squeue: display the jobs in the scheduling queues, one job per line</li> <li>sshare: display the shares and usage for each charge account and user</li> <li>sstat: display process statistics of a running job/step</li> <li>sview: X11 graphical tool for displaying jobs, partitions, reservations</li> </ul>"},{"location":"slurm/slurm-commands-ref/#controlling-jobs","title":"Controlling Jobs","text":"<ul> <li>scancel: cancel or pause a job or job step or signal a running job or job step to pause</li> <li>scontrol: (LOCAL TIPS): display (and modify when permitted) the status of Slurm entities (jobs, nodes, partitions, reservations)</li> </ul>"},{"location":"slurm/slurm-commands-ref/#for-systems-administrators","title":"For Systems Administrators","text":"<ul> <li>sacctmgr:</li> <li>scontrol: (LOCAL TIPS): display and modify Slurm account information</li> <li>sdiag: display scheduling statistics and timing parameters</li> <li>slurmctld: central management daemon</li> <li>slurmd: client-side daemon</li> <li>sreport: generate canned reports from job accounting data and machine utilization statistics</li> </ul>"},{"location":"slurm/slurm-faq/","title":"JHPCE SLURM FAQ","text":"<p>Use the search field if desired.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#faqs-from-other-clusters","title":"FAQs from other clusters","text":"<p>Note that the information in these pages will include details which do not apply here in JHPCE. Caveat emptor.</p> <p>CECI</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#when-will-my-job-start","title":"When will my job start?","text":"Click to open <p>See this document for a description of factors affecting when jobs start.  Of course the load on the cluster impacts job start times. Please consult the output of <code>slurmpic</code> for information about the state of the cluster and its available resources. Note that your job cannot start until a match is found for the resources you specified. There may be a lot of unused CPUs on a node, for example, but if someone has allocated all of the RAM on that node your job won't fit there.</p> <p>If the Reason given for a pending job is <code>QOSMaxMemoryPerUser</code> or <code>QOSMaxCpuPerUserLimit</code> then you can read about Quality of Service in our document here.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#what-partitions-exist","title":"What partitions exist?","text":"Click to open <p>The default partition is \"shared\". By default <code>slurmpic</code> describes the state of this partition. (Run <code>slurmpic -h</code> to see a list of options, including the flag to show other partitions.)</p> <p>See this document for a description of partitions and their purposes and limits.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-will-i-know-if-my-job-ended","title":"How will I know if my job ended?","text":"<p>See this document for information about monitoring pending, running and completed jobs.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-cancel-a-job","title":"How do I cancel a job?","text":"Click to open <p>Use <code>scancel &lt;jobid&gt;</code> where jobid is the number for your job. You can specify multiple jobs in a space-separated list.</p> <p>If your job is a member of a job array, it will have an underscore in it, e.g. 2095_15. You can cancel an array task (2095_15) or the whole job array (2095).</p> <p>If you want to cancel all of your pending jobs, use <code>scancel --me -t PENDING</code>. If you want to cancel all of your jobs, use <code>scancel -u your-username</code></p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-hold-or-modify-a-job","title":"How do I hold or modify a job?","text":"Click to open <p>Note that if you want to modify something about the job, instead of cancelling it and losing any accumulated age priority it has accrued, you can hold the job, modify it and release it with <code>scontrol</code> commands. Only some of the parameters of a job can be modified when it is pending, and even fewer if it has started running.</p> <p><code>scontrol hold &lt;jobid&gt;</code> where  can be a comma-separated list. <p>If your jobs have names, you can hold using the name. (This will hold all matching jobs.) <code>scontrol hold jobname=&lt;jobname&gt;</code> <code>scontrol update jobid=&lt;jobid&gt; ...</code> See this section of the manual page for a list of attributes which can be modified.</p> <p>We have a number of scontrol examples spelled out for you.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-control-the-order-my-jobs-start","title":"How do I control the order my jobs start?","text":"Click to open <p>You can rank your jobs with the \"nice\" and \"top\" subcommands to <code>scontrol</code>.  See the <code>scontrol</code> tips document mentioned above. You can also submit jobs with dependency directives and also create heterogenous jobs which spawn other jobs. </p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#unable-to-allocate-resources","title":"\"Unable to allocate resources\"","text":"Click to open <p>If the scheduler determines that your job is invalid in some fashion, it will generally reject it immediately instead of putting it into the queue with a pending status. There are a few causes of this. The wording of the error may or may not be clear.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#users-group-not-permitted-to-use-this-partition","title":"User's group not permitted to use this partition","text":"Click to open <p>Some of our PI partitions have UNIX groups defined to control who can submit jobs to them. If you are not a member of that group and try to submit a job, you'll see an error like this:</p> <p><code>srun: error: Unable to allocate resources: User's group not permitted to use this partition</code></p> <p>You can see which groups you belong to with the <code>groups</code> command.</p> <p>You can see which group you need to belong to by looking for the partition in question in the file /etc/slurm/partitions.conf</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#job-violates-accountingqos-policy","title":"Job violates accounting/QOS policy","text":"Click to open <p>If you ask for more resources than will ever be able to be allocated to you, you might receive one of several error messages.</p> <p>This one appeared when a job asked for more RAM than was allowed by a QOS limit:</p> <p><code>Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)</code></p> <p>But when more CPUs were requested (instead of too much RAM), the error was different:</p> <p><code>Unable to allocate resources: More processors requested than permitted</code></p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-many-jobs-can-i-run-at-a-time","title":"How many jobs can I run at a time?","text":"Click to open <p>As of 20240220 there are few limits.</p> <p>Array jobs are limited to 15,000 tasks by variable <code>max_array_tasks</code> in /etc/slurm/slurm.conf</p> <p>Total jobs at a single time is 90,000, which is determined by the variable <code>MaxJobCount</code> in /etc/slurm/slurm.conf</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#srun-error-_half_duplex","title":"srun: error: _half_duplex","text":"Click to open <p><code>srun: error: _half_duplex: read error -1 Connection reset by peer</code></p> <p>We consider this a SLURM bug. It appears when <code>srun</code> is used with the <code>--x11</code> argument. Sometimes immediately, but typically when an X11 program is launched.</p> <p>If you do not intend to run any X11 programs during your interactive session, then you can log out of that session and start a new one without the <code>--x11</code> flag.</p> <p>If you do intend to use X11 programs, when that error appears the only solution we have found is to abandon that whole login session to the compute node by typing \u201cexit\u201d to quit the shell. Back on the login node, verify that basic X11 functionality works by starting either the xterm or xclock programs. If that works, start a new srun command to get back onto a compute node. Once on a compute node, verify that basic X11 functionality works by starting either the xterm or xclock programs. If they did, then try your X11 program again.</p>","tags":["slurm"]},{"location":"slurm/slurm/","title":"SLURM","text":"<p>JHPCE has used the SGE (Sun Grid Engine) for many years. We are changing to SLURM (Simple Linux Utility for Resource Management).  The SGE codebase is not actively maintained, and the newest version is about 10 years old at this point. SLURM on the other hand is more widely used, with regular patches and updates made available.</p> <p>SLURM and SGE are conceptually similar, with the notion of \u201cjobs\u201d, \u201cnodes\u201d, \u201cpartitions\u201d (known as \u201cqueues\u201d in SGE), and resource allocation for RAM and cores. However the commands and options between the two schedulers are different.  An orientation to using SLURM on the JHPCE cluster is available, and we will be providing training sessions for end users as we get closer to the cutover date. There are also documents and example code files in /jhpce/shared/jhpce/slurm on the test nodes.</p>"},{"location":"slurm/slurm/#links","title":"Links","text":"<p>GPUs on the JHPCE Cluster under SLURM</p>"},{"location":"slurm/tips-sacct/","title":"sacct useful command examples","text":"<p>Example</p> Show my failed jobs between noon and now<pre><code>sacct -s F -o \"user,jobid,state,nodelist,start,end,exitcode\" -S noon -E now\n</code></pre> <p>sacct is a command used to display information about jobs. It has a number of subtleties, such as the time window reported on and the formatting of output. We hope that this page will help you get the information you need.</p> <p>sacct can be used to investigate jobs' resource usage, nodes used, and exit codes. It can point to important information, such as jobs dying on a particular node but working on other nodes.</p> <p>sacct will show all submitted jobs but cannot, of course, provide data for a number of fields until the job has finished. Use the sstat command to get information about running programs. \"Instrumenting\" your jobs to gather information about them can include adding one or more sstat commands to batch jobs in multiple places.</p> <p>Tip</p> <p>Much of the information on this page can be used with <code>sstat</code>, but there are differences, particularly in available output fields.</p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#sacct-basics","title":"sacct basics","text":"<ol> <li>By default only your own jobs are displayed. Use the <code>--allusers</code> flag if necessary.</li> <li>Only jobs from a certain time window are displayed by default. The window varies depending the arguments you provide. See this section of the manual page. It is recommended to always provide start (<code>-S</code>) and end (<code>-E</code>) times.</li> <li>You can choose output fields and control their width. </li> <li>Even the simplest of batch jobs contain multiple \"steps\" as far as SLURM is concerned. One of them, named \"extern\" represents the ssh to the compute node on behalf of your job. Job records consist of a primary entry for the job as a whole  as  well as entries for job steps. The Job Launch page has a more detailed description of each type of job step. You may find the <code>-X</code> flag helpful to omit clutter.</li> <li>Regular jobs are in the form: JobID[.JobStep]</li> <li>Array jobs are in the form: ArrayJobID_ArrayTaskID</li> </ol> <p>Warning</p> <p>Sacct retrieves data from a SQL database. Be careful when creating your sacct commands to limit the queries to the information you need. Narrow the search as much as possible.  That database needs to be modified constantly as jobs start and complete, so we don't want it tied up answering sacct queries. If you want to look at a large amount of data in a variety of ways, consider saving the output to a text file and then working with that file.</p>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#command-options-of-note","title":"Command Options of Note","text":"<p>Check the man page. There are other useful options.</p> <ul> <li><code>-X</code>  show stats for the job allocation itself, ignoring steps (try it)</li> <li><code>-R</code> reasonlist  show jobs not scheduled for given reason</li> <li><code>-a</code>  allusers</li> <li><code>-N</code> nodelist  only show jobs which ran on this/these nodes</li> <li><code>-u</code> userlist  only show jobs which ran by this/these users</li> <li><code>--name=</code>namelist - only show jobs with this list of names</li> <li><code>-n</code>  noheader</li> <li><code>-p</code>  parsable  puts a | between fields and at end of line</li> <li><code>-P</code>  parsable2  does not put a | at end of line</li> <li><code>--delimeter</code>  - use that char instead of | for <code>-p</code> or <code>-P</code> <li><code>--units=[KMGTP]</code> - display in this unit</li> <li><code>-k</code> minimum time - looking for jobs with time limits in a range</li> <li><code>-K</code> maximum time - looking for jobs with time limits in a range</li> <li><code>-q</code> qoslist - list of qos used</li>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#start-and-end-times","title":"Start and End Times","text":"<p>It is best to use always specify a <code>-S</code> start time and a <code>-E</code> end time.</p> <p>Special time words: today, midnight, noon, now</p> <p>now[{+|-}count[seconds(default)|minutes|hours|days|weeks]]</p> Examples: <code>now-3day</code> <code>now-2hr</code> <p>Valid time formats are:</p> <pre><code>               HH:MM[:SS][AM|PM]\n               MMDD[YY][-HH:MM[:SS]]\n               MM.DD[.YY][-HH:MM[:SS]]\n               MM/DD[/YY][-HH:MM[:SS]]\n               YYYY-MM-DD[THH:MM[:SS]]\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#job-state-values","title":"Job State Values","text":"<p>Using the <code>-s &lt;state&gt;</code> option, you can prune your search by looking for only jobs which match the state you need, such as F for failed. (All of these work: f, failed, F, FAILED)</p> <p>Warning</p> <p>Different steps of a job can have different end states. For example the \"extern\" step is often COMPLETED when the \"batch\" and overall steps are FAILED</p> <p>See this section of the manual page, which has also been saved to a text file you can copy for your own reference <code>/jhpce/shared/jhpce/slurm/docs/job-states.txt</code></p> <p>Primary job states of interest:</p> <ul> <li>CA CANCELLED</li> <li>CD COMPLETED</li> <li>F FAILED</li> <li>OOM OUT_OF_MEMORY</li> <li>PD PENDING</li> <li>R RUNNING</li> <li>TO TIMEOUT</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#available-fields","title":"Available fields","text":"<p>Field meanings are explained in this section of the manual page.</p> What output fields are available?<pre><code>sacct -e\n</code></pre> See all fields for a job<pre><code>sacct -o ALL -j &lt;jobid&gt;\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#formatting-fields","title":"Formatting fields","text":"<p>You can put a %NUMBER after a field name to specify how many characters should be printed, e.g.</p> <ul> <li>format=name%30 will print 30 characters of field name right justified.  </li> <li>format=name%-30 will print 30 characters left justified.</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can define environment variables in your shell to reduce the complexity of issuing sacct commands. You can also set these in shell scripts. Command line options will always override these settings.</p> <p>SACCT_FORMAT</p> <p>SLURM_TIME_FORMAT</p>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#formatting-datestimes","title":"Formatting Dates/Times","text":"<p>You can use most variables defined by the STRFTIME(3) system call. This web page is a starting point, but what SLURM has chosen to implement may not match.</p> <ul> <li>%a - abbrieviated name of day of the week</li> <li>%m - month as decimal, 01 to 12</li> <li>%d - day of month as decimal</li> <li>%H - hour as decimal in 24-hour notation</li> <li>%M - minute as decimal, 00 to 59</li> <li>%T - time in 24-hour notation (%H:%M:%S)</li> </ul> <p>Day of week MM-DD HH:MM<pre><code>export SLURM_TIME_FORMAT=\"%a %m-%d %H:%M\" \n</code></pre> The start and end field widths show below are suitable for the time format shown above.</p> Resources requested, used<pre><code>export SACCT_FORMAT=\"user,jobid,jobname,nodelist%12,start%-20,end%-20,state%20,reqtres%40,TRESUsageInTot%200\"\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#output-fields-of-interest","title":"Output Fields of Interest","text":"<p>These fields are probably the ones you'll want. See this section of the manual page for the list and their meaning. Capitalization does not matter; it is used for readability.</p> <ul> <li>TRES means Trackable RESources, such as RAM and CPUs.</li> <li>A number of fields (not listed) are available to tell you on which node a maximum occurred. Similarly there are fields to tell you minimum, average and maximum values for some items.</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#basics","title":"Basics","text":"<ul> <li>User</li> <li>JobId</li> <li>JobName</li> <li>Partition</li> <li>State</li> <li>ExitCode</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#times","title":"Times","text":"<ul> <li>Submit</li> <li>Start</li> <li>Elapsed - in format [DD-[HH:]]MM:SS</li> <li>End</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#nodes","title":"Nodes","text":"<ul> <li>AllocCPUS</li> <li>AllocNodes</li> <li>NNodes - number of nodes requested/used</li> <li>NodeList - </li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#resources-requested","title":"Resources Requested","text":"<ul> <li>ReqTRES # this is what you will be billed for</li> <li>ReqNodes</li> <li>ReqCPUS</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#resources-consumed","title":"Resources Consumed","text":"<ul> <li>TRESUsageInTot</li> <li>CPUTime - (elapsed)*(AllocCPU) in HH:MM:SS format</li> <li>MaxRSS - Max resident set of all tasks in job</li> <li>MaxVMSize - Max virtual memory of all tasks in job</li> <li>MaxDiskRead - Number bytes read by all tasks in job</li> <li>MaxDiskWrite - Number bytes written by all tasks in job</li> </ul>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#exit-error-codes","title":"Exit Error Codes","text":"<p>In addition to the job's \"state\", SLURM also records error codes. Unfortunately their Job Exit Codes page doesn't provide a meaning for the numerical values.</p> <p>Error <code>0:53</code> often means that something wasn't readable or writable. For example, job output or error files couldn't be written in the directory in which the job ran (or where you told SLURM to put them with a directive).</p> <pre><code>a guide for exit codes:\n\n0 \u2192 success\nnon-zero \u2192 failure\nExit code 1 indicates a general failure\nExit code 2 indicates incorrect use of shell builtins\nExit codes 3-124 indicate some error in job (check software exit codes)\nExit code 125 indicates out of memory\nExit code 126 indicates command cannot execute\nExit code 127 indicates command not found\nExit code 128 indicates invalid argument to exit\nExit codes 129-192 indicate jobs terminated by Linux signals\nFor these, subtract 128 from the number and match to signal code\nEnter kill -l to list signal codes\nEnter man signal for more information\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-sacct/#diagnostic-arguments","title":"Diagnostic Arguments","text":"<p>These can be useful to double-check what someone actually did.</p> See the full command issued to submit the job<pre><code>sacct -o SubmitLine -j &lt;jobid&gt;\n</code></pre> See batch file used<pre><code>sacct -B -j &lt;jobid&gt;\n</code></pre> Directory used by the job to execute commands<pre><code>sacct -o WorkDir -j &lt;jobid&gt;\n</code></pre> See jobs given a time limit btwn 1min &amp; 1 day<pre><code>sacct -k 00:01 -K 1-0\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-sacctmgr/","title":"sacctmgr useful command examples","text":"<p>Sacctmgr is mostly used by systems administrators. Only they are allowed to make changes.</p> <pre><code># See our currently defined QOS in a readable format\nsacctmgr show qos format=Name%20,Priority,Flags%30,MaxWall,MaxTRESPU%20,MaxJobsPU,MaxSubmitPU,MaxTRESPA%25\n\n# See all users database values\nsacctmgr show user withassoc  | less\n\n# See a particular user's database values\nsacctmgr show user withassoc where name=smburke\n\n# WHO HAS EXTRA QOS shortens the output width\n#  (but needs improvement to align column entries)\nsacctmgr show user withassoc|grep -v \"normal \"|awk '{printf \"%s\\t\\t%s\\t%s\\t\\t%s%\\n\", $1,$3,$4,$7}'\n\n# Add a QOS to a user's existing allowed QOS:\nsacctmgr mod user mmill116 set qos+=high-priority\n# or, you can redefine their whole list\nsacctmgr mod user where name=tunison set qos=normal,shared-200-2\n\n# Remove a QOS from a user's existing allowed QOS:\nsacctmgr mod user where name=tunison set qos-=shared-200-2 # to remove\n\n# How users accounts are created in the sacctmgr database\nsacctmgr -i create user name=$userid cluster=jhpce3 account=jhpce \n\n# How C-SUB users accounts are created in the sacctmgr database on jhpcecms01\nsacctmgr -i create user name=$userid account=generic cluster=cms \n\n# Define a QOS\nsacctmgr add qos job-25run50sub\n# You MUST define these flags for the QOS to work as expected\nsacctmgr modify qos job-25run50sub set flags=DenyOnLimit,OverPartQOS\nsacctmgr modify qos job-25run50sub set MaxJobsPerUser=25 MaxSubmitJobsPerUser=50\n\nsacctmgr modify qos shared-default set MaxTRESPerUser=mem=524288 MaxTRESPerUser=cpu=100\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/","title":"scontrol useful command examples","text":"<p>Scontrol is a command useful to both users and systems administrators.  It is used to display and modify SLURM configuration. It has a number of sub-commands, which take different arguments. See the index section of the manual page for a pointer to the area of interest (what kind of thing you want to see or modify).</p> <p>All  commands and options are case-insensitive, although node names, partition names, and reservation names are case-sensitive. All  commands  and options can be abbreviated to the extent that the specification is unique. </p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#scontrol-for-users","title":"Scontrol for Users","text":"","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#show-things","title":"Show things","text":"<pre><code>scontrol show --details job &lt;jobid&gt;\n</code></pre> <pre><code>scontrol show node &lt;nodename&gt;\n</code></pre> <pre><code>scontrol show partition &lt;partitionname&gt;\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#update-jobs","title":"Update Jobs","text":"<p>You can update many aspects of pending jobs, fewer for running jobs. What follows is only a sample!!! Click here for the complete list.</p>","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#pending-jobs","title":"Pending Jobs","text":"Place one of your jobs ahead of other of your jobs<pre><code>scontrol top &lt;jobid&gt;\n</code></pre> Place one of your jobs ahead or behind other of your jobs<pre><code>scontrol update jobid=&lt;jobid&gt; nice=&lt;adjustment&gt; # larger #s decrease the priority\n</code></pre> <p>Set or modify max # of tasks in an array that execute at same time<pre><code>scontrol update jobid=&lt;jobid&gt; ArrayTaskThrottle=&lt;count&gt;\n</code></pre> Users can change the time limit on their pending jobs. After a job starts to run, only a system administrator can adjust the time.</p> Set max job duration<pre><code>scontrol update jobid=&lt;jobid&gt; TimeMin=&lt;time-specification&gt;\n</code></pre> Hold one of your jobs (to prefer other of your jobs)<pre><code>scontrol hold &lt;job-list&gt;  # Can be comma-separated list of jobids\n</code></pre> Release a held job<pre><code>scontrol release &lt;job-list&gt;  # Can be comma-separated list of jobids\n</code></pre> Lower the priority of one of your jobs (to prefer other of your jobs)<pre><code>scontrol update jobid=&lt;jobid&gt; nice=10\n</code></pre> This is per-node, not per-job. In megabytes<pre><code>scontrol update jobid=&lt;jobid&gt; MinMemoryNode=1024\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#running-jobs","title":"Running Jobs","text":"<p>These can also be used on pending jobs. They're just examples of something you might want to set afterwards.</p> <p>Be notified at 80% of job duration<pre><code>scontrol update jobid=&lt;jobid&gt; mailtype=time_limit_80\n</code></pre> But only if you tell it where to send email<pre><code>scontrol update jobid=&lt;jobid&gt; mailuser=&lt;your-address@jh.edu&gt;\n</code></pre></p>","tags":["done","slurm"]},{"location":"slurm/tips-scontrol/#scontrol-for-systems-administrators","title":"Scontrol for Systems Administrators","text":"Modify debug level<pre><code>scontrol setdebug info # or verbose\n</code></pre> Display running configuration<pre><code>scontrol show config\n</code></pre> Modify a partition<pre><code>scontrol update partitionname=interactive allowqos=normal,interactive-default\n</code></pre> Put a DOWN/DRAIN node back into service<pre><code>scontrol update nodename=compute-112 state=resume reason=\"Fixed sssd problem\"\n</code></pre> Show any reservations<pre><code>scontrol show reservation\n</code></pre> Create a reservation<pre><code>scontrol create reservation starttime=now duration=UNLIMITED user=root,tunison,mmill116,jyang flags=maint,ignore_jobs,NO_HOLD_JOBS_AFTER reservation=resv-name nodes=compute-number\n</code></pre> Delete a reservation<pre><code>scontrol delete reservation=&lt;resv-name&gt;\n</code></pre> Another way to delete a reservation<pre><code>scontrol update reservation=&lt;resv-name&gt; endtime=now\n</code></pre> Add a user to an existing reservation<pre><code>scontrol update reservation=&lt;resv-name&gt; user+=&lt;username&gt;\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/user-guide-collection/","title":"SLURM USER GUIDES","text":"<p>Yale: their page</p> <p>Umich: their page</p> <p>New Mexico State Univ: their page</p> <p>BIH: their page</p> <p>C.E.C.I: their page (they have many good pages)</p> <p>Arctic Univ of Norway: their page</p>","tags":["slurm","in-progress"]},{"location":"slurm/whenstart/","title":"Factors Affecting Job Scheduling","text":"","tags":["done","slurm"]},{"location":"slurm/whenstart/#overview","title":"Overview","text":"<p>One of SLURM's primary functions is to schedule jobs so they run on various nodes. Running jobs only in the order that they are submitted on nodes in hostname order is one approach the scheduler could follow, but it turns out to be inefficient. That also doesn't allow organizations to implement policies to favor some jobs over others. So schedulers like SLURM have incorporated many features over the decades which help make maximum use of the cluster's resources and implement other goals.</p> <p>What are some of the things that go into these decisions?</p> <p>How does it choose which of a set of pending jobs to start in what order on which node and which CPU cores on the node?</p> <p>There are a number of vendor documents which document scheduling. See the \"Workload Prioritization\" and \"Slurm Scheduling\" sections at this site.</p>","tags":["done","slurm"]},{"location":"slurm/whenstart/#tldr","title":"TL;DR","text":"<p>Your jobs will start faster if you request the fewest resources required for their success, including duration. Smaller jobs \"fit\" into more slots between other jobs than larger jobs, so consider whether you can divide up your work. You should also direct your job to the most appropriate partition. For example, we have an interactive partition for small jobs.</p> <p>If the scheduler has been able to determine an estimated start date for your job, it will be shown in the output of</p> Start time estimate<pre><code>squeue --me --start\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/whenstart/#backfill","title":"Backfill","text":"<p>In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 30 cores and it will have to wait 20 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that shorter job in the meantime. This GREATLY enhances utilization of the cluster.</p> <p>For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. </p>","tags":["done","slurm"]},{"location":"slurm/whenstart/#priority","title":"Priority","text":"<p>Multiple factors are used to assign a single priority value to each job. This is described in the Multifacor Job Priority document.</p> <p>(This priority is only used to decide which jobs to dispatch first. It is not used to set a UNIX process <code>nice</code> value on the processes created by jobs out on the compute nodes.)</p> <p>Once a job starts running, its priority no longer has much meaning.</p> <p>The job's priority is an integer that ranges between 0 and 4,294,967,295. The larger the number, the higher the job will be positioned in the queue, and the sooner the job will be scheduled and started. A job's priority, and hence its order in the queue, can vary over time.</p> <p>The final priority is determined by multiplying pairs of (weights and factors) and adding the results. Factors range from 0.0 to 1.0. Weights range from 0 to 65,533.</p> <p>Currently we are using three components: Age, Fairshare and Partition</p> <p>Tip</p> <p>You can see pending job's priority values and the contributors to the final value with the <code>sprio</code> command. This sorts jobs by total prio, partition, user. Pending jobs sorted by priority<pre><code>sprio -S -y,p,u | less\n</code></pre></p> <p>Tip</p> <p>A better formatted of that command which prints only the factors we are currently using<sup>1</sup> is:</p> Pending jobs sorted by priority, well-formatted<pre><code>sprio -o \"%.15i %9r %.8u %.10Y %.10A %.10F %.10P\" -S -y,p,u\n</code></pre> <p>Tip</p> <p>You can change the priority of your jobs among your jobs with <code>scontrol</code> commands like <code>top</code> and <code>nice</code>. See this document for details. </p>","tags":["done","slurm"]},{"location":"slurm/whenstart/#fairshare","title":"Fairshare","text":"<p>To help provide equitable access to the public partitions of the cluster, the FAIRSHARE priority component is based on your recent usage of those partitions. If you have used fewer CPU minutes than someone else in the last week, then your jobs will receive a higher fairshare value.</p> <p>The fairshare priority is the result of multiplying a weight stored in a variable, PriorityWeightFairshare, and a factor which is derived from the accounting database.</p> <p>Tip</p> <p>You inspect fairshare values for ALL users with this command:</p> Fairshare values<pre><code>sshare -a | sort -k7nr | less\n</code></pre> <p>You should focus on the values in the right-hand-most column. Heaviest users of the cluster in recent days have values closer to 0.0. People who haven't run any jobs lately will have values closer to 1.0. Jobs submitted by the latter will be given higher fairshare priority values.</p>","tags":["done","slurm"]},{"location":"slurm/whenstart/#age","title":"Age","text":"<p>In addition to fairshare, any pending job will accrue AGE priority over time. Currently (20240217) this maxes out to 100 over the course of a week.</p> <p>Job arrays which started running tasks many days ago will wind up with high age priority values for all of their future tasks. You can see that fairshare somewhat counteracts that age advantage.</p> <p>If you decide that you want to change something about a pending job, consider whether you can do so using <code>scontrol</code> commands as described here instead of killing the job with <code>scancel</code> and resubmitting it.</p>","tags":["done","slurm"]},{"location":"slurm/whenstart/#partition","title":"Partition","text":"<p>We have set this experimentally on the <code>interactive</code> partition to try to aid in quick access to interactive sessions.</p> <ol> <li> <p>This command's output will be incomplete if we begin using other priority factors.\u00a0\u21a9</p> </li> </ol>","tags":["done","slurm"]},{"location":"storage/backups-restores/","title":"Backups and Restores","text":"","tags":["in-progress"]},{"location":"storage/backups-restores/#caveat","title":"Caveat","text":"<p>We try to protect your data, but ultimately you need to keep copies of your most vital files elsewhere.</p>","tags":["in-progress"]},{"location":"storage/backups-restores/#home-directories","title":"Home directories","text":"<p>Frequency, restore window. Form to request restores. Or just a description of what is needed (what is missing? when did you last see it? where do you want it restored to?)</p>","tags":["in-progress"]},{"location":"storage/backups-restores/#self-service-restores","title":"Self Service Restores","text":"<p>We make snapshots of the /users file system for fourteen days. You can restore files you have deleted recently by changing directory to the appropriate location and then copying the file or files back to your home directory (or anywhere else you desire).</p> <p>At any one time there are fourteen subdirectories in the path <code>/users/.zfs/snapshot</code></p> <p>Here is an example of looking through the collection of snapshots to find copies of a file you want to restore. Let's say that you deleted a file inside your home directory named susan that was stored in the absolute path <code>/users/your-userid/bob/frank/susan</code> You can see from the <code>ls -ld</code> output when the file existed and also the size of the possibly various versions of that file across the collection of snapshots (perhaps you changed it several times in the last two weeks).</p> <pre><code>cd /users/.zfs/snapshot\nls -ld */your-userid/bob/frank/susan\ncp -p 2024-02-16-23:00/your-userid/bob/frank/susan $HOME/restored-susan\n</code></pre> <p>If restoring substantial amounts of data, please do that work on a compute node instead of a login node. Thank you.</p>","tags":["in-progress"]},{"location":"storage/backups-restores/#project-space","title":"Project space","text":"<p>We offer optional backup service.</p> <p>What's included. Frequency, restore window, cost.</p> <p>Limit on what we'll consider (ONLY whole file systems?)</p> <p>Form to request restores. Or just a description of what is needed.</p>","tags":["in-progress"]},{"location":"storage/buying-in/","title":"How to get more space","text":""},{"location":"storage/buying-in/#home-directories","title":"Home Directories","text":"<p>Policies on how much we can give you, for how long.</p>"},{"location":"storage/buying-in/#project-space","title":"Project space","text":""},{"location":"storage/buying-in/#whats-available-to-get-now","title":"What's available to get now?","text":"<p>Costs, whom to contact.</p> <ul> <li>Un-allocated space - spinning disks</li> <li>Un-allocated space - SSD</li> </ul>"},{"location":"storage/buying-in/#future-servers","title":"Future servers","text":"<p>When will the next chance come to buy a large amount of space? How do you put in a request so we can plan?</p>"},{"location":"storage/fastscratch/","title":"Fastscratch","text":"<p>A 22TB file system created from fast Solid State Disk is available for your use. This provides a fast place to store input or output files for your compute jobs. There is no cost for using your personal scratch space.</p> <p>Note</p> <p>This scratch space is meant to be a short-term storage location; it is not a long-term storage solution.</p> <p>You can access your scratch space by using the <code>$MYSCRATCH</code> environment variable from an interactive cluster node session, or within a submitted job.</p> <p>The actual absolute path to your personal scratch space is <code>/fastscratch/myscratch/$USER</code>.</p>","tags":["done"]},{"location":"storage/fastscratch/#key-details","title":"Key details","text":"<p>Because this limited resource is shared by all users, there are some very important restrictions for using it.</p> <ul> <li>There is a 1TB quota set on the personal scratch space. (See this document for more information about disk quotas.)</li> <li>All files older than 30 days will be removed without exception.  </li> <li>Even though there is a 30 day automatic deletion of data, we ask that you please remove data from your personal scratch space once you have finished using it.</li> <li>The personal scratch space is not backed up. Therefore if you delete a file it cannot be recovered.</li> <li>The fastscratch file system has NO redundancy, so scratch space drive failures result in data loss. Thus, move important files needed to be kept from scratch space ASAP.</li> <li>Abuse of this space may result in files being deleted on an as needed basis.</li> </ul> <p>Danger</p> <p>If you <code>untar</code> or <code>unzip</code> a file, and the extracted files have a timestamp older than 30 days from the original bundle, they will be removed when the daily purge begins. To work around this, you can use the <code>touch</code> command to update the timestamp on the extracted files.</p>","tags":["done"]},{"location":"storage/quotas/","title":"Disk Quotas","text":"<p>Disk quotas are used to control disk space for certain file systems. We use \"hard\" quotas. You are not allowed to use more than your quota.  </p> <p>Danger</p> <p>Reaching your disk quota can become an obstacle of simply logging in, as even a small file needed to record some detail about your login session, such as $HOME/.Xauthority, cannot be created. Keep your usage below your quota cap.</p> <p>We use ZFS file systems for large volumes. Unfortunately, ZFS does not provide an end-user quota command with which to inspect your usage and remaining space.</p> <p>Therefore we have configured our login nodes to display your home directory disk consumption and quota during the login process.</p> <p>The figure shown during login are updated periodically. Every 30 minutes to an hour.</p> <p>Tip</p> <p>We have written a <code>getquota</code> command, which will look up your or someone else's quota.</p>","tags":["done"]},{"location":"storage/quotas/#home-directory","title":"Home Directory","text":"<p>In the JHPCE cluster, this quota is set to 100GB.</p> <p>In the C-SUB cluster, this quota is set to 500GB.</p>","tags":["done"]},{"location":"storage/quotas/#file-deletion-and-delayed-change-in-quota","title":"File Deletion and Delayed Change in Quota","text":"<p>When you delete files you may not see an immediate change in your disk consumption as far as the disk quota system is concerned.</p> <p>You can see how much space you are using in your home directory with the commands</p> <pre><code>cd\ndu -sh .\n</code></pre> <p>We use ZFS snapshots for home directories to make automated backups once an day<sup>1</sup>. These are kept for a period of time<sup>2</sup> so users and systems administrators can perform restores. See this document for instructions on performing your own restores!!!</p> <p>Snapshots work by making a record of your files at an instant in time.  They take zero space at first. As your files change, disk space is consumed to hold the changed material. Snapshot consumption is counted as part of your disk quota.</p> <p>Therefore it can take a number of days<sup>2</sup> for files you have changed in the past but now deleted to stop being counted against your quota.</p> <p>As you can imagine, the rate of change and size of files involved determines the amount of space held in snapshots.</p> <p>If you find yourself in the position where you have run into your disk quota limit, have deleted significant amounts of files but are still impacted by your snapshot'ed files, please email us at bitsupport with the details. We will give you a temporary increase in disk quota to accomodate the snapshot \"overhang\"</p>","tags":["done"]},{"location":"storage/quotas/#fastscratch","title":"Fastscratch","text":"<p>The /fastscratch file system has a 1TB quota per user.</p> <p>We have defined in the standard environment a variable <code>$MYSCRATCH</code> for users to use to access their space. (The actual absolute path to your personal scratch space is <code>/fastscratch/myscratch/$USER</code>)</p> <p>There is no reporting system currently available to display fastscratch disk usage and your quota. You can use these commands to view your current usage:</p> <pre><code>cd $MYSCRATCH\ndu -sh .\n</code></pre>","tags":["done"]},{"location":"storage/quotas/#fastscratch-automated-cleaning","title":"Fastscratch automated cleaning","text":"<p>Files which have not been modified in the last 30 days are automatically deleted by a daily process.</p> <p>If you extract files from an archive and maintain their original creation date, they may be deleted sooner than you expect. </p> <ol> <li> <p>At eleven pm (as of 20240215).\u00a0\u21a9</p> </li> <li> <p>Fourteen days (as of 20240215).\u00a0\u21a9\u21a9</p> </li> </ol>","tags":["done"]},{"location":"storage/stg-overview/","title":"STORAGE OVERVIEW","text":"<p>Warning</p> <p>Document under heavy-duty construction</p> <p>Previous stg page has some text that should move into this one.</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#types-of-storage","title":"Types of storage","text":"<p>There are three basic categories of data storage you can write to.</p> Type Example Path Quota? Use Home directory /users/yourusername Yes Store your environment Project space /dcs07/grpname/data Yes Research data Scratch space /tmp/ No Application temporary files <p>Characteristics, best uses of each.</p> <p>Mention local versus network.</p> <p>(/tmp is a local example. Are we going to keep /scratch in existence? See Github 774.)</p> <p>Speed differences between SSD (dcs06, fastscratch) and spinning.</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#specific-file-systems-you-need-to-know-about","title":"Specific File Systems You Need To Know About","text":"<p>Make a table for each of the three types of storage. Instead of one large table. Be consistent in dealing with the three types of stg.</p> <p>In each table, include</p> <ul> <li>Name or form (/dcs0N/group/data)</li> <li>the environment variable you can use to refer to it, e.g. $HOME $FASTSCRATCH (JRT doesn't know of a SLURM variable for where your job starts. There's SLURM_SUBMIT_DIR but it doesn't reflect any usage of --chdir)</li> </ul>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#dont-fill-up-tmp","title":"Don't fill up /tmp","text":"<p>What about compute node's /tmp? Users need to know about it, because they may not know that software often writes there, and they need to understand that it is a shared limited resource. if they are explicitly writing to /tmp they need to know to limit their usage and clean up after themselves</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#home-directories","title":"Home directories","text":"<p>How to learn how much you're using. How often updated? Disk quotas - only hard, no soft (so no warning) Issue with snapshots preventing you from being able to immediately reduce usage. Ask for increase in disk quota while overhang is being deleted? See this document to request increases in /users quota?</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#buying-additional-storage-space","title":"Buying additional storage space","text":"<p>Periodic offerings. How paid for (and therefore length of committment)</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#backing-up-storage","title":"Backing up storage","text":"<p>You need to ensure that you have copies of your most vital files located somewhere else. See this document for more information.</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#getting-information-about-storage","title":"Getting information about storage","text":"<ul> <li>du -sh</li> <li>df -h</li> <li>df -h .</li> </ul> <p>Consider potential impact of running certain commands that will cause lots of I/O. Not crossing file system boundaries is an option found on a number of UNIX commands that is good to use.</p>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#just-yanked-out-of-knowledgebase-doc-put-somewhere","title":"JUST YANKED OUT OF KNOWLEDGEBASE DOC - PUT SOMEWHERE","text":"","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#disk-storage-space-on-the-jhpce-cluster","title":"Disk Storage Space on the JHPCE Cluster","text":"<ul> <li>There are several types of storage on the JHPCE cluster. </li> <li>Some space is for permanent storage of files, and other spaces can be used for short term storage of data.</li> <li>For long term storage of files, most users make use of their 100GB of space in their home directory. </li> <li>For those groups needing more, we have large storage arrays and sell allocations on these large arrays. Storage docs.</li> <li>We build a new storage array about every 18 months; so if you are interested in purchasing an allocation please email bitsupport@lists.jhu.edu. </li> <li>In addition to these long-term storage offerings, there are scratch space areas for short-term data storage. <ul> <li>Scratch space tends to be faster. Using scratch space avoids taking up space in your home or project storage space.</li> </ul> </li> <li>The best area used for scratch is the <code>fastscratch</code> array on the cluster. The fastscratch array provides 22TB of space that is built on faster SSD drives, vs traditional hard drives used for project space and home directories. All users have a 1TB quota for scratch space, and data older than 30 days is purged.</li> <li>Traditionally in Unix/Linux, <code>/tmp</code> or <code>/var/tmp</code> directories are used for storing temporary files. On the JHPCE cluster, this is strongly discouraged as these directories are small. </li> <li>In R, the <code>tmpdir()</code> setting will dictate where temporary files are stored. If you are generating 10s of GB of temporary files, change <code>tmpdir()</code> to <code>fastscratch</code>.</li> <li>In SAS, the default <code>WORK</code> directory will be located under your <code>fastscratch</code> directory.</li> <li>In Stata, the default <code>tempfile</code> location is under <code>/tmp</code>. This can be changed by setting the <code>STATATMP</code> environment variable.</li> </ul>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/stg-overview/#encrypted-filesystem","title":"Encrypted filesystem","text":"<p>Hopefully</p> <p>We can just delete this info because we're getting rid of Lustre ASAP</p> <ul> <li>The JHPCE Cluster currently supports the following mechanisms for using encrypted filesystems.</li> <li>Encrypted filesystem are used to provide \u201cEncryption At Rest\u201d, meaning that the data on disk will be safely stored in an encrypted format, and only available in an unencrypted state when the data is accessed by an approved user.</li> <li>Userspace encrypted filesystems using <code>encfs</code> <code>ZFS/Lustre</code> encrypted filesystems.</li> <li>The <code>DCL02</code> storage array is built on encrypted disk devices.</li> </ul>","tags":["topic-overview","needs-major-revision"]},{"location":"storage/storage/","title":"Old Stg Page","text":"<p>Test 2</p>"},{"location":"storage/storage/#current-storage-offerings","title":"Current Storage Offerings","text":"<p>Test1</p> <p>There are 2 main categories of storage space available for purchase on the JHPCE cluster, detailed below:</p> <ol> <li> <p>Pay-as-you-go space: This includes home directory space, legacy storage space, and leased spaces. Users are charged only for the actual space used and the time data is stored there. For example, using 10 TB of <code>/dcl01/leased</code> space for a year costs $500/year.</p> </li> <li> <p>Project spaces: These are large storage arrays built approximately every 18 months, funded by various labs purchasing allocations on the storage array. For example, the buy-in cost for <code>/dcl02</code> was $43/TB, with a $300/year storage management fee for a 10TB allocation.</p> </li> </ol> <p>Pay-as-you-go spaces are generally more expensive than project spaces due to their smaller size, upfront costs being included in the annual fee, and higher maintenance requirements.</p> <p>Additionally all users have access to 1TB of \"fastscratch\" storage for storing files less than 30 days.  More information on using fastscratch can be found at [/jhpce_mkdocs/knowledge_base/#fastscratch] - Personal Scratch space: A network-based filesystem with a backend NVME based storage array, intended for short-term storage of large files.</p> <p>Off-site backup space is also available, with <code>/users</code> directory currently backed up, and other directories backed up upon request.</p> <p>For inquiries about purchasing storage, please email bitsupport@lists.jhu.edu.</p>"},{"location":"storage/storage/#current-offerings","title":"Current Offerings","text":"Type Location Env Var Capacity Quota Lifetime FY21Q3 Rate Charge Basis Notes Home ZFS /users/ $HOME 34TB 100GB Long $345/TByr Used TB - Temp scratch /scratch/temp/ $TMPDIR 500GB-4TB None Transient Free - [1][2] Personal scratch /fastscratch/myscratch/ $MYSCRATCH 22TB 1TB 30 days Free - [1][3] Leased Lustre /dcl02/leased - 200TB As agreed Intermediate $50/TByr Used TB - Project ZFS /dcs04, /dcs05, /dcs07 - 5,000TB As purchased Long $20/TByr + buyin Purchased TB [4] Backup ZFS varies - 2,675TB - Permanent $11/TByr Purchased TB [5] <p>Notes: - [All] Rates fluctuate slightly from quarter to quarter based on actual JHPCE expenses and capacities. - [1] Scratch space is only visible on compute and transfer nodes. Scratch is not visible on the login node. <code>&lt;JQT&gt;</code> = 'job.queue.task'. - [2] Scratch space varies from node to node based on local disk space. - [3] Currently there is a 1TB quota on files in /fastscratch/myscratch, with a 30-day file retention limit. - [4] The buy-in cost for DCS04 is $42/TB. Limited capacity still available for sale. - [5] Backups are done of the /users filesystem. Other filesystems may be backed up upon arrangement with PI.</p>"},{"location":"storage/storage/#previouslegacy-offerings","title":"Previous/Legacy Offerings","text":"<p>The storage spaces listed below are currently in use but are no longer available for purchase.</p> Type Location Env Var Capacity Quota Lifetime FY2019Q2 Rate Charge Basis Notes Leased Lustre /dcl01/leased - 200TB As agreed Intermediate $50/TByr Used TB - Leased ZFS /legacy - 100TB From legacy Short &lt; $1350/TByr Used TB [1] Leased ZFS /starter/starter-02 - 10TB 10TB Short $1,041/TByr Used TB [1] Project Lustre /dcl01 - 3,400TB As purchased Long $26-$ <p>There are 2 main categories of storage space available for purchase on the JHPCE cluster, detailed below:</p> <ol> <li> <p>Pay-as-you-go space: This includes home directory space, legacy storage space, and leased spaces. Users are charged only for the actual space used and the time data is stored there. For example, using 10 TB of <code>/dcl01/leased</code> space for a year costs $500/year.</p> </li> <li> <p>Project spaces: These are large storage arrays built approximately every 18 months, funded by various labs purchasing allocations on the storage array. For example, the buy-in cost for <code>/dcl02</code> was $43/TB, with a $300/year storage management fee for a 10TB allocation.</p> </li> </ol> <p>Pay-as-you-go spaces are generally more expensive than project spaces due to their smaller size, upfront costs being included in the annual fee, and higher maintenance requirements.</p> <p>Additionally all users have access to 1TB of \"fastscratch\" storage for storing files less than 30 days.  More information on using fastscratch can be found at [/jhpce_mkdocs/knowledge_base/#fastscratch] - Personal Scratch space: A network-based filesystem with a backend NVME based storage array, intended for short-term storage of large files.</p>"},{"location":"sw/adding-pkgs/","title":"Adding Your Own Python and R Libraries","text":"","tags":["needs-to-be-written"]},{"location":"sw/adding-pkgs/#how-to-add-your-own-packages-to-r-and-python","title":"How to add your own packages to R and python","text":"<p>Authoring Note</p> <p>Should this document be split into ones for R and Python?</p> <p>I think our user community would welcome instructions on how to add pkgs to R and python, at the minimum.</p> <p>There are right and wrong ways to do this.</p> <p>There are implications of loading modules in which order before doing things.</p> <p>Do you create a conda environment first, or a python virtual environment?</p> <p>WHAT DO YOU DO WHEN YOU WANT TO RUN A DIFFERENT VERSION OF A LIBRARY THAT IS FOUND IN CONDA_R?</p> <p>What information do other HPC sites deem important to tell their users? Are there examples among the pages for listed TOWARDS THE BOTTOM in this github issue?</p> <p>A GOOD EXAMPLE Example from C.E.C.I</p>","tags":["needs-to-be-written"]},{"location":"sw/adding-pkgs/#r","title":"R","text":"<p>FIRST YOU MUST LOAD R MODULE <code>module load R</code></p> <p>If you try to load a library and it is not found, then you can install one in your home directory.</p> <pre><code>&gt; install.packages(\"sf\")\n&gt; install.packages('terra', repos='https://rspatial.r-universe.dev')\n&gt; library('sf')\n&gt; library('terra')\n</code></pre> <p>You should recompile your libraries when the version of R changes.</p> <p>How do you recompile instead of install?</p> <p>When installing R packages from source with compiled programs, you can add custom compiler flags in ~/.R/Makevars. Adding optimization flags may provide a boost in performance for some packages.  <pre><code>STDFLAGS = -O2 -pipe -Wall \n</code></pre></p> <p>We have a wide variety of CPU architecture across the cluster, so you probably don't want to add to STDFLAGS <code>-march=</code> and <code>-mtune=</code> arguments.</p>","tags":["needs-to-be-written"]},{"location":"sw/adding-pkgs/#python","title":"Python","text":"<p>C.E.C.I has a page with what I think might be useful info:</p> <p>It is important when you install a package that you load the correct Python module, and use the Pip option --no-binary :all: to recompile from source rather than install pre-compiled binaries whenever possible. See more information in the PIP documentation </p>","tags":["needs-to-be-written"]},{"location":"sw/building-from-src/","title":"Compile from Source","text":"<p>Authoring Note</p> <p>Our younger users haven't necessarily ever built anything from source. Or on machines owned by someone else where you cannot use sudo dnf install blah. We can encourage them that it is possible, give them a few pointers to doing it successfully.</p> <p>For example, telling them that they will have to modify the destination location from the typical /usr/local/bin and that you can usually do that easily in the config stage with <code>--config=</code></p> <p>Please do it on a compute node in an interactive shell.</p> <p>If we don't write something, then we can provide pointers to the work of others. If so, that would probably be inside a larger document.</p> <p>Software installation with Conda</p> <p>Example building from source page</p>"},{"location":"sw/building-from-src/#please-visit-the-links-below-poke-around-to-nearby-topics-on-those-site-pages-if-you-were-a-new-jhpce-user-and-new-to-unix-and-hpc-what-would-you-like-to-see-using-these-examples","title":"**Please visit the links below, poke around to nearby topics on those site pages. If you were a new JHPCE user and new to UNIX and HPC, what would you like to see, using these examples?","text":"<p>**</p> <p>Maybe the info JRT has collected below about toolchains, maybe these particular URL compared to others on those sites, should be inserted into a github issue and considered later. JRT wonders if we should begin to adopt the practice of being more specific about creating and using toolchains.</p> <p>Note that these clusters provide modules with toolchain information. AFAIK we've never named our modules with toolchain information (with which compiler they were built). Does ARCH? A lot of this seems to be Intel vs gcc. But wouldn't it help us keep our modules updated if we embedded the compiler generation they were built with into their name?</p> <p>These places give date names to collections of tools. Yale's toolchain page has an illustration of \"foss\" versus \"foss-cuda\"</p> <p>Example from C.E.C.I</p> <p>Another example for frontera cluster Very nice info to help you optimize your code. Advice about locality, I/O performance, machine learning etc nearby in sidebar.</p> <p>Another example from Yale warns users about considering node CPU capabilities. (So does Frontera where you get practical info about gcc -march and -mtune)</p> <p>The C.E.C.I folks are surprisingly opposed to people using Anaconda. I guess this is specifically about the full Anaconda versus Miniconda?????</p>"},{"location":"sw/conda/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"sw/sw-stub.md\"</p>","tags":["needs-to-be-written","jiong"]},{"location":"sw/containers/","title":"Containers","text":"","tags":["needs-to-be-written","mark"]},{"location":"sw/containers/#using-our-container-solutions","title":"Using Our Container Solutions","text":"<p>We have two solutions that you can use.</p> <p>Maybe this is a good time to at least get credit for the work Mark has done building the singularity containers we now offer.</p> <p>What do you need to know about using them? Quitting them? Waiting how long for them to finish starting?</p> <p>What verbiage about this can be shared with Web Portal docs?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers/#building-your-own-containers","title":"Building Your Own Containers","text":"<p>Maybe we can document this later in terms of how users can build them. In the mean time, provide some links??</p> <p>They don't have root access, so what is that trick that allows you to make a container image?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers/#how-do-you-run-a-batch-slurm-job-using-a-container","title":"How do you run a batch SLURM job using a container?","text":"<p>Where is the best file system to host a container file if you're going to run it in batch mode on multiple nodes? /fastscratch? Does it make a difference?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers/#examples-from-elsewhere","title":"Examples from Elsewhere","text":"<p>The Frontera site's section on containers mentions something that users building their own containers on JHPCE need to know to add to their containers -- e.g. which file systems to include.</p> <p>NERSC has a well-written set of pages about using podman-hpc, which might serve as an example of just building and using containers even without podman-hpc</p> <p>Zurich pages on Singularity and Recipes and Hints</p> <p>USC pages on Singularity</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/gui-tools/","title":"Helpful GUI Programs","text":"<p>This chart is C-SUB specific.</p> <p>JRT thinks JHPCE users would benefit from seeing a chart that is correct for that cluster. (A Github issue exists about considering installing some of these C-SUB-only tools onto main cluster.)</p>","tags":["needs-major-revision"]},{"location":"sw/jupyter/","title":"Jupyter","text":"<p>Everything you need to know to get started using Jupyter and related friends.</p>","tags":["needs-to-be-written"]},{"location":"sw/jupyter/#using-the-web-portal","title":"Using the Web Portal","text":"<p>Authoring Note</p> <p>Put here information about aspects of using this tool. Perhaps a warning that it can take some time to launch.</p> <p>Please see this document about this service.</p>","tags":["needs-to-be-written"]},{"location":"sw/matlab/","title":"Matlab","text":""},{"location":"sw/matlab/#running-basic-matlab-jobs-over-slurm","title":"Running basic Matlab jobs over SLURM","text":""},{"location":"sw/matlab/#-using-matlab-interactively","title":"- Using Matlab interactively","text":"<pre><code>[test@login31 ~]$ srun --mem 10G --x11 --pty bash\nsrun: job 3109996 queued and waiting for resources\nsrun: job 3109996 has been allocated resources\n[test@compute-152 ~]$ module load matlab\n[test@compute-152 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) matlab/R2023a\n\n[test@compute-152 ~]$ matlab\n</code></pre>"},{"location":"sw/matlab/#-using-matlab-in-batch-mode","title":"- Using Matlab in batch mode","text":"<ol> <li> <p>Write your matlab source in a file with .m extension (e.g. matlab_example.m) <pre><code>x = [1 2 3 4];\nfprintf('Example number = %i\\n', x)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. matlab_example.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=5:00\n\n#SBATCH -o slurm.%N.%J.%u.out   # STDOUT\n#SBATCH -e slurm.%N.%J.%u.err   # STDERR\n\nmodule load matlab\nmatlab -nojvm -nodisplay -r \"matlab_example;quit;\"\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch matlab_example.sh \nSubmitted batch job 3110505\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3110505    shared matlab_e    test  R       0:03      1 compute-100\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 12 17:09 slurm.compute-100.3110505.test.err\n-rw-r--r-- 1 test test 410 Mar 12 17:10 slurm.compute-100.3110505.test.out\n</code></pre></p> </li> <li> <p>Look at the results from output file, file with stderr is empty(job without errors): <pre><code>[test@login31 ~]$ cat slurm.compute-100.3110505.test.out\n                                May 25, 2023\n\nFor online documentation, see https://www.mathworks.com/support\nFor product information, visit www.mathworks.com.\n\nExample number = 1\nExample number = 2\nExample number = 3\nExample number = 4\n</code></pre></p> </li> </ol>"},{"location":"sw/modules/","title":"Environment modules","text":"<p>Authoring Note</p> <p>The last section needs to be updated for 2024. LIBD is not mentioned, documentation is not linked correctly anymore, etc. Convert to a Markdown table?</p>","tags":["needs-review","jiong"]},{"location":"sw/modules/#introduction","title":"Introduction","text":"<p>The JHPCE cluster uses the Lmod module system to allow users to configure their shell environments. Some applications will not run until you load the corresponding modulefile. </p> <p>A handful of widely used modulefiles are loaded by default when you log into the cluster. To see what modules are loaded you can enter the following command at the shell prompt:</p> <p><pre><code>module list\n</code></pre> The naming convention for modules is \"software_name/version\" (e.g. bowtie/2.5.1). If a module is loaded, it will be followed by <code>(L)</code> The default version, if designatedk will be followed by a <code>(D)</code></p> <p>Modules cure the the age-old headaches associated with configuring paths, environment variables and different software versions. For example, gcc or open64 compilers need different libraries. Some users need python 3.9 while other users need python 3.10. Some users want a standard stable R, while some want the latest and greatest development version of R that was compiled the night before.</p>","tags":["needs-review","jiong"]},{"location":"sw/modules/#basic-module-commands-for-users","title":"Basic module commands for users","text":"<p>A module file is a script that sets up the paths and environment variables that are needed for a particular application or development environment. Most users will just use our modulefiles. But if you want to finely control your shell environment, you can start developing your own custom module files. </p> <p>There are a few basic commands that users should know:</p> <pre><code>module list                 # list your currently loaded modules\nmodule load   &lt;MODULEFILE&gt;  # configures your environment according to modulefile \nmodule unload &lt;MODULEFILE&gt;  # rolls back the configuration performed by the associated load\nmodule avail                # shows what modules are available for loading\nmodule help                 # \nmodule spider               # lists all modules, including ones not in your MODULEPATH\nmodule spider &lt;NAME&gt;        # search for a module whose name includes &lt;NAME&gt;\nmodule save &lt;NAME&gt;.         # save currently-loaded modules to \"default\" or optionally to &lt;NAME&gt;\nmodule restore &lt;NAME&gt;.      # load the saved collection\n</code></pre>","tags":["needs-review","jiong"]},{"location":"sw/modules/#defaults","title":"Defaults","text":"<p>By default the following modules are loaded on all compute hosts and the login hosts when you log in</p> <pre><code>JHPCE_ROCKY9_DEFAULT_ENV\nJHPCE_tools/3.0\n</code></pre>","tags":["needs-review","jiong"]},{"location":"sw/modules/#configuring-your-bashrc","title":"Configuring your .bashrc","text":"<p>It is critical that your <code>.bashrc</code> file sources the system-wide bashrc file. Otherwise basic programs will not work! After you source the system-wide bashrc file you can add code which will modify your environment. For example:</p> <pre><code># Always source the global bashrc\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n\n# If I prefer gcc/4.8.1 as my default compiler\nmodule load gcc/4.8.1\n</code></pre>","tags":["needs-review","jiong"]},{"location":"sw/modules/#community-maintained-applications","title":"Community maintained applications","text":"<p>We use a community-based model of application maintenance to support our diverse user base. Briefly, this means that we support essentially few applications as a service center. Instead we encourage and facilitate power users to maintain their tools in a manner that makes their tools available to all users. These users maintain their applications as well as the corresponding modulefiles. Below is a list of applications and application suites that are maintained by community maintainers. Please refer to their documentation for details. Also please be considerate. Maintaining software for you is not their day job. There is absolutely no point in getting bent out of shape if they can\u2019t (or won\u2019t) service your request.</p> <pre><code>Description Maintainer  Documentation\nR   Kasper Hansen   Documentation\nPerl    Jiong Yang  Documentation\nPython  Alyssa Frazee   Documentation\nShortRead Tools Kasper Hansen   Documentation\n</code></pre>","tags":["needs-review","jiong"]},{"location":"sw/python-pkg/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"sw/sw-stub.md\"</p>","tags":["needs-to-be-written","jiong"]},{"location":"sw/python/","title":"Python","text":""},{"location":"sw/python/#running-python-jobs-over-slurm","title":"Running Python Jobs over SLURM","text":""},{"location":"sw/python/#-using-python-interactively","title":"- Using Python Interactively","text":"<pre><code>[test@login31 ~]$ srun --pty bash\nsrun: job 3168742 queued and waiting for resources\nsrun: job 3168742 has been allocated resources\n[test@compute-100 ~]$ module load python\n[test@compute-100 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) python/3.9.14\n\n[test@compute-100 ~]$ python3\nPython 3.9.14 (main, May 16 2023, 14:32:18) \n[GCC 11.3.1 20220421 (Red Hat 11.3.1-2)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n</code></pre>"},{"location":"sw/python/#-using-python-in-batch-mode","title":"- Using Python in batch mode","text":"<ol> <li> <p>Write your python source in a file with .py extension (e.g. python_example.py) <pre><code>values = [88, 47, 96, 85, 72]\nfor i in values:\n    print('Example number = ', i)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. python_example.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\n#SBATCH -o slurm.%N.%J.%u.out   # STDOUT\n#SBATCH -e slurm.%N.%J.%u.err   # STDERR\n\nmodule load python\npython3 python_example.py\n</code></pre></p> </li> <li> <p>Submit your python job <pre><code>[test@login31 ~]$ sbatch python_example.sh \nSubmitted batch job 3169570\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3169570    shared python_e    test R       0:01      1 compute-153\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 13 12:45 slurm.compute-153.3169570.test.err\n-rw-r--r-- 1 test test 105 Mar 13 12:45 slurm.compute-153.3169570.test.out\n</code></pre></p> </li> <li> <p>Look at the results from output file, file with stderr is empty(job without errors): <pre><code>[test@login31 ~]$ cat slurm.compute-153.3169570.test.out\nExample number =  88\nExample number =  47\nExample number =  96\nExample number =  85\nExample number =  72\n</code></pre></p> </li> </ol>"},{"location":"sw/r-n-friends/","title":"R Basics","text":"<ul> <li>R</li> <li>RStudio</li> <li>RStudio Server</li> </ul> <p>Authoring Note</p> <p>Jeffrey started copying over RStudio and RStudio Server images and text from the old web site. Then switched to another task. So those sections, especially, need careful review in this document. They may look complete but is not.</p> <p>Authoring Note</p> <p>Multiple other documents refer to this one. If it is decided to break this document up into multiple documents, those reference links will need to be updated.</p> <p>We have a version of R available as a module.</p> <p>This version of R is also known as conda_R. It was built with conda and contains many extra packages commonly used on the cluster.</p> <p>You can install your own packages to your home directory, as described in this document about building your own R and python pkgs. You can also install your own version of R in your home directory, preferably using conda.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#examples-of-running-r","title":"Examples of Running R","text":"<p>During orientation, participants are shown a number of example files stored in a directory copied into new account home directories. There is a copy of the latest set of those in the directory /jhpce/shared/jhpce/class-scripts/</p> <p>We have one set for users of the main cluster and a second for users of the C-SUB. </p> <p>You can copy the latest files into your home directory with this command, replacing \"clustername\" with either \"jhpce\" or \"c-sub\" <code>rsync -av /jhpce/shared/jhpce/slurm/class-scripts/clustername/ $HOME/class-scripts</code> For that rsync command to work correctly, there needs to be a trailing slash on the path to the originals of the class-scripts.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-r-interactively","title":"Running R Interactively","text":"<p>In $HOME/class-scripts/R-demo there are two files, a SLURM batch job file and an R program file. </p> <ul> <li><code>cd $HOME/class-scripts/R-demo</code></li> <li>Inspect the batch job file: <code>cat plot1.sh</code></li> <li><code>srun --pty --x11 bash</code></li> <li><code>module load conda_R</code></li> <li>Run plot1.r: <code>R CMD BATCH plot1.r</code></li> <li>Running the program creates a file \"plot1-R-results.pdf\"</li> <li>You can view it with a program or a web browser:</li> <li><code>xpdf plot1-R-results.pdf &amp;</code></li> <li><code>chromium-browser file:///$HOME/class-scripts/R-demo/plot1-R-results.pdf &amp;</code></li> </ul>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-r-in-a-batch-job","title":"Running R In A Batch Job","text":"<ul> <li>Inspect the batch job file: <code>cat plot1.sh</code></li> <li>Submit the job to run: <code>sbatch plot1.sh</code></li> <li>See your</li> </ul>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#examples-of-running-rstudio","title":"Examples of Running RStudio","text":"","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-from-the-command-line","title":"Running from the Command Line","text":"<ul> <li><code>srun --pty --x11 --mem=10G bash</code></li> <li><code>module load conda_R</code></li> <li><code>module load rstudio</code></li> <li><code>rstudio &amp;</code></li> </ul>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-via-the-web-portal","title":"Running via the Web Portal","text":"<p>Please see our page about using it.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-rstudio-server","title":"Running RStudio Server","text":"","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#running-from-the-command-line_1","title":"Running from the Command Line","text":"<p>Depending on which cluster you are using, run one of the following two scripts after starting an interactive session and landing on a compute node. (The discussion below assumes that you are on the jhpce cluster.) </p> <ul> <li>jhpce-rstudio-server</li> <li>csub-rstudio-server</li> </ul> <p>Rstudio Server is a web based environment for developing R programs.  On the JHPCE cluster we have put together a script called \u201cjhpce-rstudio-server\u201d which will allow you to run your own personal copy of Rstudio Server and access it from a browser on your laptop or desktop.  When the \u201cjhpce-rstudio-server\u201d program is run, it starts an instance of the Rstudio Server web server within a Singularity image on a unique port number, and then provides instructions for setting up an ssh tunnel to allow you to access Rstudio Server from your local system.</p> <p>You will need to perform one step to enable access to this Rstudio Server from your local laptop/desktop;  specifically, you will need to add a tunnel to your existing ssh session to the JHPCE cluster.</p> <p>Tip</p> <p>In UNIX, you send an interrupt signal to a running foreground program using the key combination <code>CONTROL c</code> This is historically written as <code>^C</code>  Note that you DO NOT actually use the SHIFT key to capitalize the C. The way it is written is misleading. You do NOT type <code>CONTROL SHIFT c</code>. However this is the way it has been written for decades, and we will do so.</p> <p>Warning</p> <p>However, you DO need to capitalize the letter <code>c</code> when trying to send an interrupt signal to the ssh program on Mac or Linux computers. The key combination for an ssh interrupt is <code>~ SHIFT c</code></p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#for-mac-or-linux-computers","title":"For Mac or Linux computers:","text":"<p>To add this tunnel, first type ~C (while holding down SHIFT, press \u201c~\u201d then \u201cC\u201d).  The ~C is used to send an interrupt to your ssh session.  The ~C will likely not show up, but you should see an \u201cssh&gt;\u201d prompt as a result.  At this \u201cssh&gt;\u201d prompt you activate the tunnel by typing  -L XXXXX:compute-YYY:XXXXX  .  This will allow your laptop/desktop to access the compute node compute-YYY on port XXXXX (in the above example, the port used was 12345 and the compute node used was compute-012).</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#for-windows-computers-or-from-the-safe-desktop","title":"For Windows computers, or from the SAFE Desktop:","text":"<p>If you connected to the JHPCE cluster with MobaXterm from a Windows-based system or SAFE desktop, you should ignore the first step (entering ~C and adding the local tunnel). </p> <p>Instead, you will need to  add a tunnel from MobaXterm.</p> <p>Before setting up the tunnel you may find it helpful to set up an SSH key using the steps at https://jhpce.jhu.edu/knowledge-base/mobaxterm-configuration#ssh-keys While not a requirement, this will eliminate the need to login using your password and Google Verification Code.  Note that if you are setting up the tunnel for the C-SUB, you will not be able to use SSH keys due to the enhanced security of the C-SUB.</p> <p>To start, click on the \u201cTunneling\u201d icon at the top of MobaXterm, and you should see the window below:</p> <p></p> <p>Click on \u201cNew SSH Tunnel\u201d, and you should see:</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#shutting-down-the-rstudio-server","title":"Shutting down the Rstudio Server","text":"<p>When you have finished using Rstudio Server, you should close the browser tab or window that you are using to run Rstudio Server, and then return to the ssh session where you ran the \u201cjhpce-rstudio-server\u201d command.  To stop the Rstudio Server, type \u201c^C\u201d.  You will then be given a few additional steps to run to deactivate the port forward. As with the establishment of the tunnel, these steps are for MacOS and Linux based desktops/laptops.  You will again be prompted to type \u201c~C\u201d, and then enter \u201c-KL XXXXX\u201d at the \u201cssh&gt;\u201d prompt to stop the forwarding (NOTE: you\u2019ll need to hit  once before typing \u201c~C\u201d).  The session should look similar to: <p>Once you enter your login and password, you should see Rstudio running in your browser.</p> <p>For Windows desktops/laptops, you should also use \u201c^C\u201d to terminate the Rstudio Server, but to stop the tunnel you will need to return to the MobaSSHTunnel screen, and use the \u201cStop\u201d icon  in the Start/Stop column.  You can keep this tunnel configuration in MobaSSHTunnel, and reuse it the next time you run Rstudio Server, however you will need to edit the tunnel configuration and change the \u201cRemote Server\u201d to match the compute node you are running on.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-n-friends/#faqscomments","title":"FAQs/Comments","text":"<p>Q) Why did you do this?  R works just fine for me on the cluster!</p> <p>A) On the JHPCE cluster we have historically had several ways to run R programs.  Often  people will use the text-based version of R to run programs, and that works well for a lot of people.  Some people prefer to work in a graphical environment, so we also have the X11-based \u201cRstudio\u201d available on the cluster, which is great, except that on a slower network connection, this can get quite laggy.  The web based Rstudio Server provides the same graphical version available in Rstudio, but over a much lighter network protocol than the X11-based Rstudio, so it is much faster and more responsive to use.</p> <p>Q) Why not just set up a dedicated web server to run Rstudio Server like I had back at ZZZZ?</p> <p>A) Rstudio Server does not play well with clusters.  For us to run a dedicated Rstudio Server server, we would need to purchase a fairly large system with lots of RAM and CPU power.  This was considered, but in the end was deemed cost prohibitive, and it didn\u2019t allow the use of the JHPCE cluster resources to run R programs.  This solution allows the nice web-based Rstudio Server to be used, while making use of the existing CPU and RAM resources available on the cluster.</p> <p>Q) My program can\u2019t run because it needs XXX package!</p> <p>A) The R that is run within the Rstudio Server is completely separate  from the default version of R that is used on the JHPCE cluster, therefor you may need to install packages using the install.packages() function, or through the Rstudio Server GUI .</p> <p>Q) I forgot to cleanly disconnect from the Rstudio Server/My session got disconnected.</p> <p>A) This should be fine.  Your interactive srun session will eventually time out and will kill the Rstudio Server that was running.  You may get warning messages about ports being in use \u2013 if so, please wait a few minutes and try again.</p> <p>Q) When I try to add the port forward, I get an error message about \u201cPort is in use\u201d.  How do I fix this?</p> <p>A) You can only run one instance of Rstudio Server. You likely have another SSH session running that has the port forward lingering.  If you had been using Rstudio Server in another SSH session, you will need to either need to log out of that ssh session, or run the \u201c~C\u201d \u201c-KL XXXXX\u201d command to tear down the port forward.</p> <p>If you have any questions  about using Rstudio Server, please feel free to email us at bitsupport.</p>","tags":["in-progress","refers-to-old-website"]},{"location":"sw/r-pkg/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"sw/sw-stub.md\"</p>","tags":["needs-to-be-written","jiong"]},{"location":"sw/r-slurmjobs/","title":"Working with SLURM via R and slurmjobs","text":"<p>slurmjobs provides helper functions for interacting with SLURM-managed high-performance-computing environments from R. It includes functions for creating submittable jobs (including array jobs), monitoring partitions, and extracting info about running or complete jobs.</p> <p>R is an open-source statistical environment which can be easily modified to enhance its functionality via packages. </p> <p>slurmjobs is a R package available via the Bioconductor repository for packages. R can be installed on any operating system from CRAN after which you can install slurmjobs </p> <p>For more information about slurmjobs, see</p> <p>http://research.libd.org/slurmjobs/articles/slurmjobs.html</p>"},{"location":"sw/r/","title":"R","text":""},{"location":"sw/r/#running-r-jobs-over-slurm","title":"Running R Jobs over SLURM","text":""},{"location":"sw/r/#-using-r-interactively-from-command-line","title":"- Using R Interactively from command line","text":"<pre><code>[test@login31 ~]$ srun --pty bash\nsrun: job 3176550 queued and waiting for resources\nsrun: job 3176550 has been allocated resources\n[test@compute-152 ~]$ module load conda_R\nLoading conda_R/4.3.x\n(4.3.x)[test@compute-152 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) conda/3-23.3.1   4) conda_R/4.3.x\n\n(4.3.x)[test@compute-152 ~]$ R\n\nR version 4.3.2 Patched (2024-02-08 r85876) -- \"Eye Holes\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-conda-linux-gnu (64-bit)\n...\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n...\n&gt; \n</code></pre>"},{"location":"sw/r/#-using-r-in-batch-mode","title":"- Using R in batch mode","text":"<ol> <li> <p>Write your R source in a file with .r extension (e.g. plot1.r) <pre><code># Set output file name\npdf(\"plot1-R-results.pdf\")\n\n# Define 2 vectors\ncars &lt;- c(1, 3, 6, 4, 9)\ntrucks &lt;- c(2, 5, 4, 5, 12)\n\n# Graph cars using a y axis that ranges from 0 to 12\nplot(cars, type=\"o\", col=\"blue\", ylim=c(0,12))\n\n# Graph trucks with red dashed line and square points\nlines(trucks, type=\"o\", pch=22, lty=2, col=\"red\")\n\n# Create a title with a red, bold/italic font\ntitle(main=\"Autos\", col.main=\"red\", font.main=4)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. plot1.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load conda_R\nR CMD BATCH plot1.r\n</code></pre></p> </li> <li> <p>Submit your R job <pre><code>[test@login31 ~]$ sbatch plot1.sh \nSubmitted batch job 3177833\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3177833    shared plot1.sh    test  R       0:03      1 compute-095\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r--    1 test test        22 Mar 13 16:29  slurm-3177833.out\n-rw-r--r--    1 test test      4988 Mar 13 16:29  plot1-R-results.pdf\n</code></pre></p> </li> <li> <p>Look at the result file, plot1-R-results.pdf; you can use xpdf to view it if you have set X11 forward when login.    Otherwise, you need download the file to local machine to view it. <pre><code>[test@login31 ~]$ xpdf plot1-R-results.pdf\n</code></pre></p> </li> </ol>"},{"location":"sw/sas-old/","title":"SAS","text":"","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#we-have-a-sas-partition","title":"We Have A SAS Partition","text":"<p>Containing two nodes.</p>","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#other-ways-to-access-sas","title":"Other ways to access SAS","text":"<p>SAS is available in a virtual Windows environment called SAFE. Here is a link to information about SAFE.</p>","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#marketscan-database","title":"MarketScan Database","text":"","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#how-to-get-permission-to-access-it","title":"How To Get Permission To Access It","text":"","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#where-is-it","title":"Where Is It","text":"","tags":["in-progress","jiong"]},{"location":"sw/sas-old/#when-running-sas-an-error-dialog-pops-up-about-remote-browser","title":"When running SAS, an error dialog pops up about Remote Browser","text":"<p>When running SAS, you may need to specify options to indicate which browser to use when displaying either help or graphical output. We recommend using the Chromium browser. You can use the following options to the SAS command to do so:</p> <p><code>sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/chromium-browser'\" -xrm \"SAS.helpBrowser:'/usr/bin/chromium-browser'\"</code></p> <p>Here is some code you can add to your .bashrc file which contain some convenient bash aliases for starting SAS with browser support configured. Once that becomes part of your environment (by sourcing the file or by logging out and back in again), after loading the SAS module you can start SAS using either <code>csas</code> or <code>fsas</code> so that it can open the desired web browser if needed.</p> <p>Warning</p> <p>These definitions include optional syntax (<code>&gt; /dev/null 2&gt;&amp;1</code>) which hide error messages. If you are having problems displaying SAS material in web browsers, you may need to run SAS without that output redirection. The \"srun half duplex\" error is an example of such a case.</p> <pre><code># SAS routines for __interactive__ sessions where plotting is involved\n# (because SAS generates HTML for the plots when run in interactive mode)\n# \n# (YOU HAVE TO RUN \"module load SAS\" before calling either of these routines)\n#\n# If you want to use Firefox as your web browser for SAS:\n#\nfsas() { sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/firefox'\" -xrm \"SAS.helpBrowser:'/usr/bin/firefox'\" \"$@\" &gt; /dev/null 2&gt;&amp;1; }\n#\n# If you want to use Chromium-browser as your web browser for SAS:\n#\ncsas() { sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/chromium-browser'\" -xrm \"SAS.helpBrowser:'/usr/bin/chromium-browser'\" \"$@\" &gt; /dev/null 2&gt;&amp;1; }\n</code></pre>","tags":["in-progress","jiong"]},{"location":"sw/sas/","title":"SAS","text":""},{"location":"sw/sas/#running-basic-sas-jobs-over-slurm","title":"Running basic SAS jobs over SLURM","text":""},{"location":"sw/sas/#-using-sas-interactively","title":"- Using SAS interactively","text":"<pre><code>[test@login31 ~]$ srun --partition sas --mem 10G --x11 --pty bash\nsrun: job 3178287 queued and waiting for resources\nsrun: job 3178287 has been allocated resources\n\n[test@compute-101 ~]$ module load sas\n[test@compute-101 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) sas/9.4\n\n[test@compute-101 ~]$ sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/chromium-browser'\" -xrm \"SAS.helpBrowser:'/usr/bin/chromium-browser'\"\n\n## if you want to use Firefox as your web browser for SAS, run the following command instead\n[test@compute-101 ~]$ sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/firefox'\" -xrm \"SAS.helpBrowser:'/usr/bin/firefox'\"\n</code></pre> <p>Notes:   * You may need to accept popups in the chromium/firefox browser that gets started in order to see the windows that SAS is trying to display   * In the terminal session that you started \u201csas\u201d, you may see messages similar to ones below.  These can be ignored because the browser wants to run on a local system with a graphics card, and the X11 session doesn\u2019t allow that <pre><code>[2970887:2970887:1024/152818.092311:ERROR:chrome_browser_cloud_management_controller.cc(163)] Cloud management controller initialization aborted as CBCM is not enabled.\n[2970887:2971086:1024/152818.161869:ERROR:login_database.cc(922)] Password store database is too new, kCurrentVersionNumber=35, GetCompatibleVersionNumber=39\n[2970887:2971087:1024/152818.164247:ERROR:login_database.cc(922)] Password store database is too new, kCurrentVersionNumber=35, GetCompatibleVersionNumber=39\n[2970887:2971086:1024/152818.167534:ERROR:login_database_async_helper.cc(59)] Could not create/open login database.\n[2970887:2971087:1024/152818.170351:ERROR:login_database_async_helper.cc(59)] Could not create/open login database.\n[2970887:2970887:1024/152818.626429:ERROR:object_proxy.cc(590)] Failed to call method: org.freedesktop.portal.Settings.Read: object_path= /org/freedesktop/portal/desktop: org.freedesktop.portal.Error.NotFound: Requested setting not found\nlibGL error: No matching fbConfigs or visuals found\nlibGL error: failed to load driver: swrast\n</code></pre></p>"},{"location":"sw/sas/#-using-sas-in-batch-mode","title":"- Using SAS in batch mode","text":"<ol> <li> <p>Write your sas source in a file with .sas extension (e.g. class-info.sas) <pre><code>DATA CLASS;\n     INPUT NAME $ 1-8 SEX $ 10 AGE 12-13 HEIGHT 15-16 WEIGHT 18-22;\nCARDS;\nJOHN     M 12 59 99.5\nJAMES    M 12 57 83.0\nALFRED   M 14 69 112.5\nALICE    F 13 56 84.0\n\nPROC MEANS;\n     VAR AGE HEIGHT WEIGHT;\nPROC PLOT;\n     PLOT WEIGHT*HEIGHT;\nENDSAS;\n;\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. sas-demo1.sh) <pre><code>#!/bin/bash\n\n#SBATCH --partition=sas\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load sas\nsas class-info.sas\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch sas-demo1.sh\nSubmitted batch job 3178475\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            3178475       sas sas-demo    test R       0:01      1 compute-101\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test    0 Mar 13 16:49 slurm-3178475.out\n-rw-r--r-- 1 test test 2108 Mar 13 16:49 class-info.lst\n</code></pre></p> </li> <li> <p>Look at the results from output file <pre><code>[test@login31 ~]$ cat class-info.lst\n\n                                                        The MEANS Procedure\n\n                           Variable    N            Mean         Std Dev         Minimum         Maximum\n                           -----------------------------------------------------------------------------\n                           AGE         4      12.7500000       0.9574271      12.0000000      14.0000000\n                           HEIGHT      4      60.2500000       5.9651767      56.0000000      69.0000000\n                           WEIGHT      4      94.7500000      14.0386372      83.0000000     112.5000000\n                           -----------------------------------------------------------------------------\n</code></pre></p> </li> </ol>"},{"location":"sw/singularity/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"sw/sw-stub.md\"</p>","tags":["needs-to-be-written","jiong"]},{"location":"sw/stata/","title":"Stata","text":""},{"location":"sw/stata/#running-basic-stata-jobs-over-slurm","title":"Running basic Stata jobs over SLURM","text":""},{"location":"sw/stata/#-using-stata-interactively","title":"- Using Stata interactively","text":"<pre><code>[test@login31 ~]$ srun --mem 10G --x11 --pty bash\nsrun: job 3179341 queued and waiting for resources\nsrun: job 3179341 has been allocated resources\n[test@compute-092 ~]$ module load stata\n[test@compute-092 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) stata/17\n\n[test@compute-092 ~]$ xstata\n</code></pre>"},{"location":"sw/stata/#-using-stata-in-batch-mode","title":"- Using Stata in batch mode","text":"<ol> <li> <p>Write your Stata source in a file with .do extension (e.g. stata-demo1.do) <pre><code>program define hello\ndi \"Hello There World 123\"\nend\nhello\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. stata-demo1.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load stata\nstata -b stata-demo1.do\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch stata-demo1.sh\nSubmitted batch job 3179530\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            3179530    shared stata-de    test  R       0:01      1 compute-092\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 13 17:29 slurm-3179530.out\n-rw-r--r-- 1 test test 890 Mar 13 17:29 stata-demo1.log\n</code></pre></p> </li> <li> <p>Look at the result <pre><code>[test@login31 ~]$ cat stata-demo1.log\n...\n. do \"stata-demo1.do\" \n\n. program define hello\n  1. di \"Hello There World 123\"\n  2. end\n\n. hello\nHello There World 123\n\n. \nend of do-file\n</code></pre></p> </li> </ol>"},{"location":"sw/sw-overview/","title":"SOFTWARE TOPIC - page to develop outline","text":"","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#what-weve-installed-how-to-use-it","title":"What we've installed &amp; how to use it","text":"","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#overview","title":"Overview","text":"","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#modules","title":"Modules","text":"<p>Who creates our modules? Where is the document about using modules?</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#in-the-operating-system","title":"In the operating system","text":"<p>Is there anything useful to say about what's available to people outside of what they see listed by <code>module avail</code>?</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#suggested-programs-for-gui-use","title":"Suggested programs for GUI use","text":"<p>Jeffrey includes in C-SUB orientation a table of GUI programs people might find useful, particularly those new to UNIX and the CLI.</p> <p>There's an open github issue about whether to install some of those C-SUB tools on the JHPCE cluster.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#software-by-category","title":"Software by category","text":"<p>We don't have to aim for a complete and frequently updated list. But we might give folks pointers to some important tool sets. Maybe our bithelp members can help write this document/section.</p> <p>Example from C.E.C.I</p> <p>Available software, example of</p> <p>Biomolecular simulation pkgs, example of</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#how-to-add-your-own-packages-to-r-and-python","title":"How to add your own packages to R and python","text":"<p>Mark seems to think that we don't need to provide much in the way of instructions to users on building software. But JRT feels that this subtopic, at least, deserves a document.</p> <p>Here is a starter document about adding your needed packages to existing python and R modules.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#building-your-own-software","title":"Building your own software","text":"<p>If we don't write much beyond the adding-pkgs-to-the-two-popular-interpreted-languages, perhaps this section or document merely provides links to the work of others.</p> <p>JRT thinks that people who have spent decades building software from source may be underestimating what younger people know.  </p> <p>We want to facilitate our users getting going, gaining proficiency. Part of what we want to differentiate us from other options is that we are more helpful than cold and impersonal organizations.</p> <p>Here is a starter document about building your own software.</p> <p>Software installation with Conda</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-overview/#gpu-information","title":"GPU Information","text":"<p>Point to documents in that section. JRT created an overview document as a starting point.</p>","tags":["topic-overview","needs-to-be-written"]},{"location":"sw/sw-stub/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"sw/sw-stub.md\"</p>","tags":["needs-to-be-written","jiong"]},{"location":"sw/vscode/","title":"Virtual Studio Code","text":"","tags":["needs-to-be-written"]},{"location":"ourtools/tags/","title":"Tagged Files","text":"<p>Tags and the files they are mentioned in are listed here, automatically generated by Material for MkDocs.</p> <p>While the web site is under development, they guide site authors to documents which need help and notify users how to approach the information found on tagged pages.</p> <p>After the web site content stabilizes, the tags that remain will primarily consist of keywords.</p>"},{"location":"ourtools/tags/#adi","title":"adi","text":"<ul> <li>GUI Applications</li> </ul>"},{"location":"ourtools/tags/#done","title":"done","text":"<ul> <li>Access Overview</li> <li>Submitting Good Queries</li> <li>Self Service MFA, Password Requests</li> <li>Quality of Service (QOS)</li> <li>sacct useful command examples</li> <li>scontrol useful command examples</li> <li>When Will My Job Start?</li> <li>Fastscratch</li> <li>Quotas</li> </ul>"},{"location":"ourtools/tags/#gpu","title":"gpu","text":"<ul> <li>GPU</li> </ul>"},{"location":"ourtools/tags/#in-progress","title":"in-progress","text":"<ul> <li>SSH</li> <li>X11</li> <li>What Is The C-SUB?</li> <li>ACL</li> <li>Copying within cluster</li> <li>Sharing Files</li> <li>External Guides</li> <li>FAQ</li> <li>Tips &amp; Conventions</li> <li>GUI Applications</li> <li>Crafting Jobs</li> <li>Monitoring Your Jobs</li> <li>Example User Guides</li> <li>Backups &amp; Restores</li> <li>R Basics</li> <li>SAS</li> </ul>"},{"location":"ourtools/tags/#jeffrey","title":"jeffrey","text":"<ul> <li>Archive Files</li> <li>Copying within cluster</li> <li>Sharing Files</li> <li>External Guides</li> </ul>"},{"location":"ourtools/tags/#jiong","title":"jiong","text":"<ul> <li>Conda Environment</li> <li>Modules</li> <li>Python Packages</li> <li>R Packages</li> <li>SAS</li> <li>Singularity</li> <li>stub page for the \"Software\" topic</li> </ul>"},{"location":"ourtools/tags/#mark","title":"mark","text":"<ul> <li>GPU</li> <li>Cost Calculator</li> <li>Containers</li> </ul>"},{"location":"ourtools/tags/#needs-major-revision","title":"needs-major-revision","text":"<ul> <li>File Transfer - Overview</li> <li>New User</li> <li>General Tips/Requests</li> <li>Storage Overview</li> <li>Helpful GUI Programs</li> </ul>"},{"location":"ourtools/tags/#needs-review","title":"needs-review","text":"<ul> <li>OneDrive via rclone</li> <li>FAQ</li> <li>Modules</li> </ul>"},{"location":"ourtools/tags/#needs-to-be-written","title":"needs-to-be-written","text":"<ul> <li>Staff</li> <li>Archive Files</li> <li>Data Security</li> <li>Files Overview</li> <li>GPU</li> <li>Cost Calculator</li> <li>New node?</li> <li>New PI information and form</li> <li>Need more space?</li> <li>First Login</li> <li>Orientation Overview</li> <li>Adding Your Own Python and R Libraries</li> <li>Conda Environment</li> <li>Containers</li> <li>Jupyter</li> <li>Python Packages</li> <li>R Packages</li> <li>Singularity</li> <li>Overview (Outline)</li> <li>stub page for the \"Software\" topic</li> <li>VS Code</li> </ul>"},{"location":"ourtools/tags/#refers-to-old-website","title":"refers-to-old-website","text":"<ul> <li>Joint HPC Exchange</li> <li>JHPCE Model</li> <li>File Transfer - Overview</li> <li>SSH</li> <li>X11</li> <li>New User</li> <li>GPU</li> <li>General Tips/Requests</li> <li>New PI information and form</li> <li>New user form</li> <li>R Basics</li> </ul>"},{"location":"ourtools/tags/#slurm","title":"slurm","text":"<ul> <li>Crafting Jobs</li> <li>Monitoring Your Jobs</li> <li>Partitions</li> <li>Quality of Service (QOS)</li> <li>SLURM FAQ</li> <li>sacct useful command examples</li> <li>sacctmgr useful command examples</li> <li>scontrol useful command examples</li> <li>Example User Guides</li> <li>When Will My Job Start?</li> </ul>"},{"location":"ourtools/tags/#ssh","title":"ssh","text":"<ul> <li>SSH</li> </ul>"},{"location":"ourtools/tags/#topic-overview","title":"topic-overview","text":"<ul> <li>Access Overview</li> <li>File Transfer - Overview</li> <li>What Is The C-SUB?</li> <li>Files Overview</li> <li>GPU</li> <li>Orientation Overview</li> <li>Storage Overview</li> <li>Overview (Outline)</li> </ul>"}]}