{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Joint HPC Exchange","text":"<ul> <li> <p> About Us</p> <p>The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. JHPCE began in 2008 as a  collaborative effort between Biostatistics and the Computational Biology &amp; Research Computing group in the department of Molecular Microbiology and Immunology. Since then the facility has grown to provide HPC services to over 100 labs and departments in the JHU Bloomberg School of Public Health, JHU School of Medicine, JHU Carey Business school, the Lieber Institue for Brain Development, Kennedy Krieger institute, and numerous departments on the JHU Homewood campus.  The facility is open to all Johns Hopkins affiliated researchers.</p> </li> <li> <p> Community</p> <p>The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. All of the computing resources on JHPCE have been provided by various stakeholders on the cluster.  To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders.</p> <p>Throughout the years, JHPCE has provided HPC services to over 3000 researchers across JHU, with 300 active users at any given quarter.</p> <p>The JHPCE cluster has over 100 statistical and genomics applications installed, including R, SAS, plink, bowtie....  Many of these applications have been installed, and are maintained by members of the community. We allow for users to install their own applications on the cluster, and encourage sharing of applications as a \"module\" with other users.</p> </li> <li> <p> Cluster Details</p> <p>The computing and storage systems on the JHPCE cluster are optimized for genomics and biomedical research. The cluster has 87 compute nodes, providing over 4000 cores, 53TB of DRAM and over 20 PB of low-cost networked mass storage (ZFS and Lustre-over-ZFS). The network fabric consists of a 100 Gbps ethernet backbone with nodes connected via 10 Gbps. The JHPCE cluster also has 7 GPU nodes with 23 Nvidia H100, V100, and A100 GPUs in support of numerous AI, ML, and LLM efforts across the JHU campuses. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ.</p> <p>The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc.  Job scheduling is performed with the Simple Linux Utility for Resource Management (SLURM).</p> </li> </ul> <p>If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the directors (jhpce@jhu.edu) to determine whether we can accommodate your needs.</p> <p>If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form.</p> <p>Mark Miller and Brian Caffo Co-Directors, JHPCE</p>"},{"location":"all-listings/","title":"JHPCE Documentation - All search results","text":"<p>You will find here all the search results listings.</p> <p>JHPCE_PLACEHOLDER_FOR_THE_LISTINGS_PLUGIN</p>"},{"location":"getting-started-overview/","title":"Joint HPC Exchange","text":"<ul> <li> <p> About Us</p> <p>The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. JHPCE began in 2008 as a  collaborative effort between Biostatistics and the Computational Biology &amp; Research Computing group in the department of Molecular Microbiology and Immunology. Since then the facility has grown to provide HPC services to over 100 labs and departments in the JHU Bloomberg School of Public Health, JHU School of Medicine, JHU Carey Business school, the Lieber Institue for Brain Development, Kennedy Krieger institute, and numerous departments on the JHU Homewood campus.  The facility is open to all Johns Hopkins affiliated researchers.</p> </li> <li> <p> Community</p> <p>The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. All of the computing resources on JHPCE have been provided by various stakeholders on the cluster.  To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders.</p> <p>Throughout the years, JHPCE has provided HPC services to over 3000 researchers across JHU, with 300 active users at any given quarter.</p> <p>The JHPCE cluster has over 100 statistical and genomics applications installed, including R, SAS, plink, bowtie....  Many of these applications have been installed, and are maintained by members of the community. We allow for users to install their own applications on the cluster, and encourage sharing of applications as a \"module\" with other users.</p> </li> <li> <p> Cluster Details</p> <p>The computing and storage systems on the JHPCE cluster are optimized for genomics and biomedical research. The cluster has 87 compute nodes, providing over 4000 cores, 53TB of DRAM and over 20 PB of low-cost networked mass storage (ZFS and Lustre-over-ZFS). The network fabric consists of a 100 Gbps ethernet backbone with nodes connected via 10 Gbps. The JHPCE cluster also has 7 GPU nodes with 23 Nvidia H100, V100, and A100 GPUs in support of numerous AI, ML, and LLM efforts across the JHU campuses. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ.</p> <p>The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc.  Job scheduling is performed with the Simple Linux Utility for Resource Management (SLURM).</p> </li> </ul> <p>If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the directors (jhpce@jhu.edu) to determine whether we can accommodate your needs.</p> <p>If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form.</p> <p>Mark Miller and Brian Caffo Co-Directors, JHPCE</p>"},{"location":"listing-search/","title":"JHPCE Listing search","text":"<p>This page has a search function for all listings.</p>"},{"location":"aboutus/GrantAndAckBlurbs/","title":"Grant and Acknowledgment Blurbs","text":"<p>If you need to refer to the JHPCE cluster or CSUB environment for a Grant submission or an acknowledgement, please use the verbiage below:</p>"},{"location":"aboutus/GrantAndAckBlurbs/#jhpce-cluster-grant-submission-blurb","title":"JHPCE Cluster Grant Submission Blurb","text":"<p>The Joint High Performance Computing Exchange (JHPCE) is a Johns Hopkins University service center based out of the Biostatistics department of the Johns Hopkins  Bloomberg School of Public Health. The JHPCE has been providing cost-effective HPC services since 2008 in support of over 100 research groups in the  Johns Hopkins Bloomberg School of Public Health, the Johns Hopkins School of Medicine, the Johns Hopkins Cary Business School, and several JHU Affiliated organizations. The JHPCE provides a rich ecosystem of Linux-based statistical and genomics applications, and is specialized to support development in R, python, SAS, and Stata languages.</p> <p>As of September 2024, the JHPCE cluster is comprised of 87 compute nodes, providing over 4000 64-bit cores/threads and 53 TB of DDR-SDRAM. The JHPCE cluster also has 7 GPU nodes  with 23 Nvidia H100, V100, and A100 GPUs in support of numerous AI, ML, and LLM efforts across the JHU campuses. The storage environment on the cluster contains  a total of 20PB of storage with a 2PB Lustre storage cluster, 16PB of ZFS NAS storage, and a 2PB ZFS NAS storage array for offsite disk-to-disk backup,  all built on the \u201cdirt cheap\u201d storage philosophy.  In addition, a 25 TB NVME SSD based \u201cfast scratch\u201d storage array to support short-term storage of  high-IO processing. </p>"},{"location":"aboutus/GrantAndAckBlurbs/#jhpce-c-sub-grant-submission-blurb","title":"JHPCE C-SUB Grant Submission Blurb","text":"<p>The Joint High Performance Computing Exchange (JHPCE) CMS Sub-cluster (C-SUB) is a High Performance Compute Cluster which has been built to  provide secure HPC services for accessing Medicare and Medicaid data in support of researchers in the Johns Hopkins Bloomberg School of Public Health  and the Johns Hopkins School of Medicine. The C-SUB is a service of the JHPCE, a High-Performance Computing facility in the  Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health that has been in existence since 2008.  The C-SUB is maintained at a high-security level in accordance with federal regulations governing secure computer  systems (e.g., the Federal Information Security Management Act-FISMA) and meets strict security and privacy requirements of Centers for Medicare &amp; Medicaid Services (CMS). The JHPCE provides a rich ecosystem of Linux-based statistical and genomics applications, and is specialized to support development in R, python, SAS, and Stata languages.</p> <p>As of August 2024, the JHPCE C-SUB cluster is comprised of a 4-node compute cluster, providing 480 64-bit cores  and 7.5 TB of DDR-SDRAM. The storage environment for the C-SUB is provided by a 100 TB allocation of ZFS encrypted \"dirt cheap\" storage, and a 50 TB allocation of ZFS encrypted SSD-based storage.</p>"},{"location":"aboutus/GrantAndAckBlurbs/#jhpce-acknowledgment-blurb","title":"JHPCE Acknowledgment Blurb","text":"<p>The authors gratefully acknowledge use of the facilities at the Joint High Performance Computing Exchange (JHPCE) in the Department of Biostatistics,  Johns Hopkins Bloomberg School of Public Health that have contributed to the research results reported within this paper.</p>"},{"location":"aboutus/model/","title":"Joint HPC Exchange","text":""},{"location":"aboutus/model/#about-us","title":"About Us","text":"<p>The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This fee-for-service core began in 2008 as a collaborative effort between Biostatistics and the Computational Biology &amp; Research Computing group in the department of Molecular Microbiology and Immunology. The facility has grown over the years and is open to all Johns Hopkins affiliated researchers.</p>"},{"location":"aboutus/model/#the-jhpce-service-center-manages-2-hpc-computing-environments","title":"The JHPCE Service Center manages 2 HPC computing environments.","text":""},{"location":"aboutus/model/#jhpce","title":"JHPCE","text":"<p>The larger HPC environmnet is for general HPC computing on non-HIPAA data. We generally refer to this as the \"J H P C E\" cluster (each letter pronounced).  Informaion on the JHPCE cluster can be found at the Introduction link.</p>"},{"location":"aboutus/model/#c-sub","title":"C-SUB","text":"<p>We also manage a smaller sub-cluster for working on CMS Medicare and Medicaid data. We refer to this cluster as the C-SUB (CMS Sub) cluster. While the JHPCE service cnter manages this HPC cluster, it's operation is overseen by the HARP organization in JHU. Technical infomation about the C-SUB can be found at the CSUB Overview.</p>"},{"location":"aboutus/model/#community","title":"Community","text":"<p>The facility is used primarily by labs and research groups in the Johns Hopkins Bloomberg School of Public Health (SPH), the Johns Hopkins School of Medicine (SOM) and the Lieber Institute for Brain Development (LIBD). We support the HPC needs of over 2000 user accounts, with 300 active users each quarter.</p>"},{"location":"aboutus/model/#cluster-details","title":"Cluster Details","text":"<p>As of November, 2024, the computing and storage systems are optimized for genomics and biomedical research. The cluster has 84 compute nodes, providing about 3100 cores, 40TB of DRAM and over 20 PB of networked mass storage. The JHPCE cluster also have 37 Nvidia GPUs in support of AI and ML research done in our community.  The network fabric consists of 10/40 Gbps ethernet connections. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ.</p> <p>Networked mass storage uses open-source file systems (ZFS and Lustre-over-ZFS) to provide low cost file systems. We also have a 2PB disk-to-disk backup system off site for backing up more critical data.</p> <p>The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc.  Job scheduling is performed with the Simple Linux Utility for Resource Management (SLURM).</p>"},{"location":"aboutus/model/#cost-recovery","title":"Cost Recovery","text":"<p>The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders.</p> <p>If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the directors (jhpce@jhu.edu) to determine whether we can accommodate your needs.</p> <p>If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form.</p> <p>Mark Miller and Brian Caffo Co-Directors, JHPCE</p>"},{"location":"aboutus/principles/","title":"Joint HPC Exchange Constitutional Principles","text":""},{"location":"aboutus/principles/#the-hpc-service-center-is-a-steward-of-a-commons","title":"The HPC service center is a steward of a commons.","text":"<p>It merely manage resources that are literally owned the stakeholders.</p>"},{"location":"aboutus/principles/#policies-must-be-consistent-with-ostroms-design-principles","title":"Policies must be consistent with Ostrom\u2019s design principles","text":"<p>This ensures sustainable utilization of common pool resources.</p>"},{"location":"aboutus/principles/#organizations-have-autonomy-in-structuring-payment-model","title":"Organizations have autonomy in structuring payment model.","text":"<p>Each organization can configure their computing budgets and resources, in a manner that is consistent with their research agendas and funding profiles, while being protected from exploitation by larger stakeholder organizations. Organizations, e.g. divisions, departments, institutes, collaborations and individual PIs, can provide their local community with: 1) free computing, 2) subsidized computing, 3) startup packages, or 4) \u201cbridge funding\u201d for PIs who are \u201cbetween grants\u201d.</p>"},{"location":"aboutus/principles/#free-computing-is-not-provided-by-the-service-center","title":"Free computing is not provided by the service center.","text":"<p>Users must ask their funding agencies, collaborators, departments or deans to support their computing.</p>"},{"location":"aboutus/principles/#policies-for-stakeholders","title":"Policies for Stakeholders","text":"<p>All computing queues have associated rates and charges \u2013 there is no free computing for anyone, even stakeholders.  Organizations can become stakeholders by purchase nodes.  Stakeholders have priority access to their nodes via their dedicated queues.  Stakeholder\u2019s are required to make excess compute cycles available to all users via the shared queue.  Under reasonable circumstances (e.g. grant or publication deadlines), stakeholders are allowed to temporarily block the shared queue from sending jobs to their hardware.  Stakeholders are free (within broad limits) to decide on policies for their community within their queues.  Lack of capacity on a dedicated queue is typically a signal to stakeholders that it\u2019s time to invest.</p>"},{"location":"aboutus/principles/#policies-for-non-stakeholders","title":"Policies for Non-Stakeholders","text":"<p>Users who have no stakeholder affiliation are allowed to compute only on the shared queue.  Users who have no stakeholder affiliation have no \u201cright to compute\u201d, but historically, this has never been an issue, as there has always been sufficient excess capacity available.  Lack of capacity on the shared queue is a signal to non-stakeholders that it\u2019s time to become a stakeholder \u2013 or compute elsewhere.  Practically speaking, there is usually plenty of capacity on the shared queue.</p>"},{"location":"aboutus/principles/#the-shared-partition","title":"The Shared Partition","text":"<p>Shared queue charges defray the operating charges of the stakeholders. This incentivizes sharing, e.g. if a stakeholder is not using their hardware, the shared queue charges (incurred by other users) will cover the operating expenses of the stakeholder\u2019s node.  Stakeholders use the shared queue for surge capacity when their own hardware does not suffice.  Lack of capacity on the shared queue is typically a signal to stakeholders and non-stakeholders alike that it\u2019s time to invest in nodes.  Historically, the cost for using the shared partition is about $0.01 per hour for a job using a single core and 5GB of RAM.</p>"},{"location":"aboutus/principles/#salient-storage-policies","title":"Salient storage policies","text":"<p>There is no free storage  Each user account comes with 100GB of home directory space (current rate is $0.45/GB/year, so $45/yr MAX.  Allocated (guaranteed) storage rate is between $18-$25 TB/year  limited \u2018leased\u2019 storage is available. Recent rate is ~$44/TB/year (based on TB actually used)  Home directories are backed up by default, all other storage is backed up upon request.</p>"},{"location":"aboutus/staff/","title":"Staff","text":"<p>The care and feeding of the JHPCE Clustering environment is done by 5 individuals, providing a total of just under 3 FTEs.</p> <ul> <li>Mark Miller - Technology Manager &amp; Co-director </li> <li>Brian Caffo, PHD - Biostatistics Professor &amp; Co-Director https://www.bcaffo.com</li> <li>Jiong Yang - Systems Engineer</li> <li>Jeffrey Tunison - Systems Engineer</li> <li>Adi Gherman - Senior Research Associate JHU Bio</li> </ul> <p>Please do not default to sending individual team members requests for general assistance. Mailing lists exist so that the entire team (or more!) sees requests/problems as described here. Thank you.</p> <p>Oversight of the JHPCE is done by the BIT Committee, composed of the following individuals.</p> <ul> <li>Brian Caffo, PHD -  Biostatistics Professor  https://www.bcaffo.com</li> <li>Kasper D Hansen PHD - Biostatistics Professor http://www.hansenlab.org</li> <li>John Muschelli PHD - Biostatistics Professor https://www.johnmuschelli.com</li> </ul>"},{"location":"access/access-overview/","title":"Accessing the Cluster: Overview","text":"","tags":["topic-overview"]},{"location":"access/access-overview/#cluster-structure-overview","title":"Cluster structure overview","text":"<p>The cluster consists of some public-facing hosts with the remaining computers \"behind them\" on private networks. An incomplete and very oversimplified approximation:</p> <pre><code>graph TD\n   A[Your computer] --&gt; B([Login nodes])\n   A[Your computer] -- Only via Hopkins/VPN --&gt; D((Web Portal))\n   A[Your computer] --&gt; C([Transfer node])\n   B([Login nodes]) --&gt; E[Compute node1]\n   D((Web Portal)) --&gt; E[Compute node1]\n   B([Login nodes]) --&gt; F[Compute node2]\n   D((Web Portal)) --&gt; F[Compute node2]\n   B([Login nodes]) --&gt; G[Compute node3]\n   D((Web Portal)) --&gt; G[Compute node3]\n   C([Transfer node]) &lt;--&gt; H[(storage servers)]</code></pre>","tags":["topic-overview"]},{"location":"access/access-overview/#public-facing-login-and-transfer","title":"Public-facing: Login and Transfer","text":"<p>The login and transfer nodes are accessible to the wider Internet.  For security reasons, the web portal is only available to computers on Hopkins networks. If you are not on a Hopkins campus, that means that you need to use the VPN to be able to see that node.</p> <ul> <li>login: Normally jhpce01.jhsph.edu and jhpce02.jhsph.edu</li> <li>transfer: jhpce-transfer01.jhsph.edu</li> <li>web portal: jhpce-app02.jhsph.edu</li> </ul>","tags":["topic-overview"]},{"location":"access/access-overview/#ssh-is-the-primary-method","title":"SSH Is The Primary Method","text":"<p>Access to the JHPCE cluster requires the use of SSH.</p> <p>SSH stands for Secure SHell. SSH is actually a set of internet standard protocols. Programs implementing these protocols include both command line interface (CLI) tools and those with graphic user interfaces (GUI).  They all enable you to make secure, encrypted connections from one computer to the next.</p> <p>This document provides more information on SSH.</p>","tags":["topic-overview"]},{"location":"access/access-overview/#x11the-x-window-system","title":"X11/The X Window System","text":"<p>The X Window System (aka X11, aka X) is the primary GUI system for UNIX computers. X allows a program (an X \"client\") running on a UNIX computer in the cluster to be displayed on a remote computer (running an X \"server\"<sup>1</sup>) over the network. To use it, your local computer needs to have X server software installed on it.</p> <p>SSH provides support for tunnelling X11 over an encrypted connection. You may need to tell SSH that you want that service, by, for example, adding the -X flag to an ssh command in a macOS Terminal.</p> <ul> <li>macOS users need to install XQuartz from xquartz.org.</li> <li>Windows users need to use a program like MobaXterm (highly recommended) or Cygwin. We have a page describing installing and using MobaXterm.</li> <li>Linux laptop or desktop users are already using X as their windowing system.</li> </ul> <p>For more information, see our X11 document.</p>","tags":["topic-overview"]},{"location":"access/access-overview/#multi-factor-authentication-mfa","title":"Multi-factor authentication (MFA)","text":"<p>There are two basic \"factors\" required to log into a computer, whether your laptop or a remote UNIX cluster login node -- your username and a password. JHPCE requires the use of an additional factor, either a one-time password (OTP) six digit code, or the use of SSH key pairs. </p>","tags":["topic-overview"]},{"location":"access/access-overview/#one-time-passwords","title":"One Time Passwords","text":"<p>When you SSH into JHPCE, you will be prompted for a \u201cVerification Code:\u201d This is your cue to enter in a one-time password six digit code.</p> <p>Programs like <code>Google Authenticator</code> and <code>Micrsoft Authenticator</code> generate one-time password codes (OTP). These are only good for a single use, whether you successfully log in or not. Typically they are used to generate a stream of time-based OTPs, or TOTPs. These are only good for one minute, adding another layer of difficulty for someone trying to impersonate you.</p> <p>These programs are usually used on smartphones, but there are programs available to create them on laptops and desktops. The key with using ANY OTP program is to get it from a trusted source. We will default to mentioning the <code>Google Authenticator</code>.</p> <p>After you log into JHPCE for the first time, you should immediately configure your OTP program using a \"secret\" accessible to you on the cluster via the <code>auth_util</code> program. Instructions for doing that are found in the orientation documents.</p>","tags":["topic-overview"]},{"location":"access/access-overview/#web-portal","title":"Web Portal","text":"<p>We have a web server named jhpce-app02.jhsph.edu configured to offer a growing number of services. Click on the links to learn more.</p> <ul> <li>Reset your password or generate a OTP (learn more)</li> <li>Run applications on the cluster (RStudio, JupyterLab, VS Code) (learn more)</li> <li>Inspect a catalog of research databases (under development) (learn more)</li> </ul>","tags":["topic-overview"]},{"location":"access/access-overview/#safe-desktop","title":"SAFE Desktop","text":"<p>A virtual desktop named the Secure Analytic Framework Environment (SAFE) is a resource that some people find useful for their computing, as well as a means to access JHPCE (via the MobaXterm application). It is a virtual Windows computer equipped with many applications JHPCE members use for their research, including SAS and Stata. It includes 100GB of secure data storage for sensitive (PHI, PII) information. That data can be shared by research groups. Free for Johns Hopkins Medicine staff and students, it requires filling out a form and waiting for approval.</p>","tags":["topic-overview"]},{"location":"access/access-overview/#file-transfers","title":"File Transfers","text":"<p>We have a transfer server jhpce-transfer01.jhsph.edu for file transfers into and out of the cluster. It is connected by a 40G Ethernet link to Hopkins networks. This computer also offers a Globus Endpoint service (described here) for transfers from personal computers and other institutions.</p> <p>Transferring data into or out of the cluster is documented here.</p> <p>The login nodes SHOULD NOT be used for file transfers into and out of the cluster beyond extremely trivial cases. Their connections are four times slower and they are relied upon by all of your peers.</p> <p>Please use compute nodes for transfers WITHIN the cluster. For example, copying significant volumes of files from one file system to another, such as <code>/dcs05/a-place/</code> to <code>/dcs07/somewhere-else/</code>. Information about doing that can be found here</p> <ol> <li> <p>Note that X reverses the normal conception of client/server operation, which is that the remote computer is the \"server\". In X11, the \"server\" is the program receiving the keyboard and mouse inputs and displaying the output of remote \"client\" programs.\u00a0\u21a9</p> </li> </ol>","tags":["topic-overview"]},{"location":"access/file-transfer/","title":"File Transfer To and From the JHPCE Cluster","text":"<p>There are 2 general ways to think about transfering data to and from the JHPCE cluster. </p> <ul> <li>The most common way to think about tranferring data is where the transfer is initiated from some place outside of the JHPCE cluster, such as one's laptop or some other HPC system external to the cluster. </li> <li>The second approach to transferring data is where the connection is initiated from within the JHPCE cluster.  In this case one would have already logged into the cluster, and from within the cluster, one needs to reach out to some external location to copy data from that location into the cluster.</li> </ul> <p>In both cases, file transfers should be done through the JHPCE transfer node, and not the login node.  Details about using the transfer node are given below.  Also, in both scenarios, there are a slew ways that data transfer occurs.</p> <p>Transferring files within the cluster</p> <p>Copying files around inside the cluster, between JHPCE file systems, is a different activity. We have a document about using rsync to copy files. This tool can be used for both internal copies and for moving files into or out of the cluster.</p>"},{"location":"access/file-transfer/#transfering-data-from-outside-of-jhpce","title":"Transfering data from outside of JHPCE","text":"<p>For transferring files to and from the cluster, you should use the transfer node rather than the login node. The name of the transfer node is <code>jhpce-transfer01.jhsph.edu</code> (compared to the primary login node <code>jhpce01.jhsph.edu</code>). The transfer node is significantly faster, as the transfer node has a 40G Ethernet connection to the high-speed JHU ScienceDMZ  while the login nodes have 10G connections.  The use of the transfer node also relieves the login node from having to bear the stress of file transfer traffic, which can put a significant load on the network capacity of the node. Since the login node is used by everyone to access the cluster, data transfers through the login node can slow cluster access down for all users, and this makes users grumpy.</p> <p>A number of options exist for transfering files to-and-fro between JHPCE and your local host. Which solution you chose depends on your use case.</p>"},{"location":"access/file-transfer/#sftp-secure-file-transfer-program-or-protocol","title":"SFTP - Secure File Transfer Program (or Protocol)","text":"<p>The most common method for transferring files to and from JHPCE is with sftp.  The sftp command is part of the ssh suite of programs used for interacting securely with remote systems.  The sftp command allows you to transfer files to and from JHPCE over a secure encypted channel.</p>"},{"location":"access/file-transfer/#text-based-sftp-usage","title":"Text based SFTP usage","text":"<p>By default, sftp is a text-baseed interactive program. The \"sftp\" progam is installed by default on MacOS and Linux desktops, and is availble from running a Terminal on one's local system.  To use <code>sftp</code>, you would run the command (substituting JHPCE-USERID with your JHPCE login ID) :</p> <p><pre><code>sftp JHPCE-USERID@jhpce-transfer01.jhsph.edu\n</code></pre> and then proceed to login with your JHPCE password and Google Authenticator.</p> <p>Once you\u2019ve connected, you\u2019ll be shown an <code>sftp&gt;</code> prompt.  From here you can use the standard shell commands <code>ls</code> to get a directory listing, and <code>cd</code> to change directories.  While the <code>ls</code> and <code>cd</code> commands refer to your location on the cluster, there are sister commands <code>lls</code> and <code>lcd</code> to navigate on you local system.  In addition to the <code>ls</code> and <code>cd</code> commands, there are commands for actually doing the file transfers.</p> <ul> <li>You can use the <code>get</code> command to transfer a file from the cluster to your local system</li> <li>You can uhe <code>put</code> command to transfer a file from your local system to the cluster.</li> </ul> <p>Once you are done with <code>sftp</code>, you would type <code>exit</code> to end the session.</p> <p>Here is an example of using sftp from a terminal running on a MacOS system.</p> <pre><code>\nMyMac: sftp mmill116@jhpce-transfer01.jhsph.edu\n(mmill116@jhpce-transfer01.jhsph.edu) Verification code: (JHPCE Verification Code Typed In)\n(mmill116@jhpce-transfer01.jhsph.edu) Password: (JHPCE Password Typed In)\nConnected to jhpce-transfer01.jhsph.edu.\nsftp&gt; ls\n10billion                                                                       \n2024-09                                                                         \n500                                                                             \nBird.jpeg                                                                       \nCITATION                                                                        \nbin                                                                             \ndata                                                                            \njhpce-app                                                                       \nscripts                                                                         \ntmp                                                                             \nsftp&gt; cd tmp\nsftp&gt; ls\nACL                                     HLA                                     \na                                       a1                                      \naaa                                     aaaa                                    \nabc                                     acl-test                                \nshared                                  temp-26800-7554-21111.sam               \ntest-rsync                              test1                                   \ntest2                                   testhere                                \ntmp                                     tmp2                                    \ntmp3                                    x                                       \nzzz                                     zzzz                                    \n\nsftp&gt; lcd Documents\nsftp&gt; lls\nACL-CMS.txt\nScreenshot 2024-11-12 at 3.30.24\u202fPM.png\nScreenshot 2025-02-25 at 10.34.09\u202fAM.png\nScreenshot 2025-02-25 at 10.34.18\u202fAM.png\nScreenshot 2025-02-26 at 3.28.57\u202fPM.png\nZoom\nshow-example.txt\nsftp&gt; put show-example.txt\nUploading show-example.txt to /users/mmill116/tmp/show-example.txt\nshow-example.txt                          100% 4798   182.1KB/s   00:00    \nsftp&gt; ls\nACL                                     HLA                                     \na                                       a1                                      \naaa                                     aaaa                                    \nabc                                     acl-test                                \nshared                                  show-example.txt                    \ntemp-26800-7554-21111.sam               test-rsync                              \ntest1                                   test2                                   \ntesthere                                tmp                                     \ntmp2                                    tmp3                                    \nx                                       zzz                                     \nzzzz \nsftp&gt; exit\nMyMac: \n\n</code></pre>"},{"location":"access/file-transfer/#graphical-sftp-cyberduckmobaxterm","title":"Graphical SFTP - Cyberduck/MobaXterm","text":"<p>If you would rather use a graphical SFTP connection, wherein you can drag-and-drop files between your local system and the cluster you can use either MobaXterm on a Windows system, or Cyberduck on a Mac system.  We have an example of setting up SFTP in MobaXterm at on this page</p> <p>For Macos, we recommend Cyberduck from this site.  Cyberduck still uses sftp under the hood, but presents files and folders in a graphical window, allowing drag-and-drop operations between your local system and the JHPCE cluster.</p> <p>Below are the steps to setting up Cyberduck on your local MacOS system:</p> <ul> <li>Start up the Cyberduck app.</li> <li>Click on \"+\" at bottom left </li> <li>This will open a new window where you can configure your sftp session</li> <li>Select SFTP as the Protocol</li> <li>For Label, you can use JHPCE</li> <li>For Server, use \"jhpce-transfer01.jhsph.edu\"</li> <li>For \"username\" put your JHPCE username</li> <li>Keep the port as 22</li> <li>Do not enter a Password</li> <li>If you have an SSH key for the cluster select it in the SSH Private Key dropdown </li> <li>Close the dialog box by clicking on the Macos red \"Close\" button in the upper left.</li> <li>This will save a session \"Bookmark\" for you in the Cyberduck application</li> <li>You can now double-click on the saved session.</li> <li>You will be prompted for your Verification Code and Password</li> <li>At this point you will see your JHPCE home directory displayed, and you can drag-and-drop files between your Mac and the JHPCE cluster.</li> </ul> <p>One last note - we stronly recommend setting up SSH Keys SSH key pairs. Without keys, you will  be prompted for your verification code and password for each file you transfer.</p>"},{"location":"access/file-transfer/#scp","title":"SCP","text":"<p>The <code>scp</code> command can be thought of as a <code>network cp</code> command.  Like sftp, scp is part of the ssh suite of products.  The scp command is better for situation where you know exactly which files you want to copy, and don't need to navigate around the filesystems interactively with the sftp command.</p> <p>An example of the command to transfer a file called <code>data.txt</code> from your local system to your home directory on the cluster would be:</p> <pre><code>scp LOCAL_PATH/data.txt USERID@jhpce-transfer01.jhsph.edu:REMOTE_PATH/REMOTE_TARGET_FILENAME\n</code></pre> <p>where the paths default to your current local directory and home directory on the remote. The target filename if omitted will be the local filename.</p> <p>If you want to copy a file from the cluster to your local laptop/desktop, you would reverse the arguments. For example to copy <code>data2.txt</code> from the cluster to a local file:</p> <pre><code>scp USERID@jhpce-transfer01.jhsph.edu:REMOTE_PATH/data2.txt LOCAL_PATH/LOCAL_TARGET_FILENAME\n</code></pre>"},{"location":"access/file-transfer/#scp-vs-sftp-choosing-the-right-file-transfer-protocol-per-chatgpt","title":"SCP vs. SFTP: Choosing the Right File Transfer Protocol (per ChatGPT)","text":"<p>When transferring files to and from university servers, it's essential to use a secure and efficient method. The two most common protocols are SCP (Secure Copy Protocol) and SFTP (Secure File Transfer Protocol). Below, we compare their features, use cases, and best practices.</p>"},{"location":"access/file-transfer/#what-is-scp","title":"What is SCP?","text":"<p>SCP is a simple and fast protocol for securely transferring files between computers over SSH (Secure Shell). It is commonly used for one-time transfers when speed is a priority.</p>"},{"location":"access/file-transfer/#key-features-of-scp","title":"Key Features of SCP:","text":"<ul> <li>Fast and efficient for large files or bulk transfers.</li> <li>Encrypts data and authentication credentials.</li> <li>Command-line based; supports automation via scripts.</li> <li>No file management options\u2014only direct transfers.</li> </ul>"},{"location":"access/file-transfer/#when-to-use-scp","title":"When to Use SCP:","text":"<ul> <li>When you need a quick, one-time file transfer.</li> <li>If you\u2019re working primarily from the command line.</li> <li>When speed is more important than file management.</li> </ul>"},{"location":"access/file-transfer/#what-is-sftp","title":"What is SFTP?","text":"<p>SFTP is an SSH-based protocol that offers a more comprehensive solution for secure file transfers, allowing both file transfer and remote file management.</p>"},{"location":"access/file-transfer/#key-features-of-sftp","title":"Key Features of SFTP:","text":"<ul> <li>Secure and reliable, with encrypted transfers.</li> <li>Supports interactive file management (rename, delete, list directories).</li> <li>Compatible with both command-line and GUI clients.</li> <li>Slower than SCP for large file transfers due to additional protocol overhead.</li> </ul>"},{"location":"access/file-transfer/#when-to-use-sftp","title":"When to Use SFTP:","text":"<ul> <li>When you need to browse directories or manage files remotely.</li> <li>If you prefer using a graphical client like FileZilla or WinSCP.</li> <li>When working on projects that require frequent updates and file modifications.</li> </ul>"},{"location":"access/file-transfer/#which-one-should-you-use","title":"Which One Should You Use?","text":"Feature SCP SFTP Security \u2705 SSH encryption \u2705 SSH encryption Speed \ud83d\ude80 Faster for large files \ud83d\udc22 Slightly slower File Management \u274c No file operations \u2705 Rename, delete, list, etc. Ease of Use \u26a1 Command-line only \ud83d\udda5\ufe0f GUI support available Best for Quick transfers Ongoing file management"},{"location":"access/file-transfer/#rsync","title":"Rsync","text":""},{"location":"access/file-transfer/#globus","title":"Globus","text":"<p>Globus is a web-based interface for transferring files. We have a Globus endpoint on the JHPCE cluster. Please see this document.</p>"},{"location":"access/file-transfer/#transfering-data-from-inside-of-jhpce-to-the-outside-world","title":"Transfering data from inside of JHPCE to the outside world","text":"<p>The above examples are for initiating transfers from a location outside of the JHPCE cluster.  If though you are already on the cluster, and need to pull data from an external site, there are a number of methods for doing this.  For all of these methods, you will need to first utilize the JHPCE transfer node via eiher srun or sbatch with the \"--partition transfer\" partition option.</p>"},{"location":"access/file-transfer/#sftp-interactive-use-of-the-transfer-partition","title":"SFTP - Interactive use of the transfer partition","text":"<p>In addition to using SFTP to initiate transfers from your local system, you can use SFTP from within the JHPCE cluster on the transfer node.  For transferring the data from an external site into the JHPCE cluster with sftp, you would need to:</p> <ul> <li>login to the cluster as normal</li> <li>start an srun session on the transfer partition</li> <li>cd to the directory on the cluster that you want to store the data</li> <li>initiate the sftp to their site, logging in with the credentials they provide</li> <li>within sftp, cd to the directory on their site that has the data</li> <li>within sftp, use \u201cls\u201d to get a listing of files on their site</li> <li>use \u201cget\u201d or \u201cwget\u201d to transfer the files from their site to the JHPCE cluster</li> </ul> <p>Here is an example of what this would look like:</p> <pre><code>\nMac:[25/04/09 10:13] ~ $ ssh mmill116@jhpce01.jhsph.edu\nLast login: Tue Apr  8 17:04:18 2025 from 174.172.134.59\n\ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38\nUse of this system constitutes agreement to adhere to all\napplicable JHU and JHSPH network and computer use policies.\n\ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38\n\nNeed Help? bitsupport@lists.jh.edu for SLURM, cluster, storage &amp; login issues\n           bithelp@lists.jh.edu for application issues: R/SAS/python/modules...\n\nThe SLURM shared partition is currently at 68% core usage and 69% RAM usage.\n--------------------------------------------------------------------\nSLURM: \"slurmuser\" displays per-user cluster resource usage for running/pending\nSLURM: Run \"slurmuser -h\" to see important options.\n--------------------------------------------------------------------\n--------------------------------------\n     Your Home Directory Usage        \nUsername     Space Used         Quota     \nmmill116     230G               350G      \n--------------------------------------\n[mmill116@jhpce01 ~]$ srun --pty --partition transfer bash\n[mmill116@transfer-01 ~]$ cd /dcs07/smart/data/mmtest/\n[mmill116@transfer-01 mmtest]$ sftp demo@test.rebex.net\nThe authenticity of host 'test.rebex.net ()' can't be established.\nED25519 key fingerprint is SHA256:d7Te2DHmvBNSWJNBWik2KbDTjmWtYHe2bvXTMM9lVg4.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added 'test.rebex.net' (ED25519) to the list of known hosts.\nWelcome to test.rebex.net! See https://test.rebex.net/ for more information.\n(demo@test.rebex.net) Password: \nConnected to test.rebex.net.\nsftp&gt; ls\npub          readme.txt   \nsftp&gt; cd pub\nsftp&gt; ls\nexample   \nsftp&gt; ls -la \ndrwx------ 2 demo users          0 Mar 31  2023 .\ndrwx------ 2 demo users          0 Mar 31  2023 ..\ndrwx------ 2 demo users          0 Mar 31  2023 example\nsftp&gt; cd example\nsftp&gt; ls -la\ndrwx------ 2 demo users          0 Mar 31  2023 .\ndrwx------ 2 demo users          0 Mar 31  2023 ..\n-rw------- 1 demo users      36672 Mar 19  2007 KeyGenerator.png\n-rw------- 1 demo users      24029 Mar 19  2007 KeyGeneratorSmall.png\n-rw------- 1 demo users      11546 Mar 19  2007 ResumableTransfer.png\n-rw------- 1 demo users      80000 Mar 19  2007 WinFormClient.png\n-rw------- 1 demo users      17911 Mar 19  2007 WinFormClientSmall.png\n-r-------- 1 demo users      19156 Feb 16  2007 imap-console-client.png\n-r-------- 1 demo users      16471 Feb 16  2007 mail-editor.png\n-r-------- 1 demo users      35414 Feb 16  2007 mail-send-winforms.png\n-r-------- 1 demo users      49011 Feb 16  2007 mime-explorer.png\n-rw------- 1 demo users      58024 Mar 19  2007 pocketftp.png\n-rw------- 1 demo users      20197 Mar 19  2007 pocketftpSmall.png\n-r-------- 1 demo users      20472 Feb 16  2007 pop3-browser.png\n-r-------- 1 demo users      11205 Feb 16  2007 pop3-console-client.png\n-rw------- 1 demo users        379 Sep 19  2023 readme.txt\n-rw------- 1 demo users       2635 Mar 19  2007 winceclient.png\n-rw------- 1 demo users       6146 Mar 19  2007 winceclientSmall.png\nsftp&gt; get readme.txt\nFetching /pub/example/readme.txt to readme.txt\nreadme.txt                                    100%  379     1.8KB/s   00:00    \nsftp&gt; exit\n[mmill116@transfer-01 mmtest]$ ls\nreadme.txt  test-file  test-file2  test-file3\n[mmill116@transfer-01 mmtest]$ more readme.txt \nWelcome to test.rebex.net!\n\nYou are connected to an FTP or SFTP server used for testing purposes\nby Rebex FTP/SSL or Rebex SFTP sample code. Only read access is allowed.\n\nFor information about Rebex FTP/SSL, Rebex SFTP and other Rebex libraries\nfor .NET, please visit our website at https://www.rebex.net/\n\nFor feedback and support, contact support@rebex.net\n\nThanks!\n[mmill116@transfer-01 mmtest]$"},{"location":"access/file-transfer/#wget-and-curl","title":"wget and curl","text":"<p>The wget and curl commands can be used to download files from a web site.  They work in a similar manner, and you will need to know the full URL path to the file.  The main difference is that wget will save the data downloaded to a file, and curl will display the file on standard output, so you'll likely need to redirect the output to a file to save it.</p>\n<p>Here are examples:</p>\n<pre><code>\n[mmill116@transfer-01 mmtest]$ wget https://ftp.ncbi.nlm.nih.gov/genomes/check.txt\n--2025-04-09 15:16:15--  https://ftp.ncbi.nlm.nih.gov/genomes/check.txt\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.11, 130.14.250.12, 130.14.250.7, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.11|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 73071 (71K) [text/plain]\nSaving to: \u2018check.txt\u2019\n\ncheck.txt                100%[=================================&gt;]  71.36K  --.-KB/s    in 0.001s  \n\n2025-04-09 15:16:16 (119 MB/s) - \u2018check.txt\u2019 saved [73071/73071]\n\n[mmill116@transfer-01 mmtest]$ ls -al check.txt\n-rw-r--r-- 1 mmill116 smart 73071 Jul  6  2022 check.txt\n\n[mmill116@transfer-01 mmtest]$ curl https://ftp.ncbi.nlm.nih.gov/genomes/species.diff.txt &gt; species.diff.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 49453  100 49453    0     0  52834      0 --:--:-- --:--:-- --:--:-- 52778\n[mmill116@transfer-01 mmtest]$ ls -la species.diff.txt\n-rw-r--r-- 1 mmill116 smart 49453 Apr  9 15:16 species.diff.txt\n</code></pre>"},{"location":"access/file-transfer/#batch-use-of-the-transfer-nodepartition","title":"Batch use of the transfer node/partition","text":"<p>In addition to using <code>srun</code> to perform file transfers interactively, you can create a bash shell script with the\ncommands needed to perfomr the file transfers, and then use <code>sbatch</code> to submit this as a batch job.  This is especially\nhelpful, if not required, for long running file transfers, where an srun session might get interrupted.</p>\n<p>Here is an example of a job that uses \"wget\" to do a file transfer in batch mode.</p>\n<pre><code>\n[mmill116@jhpce01 ~]$ cat wget-example.sh\n#!/bin/bash\n#SBATCH --partition=transfer\n#SBATCH --time=4-00:00:00\n\nwget -O $MYSCRATCH/SRR026674.fastq.gz \"https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?cmd=dload&amp;run_list=SRR026674&amp;format=fastq\"\n[mmill116@jhpce01 ~]$ sbatch wget-example.sh\nSubmitted batch job 15916781\n[mmill116@jhpce01 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n[mmill116@jhpce01 ~]$ more slurm-15916781.out \n--2025-04-09 16:15:32--  https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?cmd=dload&amp;run_list=SRR02\n6674&amp;format=fastq\nResolving trace.ncbi.nlm.nih.gov (trace.ncbi.nlm.nih.gov)... 130.14.29.113, 2607:f220:41e:4290::113\nConnecting to trace.ncbi.nlm.nih.gov (trace.ncbi.nlm.nih.gov)|130.14.29.113|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /Traces/index.html?cmd=dload&amp;format=fastq&amp;run_list=SRR026674 [following]\n--2025-04-09 16:15:33--  https://trace.ncbi.nlm.nih.gov/Traces/index.html?cmd=dload&amp;format=fastq&amp;ru\nn_list=SRR026674\nReusing existing connection to trace.ncbi.nlm.nih.gov:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 5713 (5.6K) [text/html]\nSaving to: \u2018/fastscratch/myscratch/mmill116/SRR026674.fastq.gz\u2019\n\n     0K .....                                                 100%  134M=0s\n\n2025-04-09 16:15:33 (134 MB/s) - \u2018/fastscratch/myscratch/mmill116/SRR026674.fastq.gz\u2019 saved [5713/5\n713]\n\n[mmill116@jhpce01 ~]$ ls -la /fastscratch/myscratch/mmill116/SRR026674.fastq.gz\n-rw-r--r-- 1 mmill116 mmi 5713 Dec 11 11:59 /fastscratch/myscratch/mmill116/SRR026674.fastq.gz\n[mmill116@jhpce01 ~]$ \n</code></pre>"},{"location":"access/file-transfer/#rcloneonedrive","title":"Rclone/Onedrive","text":"<p>Rclone can be used to access network file resources, such as OneDrive,\nGoogle Drive, and AWS. See here for instructions on using it to connect to Hopkins OneDrive storage.</p>"},{"location":"access/file-transfer/#aspera","title":"Aspera","text":"<p>Aspera is a commercial product from IBM that some site used to serve data\nout from their sites.  Aspera (per their marketing) allows file transfers that are\nreportedly 20 times faster than <code>ftp</code>.  Documentation is available for macOS, Linux and Windows.</p>\n<p>Aspera is required to download data from the\nNCBI Aspera server or download/upload data from/to JHU CIDR on the\nBayview campus.  Several other companies use Aspera servers for alowing file downloads.</p>\n<p>The Aspera license does not allow us to install the client for our users. You must install it\nyourself. You can download the Aspera Connect Linux Client from the Aspera Download site. This will download a \"tar.gz\" file that you can then upload to your account on the JHPCE cluster, with sftp or scp.</p>\n<p>You will then need to extract and run the install script from the tar.gz file.  For example:</p>\n<pre><code>tar xvf ibm-aspera-connect_4.2.13.820_linux_x86_64.tar.gz\nbash ibm-aspera-connect_4.2.13.820_linux_x86_64.sh\n</code></pre>\n<p>This will install the <code>ascp</code> command under your home directory at  <code>~/.aspera/connect/bin</code>.  You can either add <code>~/.aspera/connect/bin</code> to your <code>PATH</code>, or use the full path to the <code>ascp</code> command to run it.</p>\n<p>You may also need to do other steps, such as install an extension to your web browser. Instructions on how to do that for Linux, as an example, are available from IBM here.</p>"},{"location":"access/file-transfer/#unison","title":"Unison","text":"<p>Warning</p>\n<p>We no longer have unison installed in the cluster. You can install your own copy of it.</p>\n\n<p>Using Unison, you can keep data synchronized between directories, including ones on a single computer or between the cluster and on your local system. Both CLI and GUI versions are available. Unison needs to be installed on both computers if used across a network.</p>\n<p>Unison is a synchronization tool. It can be told to update files in both SOURCE and DESTINATION locations according to some rules.</p>\n<p>Unison home page is here with a wiki that provides access to documentation and some binaries.</p>\n<p>An extensive tutorial at ostechnix.</p>\n<p>A wiki about using it from ArchLinux.</p>\n<p>This document written by a previous JHPCE user (Jacob Fiksel) might still be useful.</p>"},{"location":"access/file-transfer/#git","title":"git","text":"<p>You can clone a git repository from a github site to the JHPCE cluster using the \"git\" command.  For example</p>\n<pre><code>\n[mmill116@transfer-01 mmtest]$ git clone https://github.com/ruanyf/simple-bash-scripts\nCloning into 'simple-bash-scripts'...\nremote: Enumerating objects: 344, done.\nremote: Counting objects: 100% (74/74), done.\nremote: Compressing objects: 100% (23/23), done.\nremote: Total 344 (delta 55), reused 54 (delta 51), pack-reused 270 (from 2)\nReceiving objects: 100% (344/344), 69.19 KiB | 13.00 KiB/s, done.\nResolving deltas: 100% (102/102), done.\nUpdating files: 100% (51/51), done.\n[mmill116@transfer-01 mmtest]$ cd simple-bash-scripts\n[mmill116@transfer-01 simple-bash-scripts]$ ls\nREADME.md  scripts\n[mmill116@transfer-01 simple-bash-scripts]$ cd scripts/\n[mmill116@transfer-01 scripts]$ ls\naddition.sh             directorysize.sh     lowercase.sh        subtraction.sh\naffect.sh               disk-space.sh        multiplication.sh   table.sh\narchive-and-encrypt.sh  division.sh          pomodoro.sh         test-file.sh\narmstrong.sh            encrypt.sh           prime.sh            thumbnail.sh\nbinary2decimal.sh       evenodd.sh           process.sh          traps.sh\ncalculator.sh           factorial.sh         random-emoji.sh     up.sh\ncollectnetworkinfo.sh   fibonacci.sh         randomfile.sh       versioncompare.sh\ncolor.sh                get-temperature.sh   rang-random.sh      weather.sh\nconvertlowercase.sh     hardware_machine.sh  read-menu.sh        whereIP.sh\ncount-lines.sh          hello-world.sh       remotebackup.sh     while-menu.sh\ncpu.sh                  hextodec.sh          server-health.sh    while-read.sh\ndec2hex.sh              interactive.sh       simplecalc.sh\ndecimal2binary.sh       list-dir.sh          special-pattern.sh\n[mmill116@transfer-01 scripts]$ bash color.sh\n  LOVE  [mmill116@transfer-01 scripts]$ \n[mmill116@transfer-01 scripts]$ \n</code></pre>"},{"location":"access/file-transfer/#rsync_1","title":"rsync","text":"<p>We have a good document about using rsync to copy files.</p>"},{"location":"access/file-transfer/#ftp","title":"ftp","text":"<p>ftp is an older, less secure, unencrypted channel for transferring files.\nHowever if you are downloading files from an older site that does not\nsupport SFTP or one of the other more modern mechanisms, you can use\nftp.  Ftp works in a similar interactive manner as sftp - see above\nHere is an example of using ftp:</p>\n<pre><code>[mmill116@transfer-01 mmtest]$ ftp ftp-air.larc.nasa.gov\nftp: ftp-air.larc.nasa.gov: Name or service not known\nftp&gt; exit\n[mmill116@transfer-01 mmtest]$ ftp wuarchive.wustl.edu\nftp: wuarchive.wustl.edu: Name or service not known\nftp&gt; exit\n[mmill116@transfer-01 mmtest]$ ftp ftp.ngdc.noaa.gov\nConnected to ftp.ngdc.noaa.gov (140.172.190.215).\n220-                    ----- Notice -----\n220-\n220- You are accessing a U.S. Government information system, which includes:\n220- 1) This computer, 2) This computer network, 3) All computers connected\n220- to this network, and 4) All devices and storage media attached to this\n220- network or to a computer on this network.\n220- \n220- You understand and consent to the following:\n220- you may access this information system for authorized use only; you have\n220- no reasonable expectation of privacy regarding any communication of data\n220- transiting or stored on this information system; at any time and for any\n220- lawful Government purpose, the Government may monitor, intercept, and\n220- search and seize any communication or data transiting or stored on this\n220- information system; and any communications or data transiting or stored\n220- on this information system may be disclosed or used for any lawful\n220- Government purpose.\n220- \n220- This information system may contain Controlled Unclassified Information\n220- (CUI) that is subject to safeguarding or dissemination controls in\n220- accordance with law, regulation, or Government-wide policy. Accessing\n220- and using this system indicates your understanding of this warning.\n220-\n220-                    ----- Notice -----\n220-\n220- Questions/Problems should be directed to ncei.webmaster@noaa.gov\n220 \nName (ftp.ngdc.noaa.gov:mmill116): anonymous\n331 Please specify the password.\nPassword:\n230 Login successful.\nRemote system type is UNIX.\nUsing binary mode to transfer files.\nftp&gt; ls\n227 Entering Passive Mode (140,172,190,215,187,166).\n150 Here comes the directory listing.\ndrwxrwxr-x   32 ftp      ftp            32 Sep 10  2012 DMSP\n-rw-rw-r--    1 ftp      ftp          1516 Feb 04  2016 INDEX.txt\n-rw-rw-r--    1 ftp      ftp          3766 Feb 09  2018 README.txt\ndrwxrwsr-x   26 ftp      ftp            29 Oct 30 20:51 STP\ndrwxr-xr-x    2 ftp      ftp             2 Sep 17  2003 Snow_Ice\ndrwxrwxr-x    2 ftp      ftp             2 Jun 22  2004 Solid_Earth\ndrwxr-xr-x    3 ftp      ftp             3 Aug 07  2017 coastwatch\ndrwxr-xr-x    2 ftp      ftp             2 Jul 22  2016 dmsp4alan\n-rw-rw-r--    1 ftp      ftp          9036 Feb 04  2016 ftp.html\ndrwxrwsr-x   11 ftp      ftp            11 Dec 19  2017 geomag\n-rw-r--r--    1 ftp      ftp            53 Jul 27  2010 google12c4c939d7b90761.html\nlrwxrwxrwx    1 ftp      ftp             8 Aug 01  2011 index.html -&gt; ftp.html\ndrwxr-sr-x    2 ftp      ftp             2 Nov 25  2003 international\ndrwxr-xr-x   21 ftp      ftp            21 Jan 03  2022 ionosonde\ndrwxrwxr-x    4 ftp      ftp             6 Dec 15  2011 mgg\ndrwxrwxr-x    8 ftp      ftp             9 Dec 12  2014 pub\ndrwxrwsr-x    4 ftp      ftp             4 Nov 08  2010 wdc\n226 Directory send OK.\nftp&gt; exit\n221 Goodbye.\n[mmill116@transfer-01 mmtest]$ \n</code></pre>"},{"location":"access/file-transfer/#write-your-own-programs-in-python-or-perl-or-c-or-r","title":"Write your own programs in python or perl or c or R ....","text":"<p>No examples here... but all programming languages provide some function call or library or module to do file downloads from within the code itself.</p>"},{"location":"access/file-transfer/#old-stuff","title":"Old Stuff","text":"<ul>\n<li>Mount remote filesystems \u2014 directories at JHPCE mounted on your local host. IS THIS MATERIAL STILL ACCURATE IN 2024? Is this example SSHFS doc worth re-using? What about Mountain Duck?</li>\n</ul>"},{"location":"access/globus/","title":"Using Globus to transfer files","text":"<p>Globus is a \u201cdropbox-like\u201d service to enable data sharing between academic and research communities. By using Globus, you can easily transfer files between Globus endpoints, share data with collaborators, and easily transfer data between the JHPCE cluster and your local desktop or laptop.</p>"},{"location":"access/globus/#setting-up-an-account","title":"Setting up an account","text":"<p>The first step in using Globus is setting up and account on the Globus site.\u00a0 To start, go to https://www.globus.org/ and clicking on the \u201cLog in\u201d button in the upper right. You should now see the screen below. Select \u201cJohns Hopkins\u201d from the list of Organizations.</p> <p></p> <p>You will now be directed to the JHU Login Screen. Enter your JHED ID and Password:</p> <p>Once you enter your JHED information you will be sent to the main Globus window:</p> <p></p>"},{"location":"access/globus/#transferring-files","title":"Transferring Files","text":"<p>With Globus, you can transfer data between nodes (known as \u201cEndpoints\u201d) that are part of the Globus network. The endpoint for the JHPCE cluster is called <code>jhpce#globus01</code>. To connect to the JHPCE endpoints, enter <code>jhpce#globus01</code> in the \u201cCollections field, and select it from the list of results displayed.</p> <p></p> <p>Once you select the <code>jhpce#globus01</code> endpoint, you will be prompted to enter your JHPCE Login and Password:</p> <p></p> <p>And once you login you will be shown a flie list of your home directory on the JHCPE cluster.</p> <p></p> <p>In order to transfer data to and from your desktop, you will need to install the Globus Connect Personal package (https://www.globus.org/globus-connect-personal) on your desktop. To do this, click on the \u201cTwo-Panel\u201d icon at the top of your globus session, and then click on Install Globus Connect Personal.</p> <p></p> <p>On the next screen, follow the steps on the next page to download the appropriate software package for your system, generate a Globus Key, and create a Globu name for your desktop/laptop.\u00a0</p> <p>Once you install and start Globus Connect Personal, you will be able to easily transfer files between your desktop/laptop and the JHPCE cluster via the Globus web interface.</p>"},{"location":"access/globus/#sharing-data-with-others","title":"Sharing data with others","text":"<p>One of the benefits fo using Globus is that you can share data from the JHPCE cluster with outside collaborators. To do this navigate to the directory you wish to share, select it, and either Right-Click on it, or select \u201cShare\u201d from the menu on the right hand side of the screen. In this example, I\u2019m sharing the <code>$HOME/class-scripts/R-demo</code> directory from within\u00a0my home directory.</p> <p>Next, you will be prompted to provide a name for your share.\u00a0 In this example, I\u2019m calling it \u201cMyRDemo\u201d.\u00a0 Once you enter the name and Description, click \u201cCreate Share\u201d:</p> <p></p> <p>Next you will be shown the current access permissions, which should be just for your account to start with.\u00a0 To grant others permission to access your share, click on \u201cAdd Permissions \u2013 Share With\u201d</p> <p>You will now be able to select which users you wish to share your directory with.\u00a0 The person you are sharing with must have a Globus account, and will need to provide their Globus ID or email with you.\u00a0 Enter their Username or Email, click \u201cAdd\u201d, and then click \u201cAdd Permission\u201d.\u00a0 We strongly recommend that you only grant \u201cread\u201d permission. for your share.</p> <p>Alternatively. you can create an \u201canonymous share\u201d to share a directory with anyone on Globus. To do this select \u201call users\u201d. Again, we strongly recommend that you only grant \u201cread\u201d permission for your share.</p> <p></p> <p>Once your share is created, you can notify your collaborators that they can access your data by using the Share name you created (in this example it\u2019s \u201cMyRDemo\u201d), and can search for your share name.</p> <p></p>"},{"location":"access/mobaxterm/","title":"Mobaxterm Configuration","text":"<p>Mobaxterm is a Windows application that provides an ssh client, scp client and X11 server all in one program.\u00a0 It is a very convenient tool for accessing the JHPCE cluster and utilizing the many features of the cluster.\u00a0 There is some configuration that needs to be done though in order to effectively use Mobaxterm in the JHPCE environment.\u00a0 This FAQ will take you through the steps needed to configure Mobaxterm.\u00a0 Before your proceed you should have your Google Authenticator app available.</p>","tags":["in-progress"]},{"location":"access/mobaxterm/#download","title":"Download","text":"<p>The first thing you will need to do is download the MobaXterm program from their web site http://mobaxterm.mobatek.net/download-home-edition.html</p> <p>Be sure to use the \"Installer Edition\" instead of the \"Portable Edition\"</p> <p></p> <p>Once the program has been downloaded, install it as you would any other Windows program.</p>","tags":["in-progress"]},{"location":"access/mobaxterm/#configuring-ssh-sessions","title":"Configuring SSH Sessions","text":"<p>Once the program is installed, start the MobaXterm program. You should see a screen like this:</p> <p></p> <p>From this screen, click on the \"Sessions\" icon </p> <p></p> <p>in the upper left corner.</p> <p>On the \"Session settings\" screen, click on \"SSH\"</p> <p></p> <p>Enter \"jhpce01.jhsph.edu\" as the \"Remote host\". Click on the \"Specify username\" checkbox, and enter your JHPCE username in the next field. Then click the \"OK\" button.</p> <p>When you click OK, you will initiate an SSH session to the JHPCE cluster. You will be prompted for your Google Authenticator \"Verification Code\", and then your password.</p> <p>Once you enter your password correctly, you will see a number of boxes pop up (usually 3) prompting for another Verification Code. Click \"Cancel on these boxes. You will then be prompted to save your password. In the lower left, check the box that says \"Do not ask this again\" and then click \"No\". (We will get rid of these annoying boxes in a couple of steps).</p> <p></p> <p>At this point you should be logged into the JHPCE cluster and sitting at a shell prompt.</p> <p>After you exit out of the JHPCE cluster, a \"jhpce01\" session will be saved as a \"Saved Sessions\".\u00a0 To login again, double-click on the jhpce01 \"Saved Session\", and you should then be prompted for \"Verification Code\" (which will come from Google Authenticator) and \"Password:\"</p>","tags":["in-progress"]},{"location":"access/mobaxterm/#configuring-sftp-sessions","title":"Configuring SFTP Sessions","text":"<p>If you use MobaXterm to access the JHPCE cluster, and you want to transfer large files to ofr from the cluster, you should set up a separate SFTP session in MobaXterm using the \u201cjhpce-transfer01.jhsph.edu\u201d data transfer node, rather than the \u201cjhpce01.jhsph.edu\u201d login node. The \u201cjhpce01.jhsph.edu\u201d node is only meant to be used for logging into the cluster, and not transferring large files. The \u201cjhpce01.jhsph.edu\u201d node only has a 1 Gb/s network connection to the JHU network, whereas the \u201cjhpce-transfer01.jhsph.edu\u201d node has a 40 Gb/s connection. While an individual file transfers would not be able to achieve the full 40Gb/s speed, it will be significantly faster than using the 1Gb/s connection on the \u201cjhpce01.jhsph.edu\u201d</p> <p>You can set up an SFTP session in MobaXterm using the following steps.</p> <ol> <li>Startup MobaXterm.  You likely already have a \u201cjhpce01.jhsph.edu\u201d session configured for logging into the JHPCE cluster </li> <li>Click on the \u201cSession\u201d icon in the upper left corner.  This will bring up a    window where you can create new sessions. </li> <li>Select \u201cSFTP\u201d </li> <li>In the \u201cRemote host\u201d field, enter jhpce-transfer01.jhsph.edu.  In the    \u201cUsername\u201d field, enter your JHPCE user ID. </li> <li>Click on the \u201cAdvanced Sftp Setting\u201d tab. </li> <li>Check the box marked \u201c2-steps authentication\u201d.  Optionally, if you have an    SSH key, you can check the \u201cUse private key\u201d box, and then enter the path to your private key. </li> <li>When you click \u201cOK\u201d, you will be prompted for you \u201cVerification Code\u201d (which    will be from Google Authenticator) and \u201cPassword\u201d.  After you enter your password, you will be prompted to \u201cSave Password?\u201d, and be sure to respond \u201cNo\u2019.  </li> <li>You should now see the list of files your home directory from the JHPCE    cluster displayed on the screen.  You can transfer files to and from the JHPCE cluster by dragging-and-dropping them onto this file list. </li> <li>To access you scratch space, you can enter the path    \u201c/fastscratch/myscratch/USERID\u201d (where USERID is your JHPCE cluster user id.</li> <li>When you are done with your sftp session, you can close the session by     clicking on the red X on the \u201cjhpce-transfer01\u201d tab.  This red X will show up when you hover your cursor over the right hand side of the tab.</li> </ol>","tags":["in-progress"]},{"location":"access/mobaxterm/#optional-setting-up-ssh-keys-in-mobaxterm","title":"OPTIONAL -- Setting up SSH Keys in MobaXterm:","text":"<p>To make logging in more streamlined and avoid the pop-up windows when you login, you can create an SSH key pair in MobaXterm. Before starting you should login to the JHPCE cluster in MobaXterm using your Google Authenticator and Password. Once you are logged in:</p> <p>Click on \"Tools -&gt; MobaKeyGen\"</p> <p></p> <p>You should then see the \"MobaXterm SSH Key Generator\" Screen. Click on \"Generate\", and you will be prompted to move the mouse around to generate random data.</p> <p></p> <p>Move your mouse until the green bar fills up.</p> <p></p> <p>Once the green bar fills up, you should see a populated screen.</p> <p></p> <p>For security purposes, we strongly recommend you protect your key with a password.\u00a0 To do so, enter a password in the \"Key passphrase\" and \"Confirm passphrase\" boxes. Next, click \"Save private key\", and save the key to a know locationon your local laptop/desktop (such as you \"Documents directory).</p> <p>Now, at the top of the window you'll see the text version of your public key. Copy the contents of this output with your mouse, making sure to scroll all the way to the bottom of the text box.\u00a0 NOTE: to do Copy/Paste in MobaXterm, you should not use \\&lt;CTRL&gt;-C and \\&lt;CTRL&gt;-V. Instead, select the text you want to copy, then use the right mouse button to bring up the context menu, and select\u00a0 Copy or select Paste when you are pasting.</p> <p>Now, go back to the tab where your JHPCE ssh session is running. From your home directory, cd into the .ssh directory. In this directory, you will need to update the file <code>authorized_keys</code>. Edit this file with your text editor of choice (nano, vi, emacs) as shown below. If the file does not exist, you can also create the file with the command below.</p> <p></p> <p>Paste in the public key that you copied from your local session. Depending on your editor, the new key may only show up on one long line, or it may wrap to multiple lines. Save the \"authorized_keys\" file when you are done.</p> <p></p> <p>If you assigned a passphrase to your key (and you really should have) we need to make a couple of extra steps to allow the passphrase to be entered only once, instead of every time you start a new login session.</p> <ul> <li>First go to \"Settings-&gt;Configuration\" and go to the \"General\" tab and click on \"MobaXterm password management\"</li> <li>At the top of the window where it says \"Save sesison passwords\", you should click \"Ask\"</li> <li>Also be sure to check the \"Save SSH keys passphrase as well\" box</li> <li>Then click \"OK\"</li> </ul> <p></p> <ul> <li>Next on the \"Configuration Window\" go to the \"SSH\" tab, and at the    bottom of the screen check the \"Use internal SSH agent \"MobAgent\"</li> <li>Just below this checkbox, click the \"+\" sign on the right side of    the \"Load Following Keys\" screen, and navigate to your \"Private    Key\", and select it.</li> </ul> <p></p> <p>Then click OK. You will be prompted to restart Mobaxterm. Go ahead and restart it. When you MobaXterm restarts, you will be prompted to enter your passphrase for your private key.</p> <p>The final step will be to add your key to the JHPCE session in the MobaXerm application. On the left pane of MobaXterm, you should see a list of \"Saved sessions\", including a session for the \"jhpce01\" login node. Right-Click on the \"jhpce01\" session, and select \"Edit Session\". This will open a window that looks like:</p> <p></p> <ul> <li>Select the \"Use private key\" checkbox.</li> <li>The field next to the checkbox should populate with a path to your local private key. If it does not, or it is not the correct path, then click the blue icon on the right side of the field, and navigate to the location of your \"private key\" file.</li> </ul> <p></p> <ul> <li>Click OK to save your changes.</li> <li>Now, in the left pane of Saved Sessions, you should be able to double click on the \"jhpce01\" session, and a new tab should open up, and log you into the JHPCE cluster without having to enter a password or Google Authenticator PIN.</li> <li>Once you have verified that you can login, exit out of all of your SSH sessions, and close the MobaXterm app.\u00a0 Reopen the MobaXterm application, double click on the \"jhpce01\" session.\u00a0 As before, a new tab should open up, and log you into the JHPCE cluster without having to enter a password or Google Authenticator PIN. </li> </ul>","tags":["in-progress"]},{"location":"access/onedrive/","title":"Using rclone to access OneDrive","text":"<p>Below is an example of using rclone to access the OneDrive network resource on the JHPCE cluster.  The initial setup is a bit involved, but regular operation is fairly straightforward.</p>"},{"location":"access/onedrive/#one-time-configuration","title":"One-time Configuration","text":"<p>Before you start, you will need to have an X11 graphical environment set up either by using MobaXterm on a Windows system or Xquartz on a Mac.  </p> <p>To start using rclone to access Onedrive, login to the cluster as normal, and then srun into a compute node with a 10G RAM request (srun --pty --x11 --mem=10G bash ) .  </p> <p>Part of the rclone setup process will involve using a web browser to generate an authentication key.  Before starting the rclone configuration process, you should set your web browser on the cluster to use Chromium with the <code>xdg-settings</code> command.  When the browser does start, you may see a stream of warning message about \u201clibGL errors\u201d, but these are because we are using X11 forwarding and not a local graphics card, and assuming the browser starts up acter a few seconds, those messages can be ignored.</p> <pre><code>[compute-113 /users/bob]$ xdg-settings set default-web-browser chromium-browser.desktop\n</code></pre> <p>Now, from your srun session, you will need to run \u201cmodule load rclone\u201d, and then run \u201crclone config\u201d to begin the rclone setup.</p> <pre><code>[jhpce01 /users/bob]$ srun --pty --x11 --mem=10G bash\n[compute-113 /users/bob]$ module load rclone\n[compute-113 /users/bob]$ rclone config\nNOTICE: Config file \"/users/bob/.config/rclone/rclone.conf\" not found - using defaults\"\nNo remotes found - make a new on\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\n</code></pre> <p>When prompted to \u201cmake a new remote\u201d, enter \u201cn\u201d for \u201cnew remote\u201d. When prompted for a name, enter something descriptive, like \u201cOneDrive\u201d. Next, you will be presented with a long list of storage types.</p> <p></p> <p>. . .</p> <p></p> <p>. . .</p> <p></p> <p>When prompted for the type of storage to use, enter \u201conedrive\u201d. When prompted for \u201cOauth Client ID\u201d, just hit enter. When prompted for \u201cOauth Client Secret\u201d, just hit enter. When prompted to \u201cEdit advanced config\u201d, just hit enter to use the default \u201cNo\u201d answer. When prompted to \u201cUse auto config?\u201d, enter \u201cy\u201d. At this point you\u2019ll see a URL with \u201chttp://127.0.0.1\u201d in the address, and get a message \u201cWaiting for code\u2026\u201d</p> <p></p> <p>Also at this point, the Chromium browser should open a new tab, and start going to the http://127.0.0.1 address, which should redirect you to the Microsoft login page.</p> <p></p> <p>From here, you should enter \u201cJHEDID@jh.edu\u201d. Where of course you specify your own JHEDID. You will then be sent to the familiar \u201cJohns Hopkins\u201d JHED Login screen, where you should enter your JHED password.</p> <p></p> <p>When prompted to \u201cSave your login\u201d, you should select \u201cDon\u2019t Save\u201d. You should then see in the web browser display, a \u201cSuccess!\u201d message, and in your \u201csrun\u201d session, you should see the message \u201cGot code\u201d, and then a selection of OneDrive site options. You should select option \u201c1\u201d for \u201conedrive\u201d.</p> <p></p> <p>Next, you should see a message where you can select which drive to use. There should only be one drive, so select \u201c0\u201d. You will also get a confirmation message, and you should select \u201cy\u201d.</p> <p></p> <p>At this point a summary of the configuration will be displayed, and you should select \u201cy\u201d to accept the configuration. Finally, you can enter \u201cq\u201d to quit the config process, and you can also close the web browser.</p> <p></p> <p>At this point your OneDrive connection has been configured, and you can start to access your OneDrive.</p>"},{"location":"access/onedrive/#regular-operation","title":"Regular Operation","text":"<p>To access your OneDrive, you\u2019ll use the \u201crclone\u201d command with various options. The most often used commands are \u201crclone lsd\u201d to list directories, \u201crclone ls\u201d to recursively list files (this can take a long time if you have a lot of files in OneDrive and you are listing the top level directory), and \u201crclone copy\u201d to copy data between the cluster and your OneDrive.</p> <p>An example of \u201crclone lsd\u201d is below. There are a couple of key items to note. </p> <ul> <li>First, the name of the argument following \u201clsd\u201d should be the same name your used for your OneDrive config. You can run \u201crclone listremotes\u201d to see the name you used. </li> <li>The second item to note is that the name of your remote must end in a colon.</li> <li>Thirdly, if you will be copying large amounts of data, this should be run from the transfer node, as seen below.</li> </ul> <p><pre><code>[jhpce01 /users/bob]$ srun --pty --x11 -p transfer bash\n[transfer-01 /users/bob]$ module load rclone\n[transfer-01 /users/bob]$ rclone lsd OneDrive:\n          -1 2019-12-21 01:54:32         2 BoxMigration\n          -1 2014-11-05 17:23:32        25 Documents\n          -1 2014-11-05 17:22:55       173 HomeDir\n          -1 2021-01-19 13:12:36         2 JHPCE-Billing-Videos\n          -1 2015-12-03 11:37:36         1 Lustre\n          -1 2019-07-11 17:25:16        10 OLDVMs\n          -1 2014-11-05 15:15:11         1 Shared with Everyone\n          -1 2019-07-11 12:13:21         0 USB\n</code></pre> To see the files in a particular directory, you would use \u201crclone ls\u201d and supply a directory name after the colon.</p> <pre><code>[compute-113 /users/bob]$ rclone ls OneDrive:Documents\n    84842 FY2014Q3 JHPCE Charges.xlsx\n    89229 FY2014Q4 JHPCE Charges.xlsx\n    57580 Globus-compute-022.rtf\n    43103 HPSCC Expenses.xlsx\n   684491 JHPCE-Overview-2014.pdf\n  1061265 JHPCE-Overview-2014.pptx\n    71168 JHU-Intel-Test-Cluster-Access.xls\n. . .\n   410012 pg10031.txt\n  4454050 pg31100.txt\n  1418582 pg6400.txt\n      426 plot1.r\n       20 plot1.sh\n       83 test1.sh\n[compute-113 /users/bob]$ \n</code></pre> <p>Finally \u201crclone copy\u201d can be used to transfer files between your OneDrive and the JHPCE cluster.</p> <pre><code>[compute-113 /users/bob]$ ls pg6400.txt\nls: cannot access pg6400.txt: No such file or directory\n[compute-113 /users/bob]$ rclone copy OneDrive:Documents/pg6400.txt .\n[compute-113 /users/bob]$ ls -l pg6400.txt\n-rw-r--r-- 1 bob mmi 1418582 Nov  5  2014 pg6400.txt\n[compute-113 /users/bob]$ touch zzz-test.txt\n[compute-113 /users/bob]$ rclone copy zzz-test.txt OneDrive:Documents\n[compute-113 /users/bob]$ rclone ls OneDrive:Documents | tail\n  2806147 ge_presentation.pdf\n   320053 pdf.tgz\n  5589891 pg100.txt\n   410012 pg10031.txt\n  4454050 pg31100.txt\n  1418582 pg6400.txt\n      426 plot1.r\n       20 plot1.sh\n       83 test1.sh\n        0 zzz-test.txt\n</code></pre> <p>This should give you a good start on using \u201crclone\u201d to access your OneDrive. Please email \u201cbitsupport\u201d if you have any questions.</p>"},{"location":"access/ssh/","title":"SSH - Key Information","text":""},{"location":"access/ssh/#ssh-basics","title":"SSH Basics","text":"<p>Access to the JHPCE cluster requires the use of SSH. Over time, the ability to launch applications via a web interface will increase, but for the foreseeable future you will need to be able to SSH in and then use command line UNIX skills to do your work.</p> <p>SSH stands for Secure SHell. SSH is actually a set of internet standard protocols. Programs implementing these protocols include both command line interface (CLI) tools and those with graphic user interfaces (GUI).  They all enable you to make secure, encrypted connections from one computer to the next.  In our case, ssh allows you to create an encrypted connection between you local system and the JHPCE login nodes (jhpce01.jhsph.edu and jhpce03.jhsph.edu), so that you can run a Unix Shell on the JHPCE cluster.</p> <p>Depending on the kind of operating system your computer uses, you may or may not need to install SSH software. Apple Macs come with CLI SSH tools pre-installed. You use them by entering commands in the Terminal app.  You can install GUI apps from various vendors, but we will only discuss the CLI tools except for some file transfer GUI programs.  On a Windows system you will need to install an ssh client.  We recommend the excellent GUI program MobaXterm. Here is a document describing how to use it. There are other programs, such as the PuTTY family of tools and WinSCP.</p>"},{"location":"access/ssh/#one-time-passwords","title":"One Time Passwords","text":"<p>By default, we use \"2 Factor Authentication\" in order to authenticate you access when you ssh into the JHPCE cluster.  This means that we will ask for 2 pieces of infomation instead of just 1.  Secifically, you will be asked for:</p> <p>1) Your Password. Your password will be a secure secret that only you know and is hard for others to guess. 2) A Verification Code.  Your Verification Code will be a one-time-use code from an App like Google Authenticator.</p> <p>As an aside, there are commonly 3 factors that can be used when authenticating in the Cybersecurity world. These are, Something you Know, Something you Have, and Something you Are.  The 2 factors we're using on JHPCE are \"Something you Know\" (a password), and \"Somthing you Have\" (your one-time-use code from Google Authenticator). An example of methods that use \"Something you Are\" would be a fingerprint scanner or the Apple's Face ID.</p> <p>You can also allow the use of SSH keys to help smooth the login process with a passwordless process.  Please see the section below for more details on SSH keys.</p> <p>Here is what a normal ssh connection looks like to login to the jhcpe01.jhsph.edu login node as user \"bsmith\", coming from his Mac named \"Bobs-Mac\".</p> <pre><code>\n[bobsmith@Bobs-Mac ~] $ ssh bsmith@jhpce01.jhsph.edu \n(bsmith@jhpce01.jhsph.edu) Password: Bob types his JHPCE password\n(bsmith@jhpce01.jhsph.edu) Verification code: Bob types the 6-digit code from Google Authenticator\nLast failed login: Mon Apr  7 09:42:08 EDT 2025 from 174.172.134.59 on ssh:notty\nLast login: Mon Apr  7 09:23:33 2025 from 174.172.134.59\n\ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38\nUse of this system constitutes agreement to adhere to all\napplicable JHU and JHSPH network and computer use policies.\n\ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38 \ud83c\udf38\n\nNeed Help? bitsupport@lists.jh.edu for SLURM, cluster, storage &amp; login issues\n           bithelp@lists.jh.edu for application issues: R/SAS/python/modules...\n\nThe SLURM shared partition is currently at 85% core usage and 70% RAM usage.\n--------------------------------------------------------------------\nDISK: Compression is enabled in our ZFS file systems like /users &amp; /dcs05\nDISK: You don't need to compress your files unless you want to use _very_\nDISK: high compression levels or transfer them outside of the cluster.\n--------------------------------------------------------------------\n--------------------------------------\n     Your Home Directory Usage        \nUsername     Space Used         Quota     \nbsmith       21G                100G      \n--------------------------------------\n\n[bsmith@jhpce01 ~]$ \n</code></pre> <p>Note</p> <p>Please note that when typing the \"Password\" and \"Verification Code\", the cursor does not move.  This is a security mechanism to prevent someone from watching what you are typing.  The computer is seeing what you are typing even if it isn't being displayed back.</p> <p>If you have entered your credentials correctly, you will see a login banner for the JHPCE cluster, and you will be sitting at a shell prompt on the login node.  However, if you are prompted for \u201cPassword:\u201d again, you likely mistyped either your password or Verification Code, and you should wait until your Google Authenticator code changes (it changes every 30 seconds) and you will need to try logging in again.</p> <p>Here are some things to check if you are unable to ssh into the JHPCE cluster: - Please make sure that you are using the password for the JHPCE cluster, and not your some other password, like you JHED or laptop password. - Please make sure you are using your JHPCE login ID. This is different from your JHED ID. - Please make sure you are using the Google Authenticator entry for the JHPCE cluster, and not some other system.</p> <p>If you have had several unsuccessful login attempts, your local system will be blocked by the login node.  Further attempts to ssh into the login node will likely give you a \u201cConnection Refused\u201d message, you will need to wait 10 minutes before trying to login again.  It you are certain you are using all of the right information for your login, and don't want to wait 10 minutes, you can try our second login node, jhpce03.jhsph.edu.</p> <p>If you continue to have login issues, please contact us at bitsupport@lists.jhu.edu.</p>"},{"location":"access/ssh/#ssh-keys","title":"SSH Keys","text":""},{"location":"access/ssh/#ssh-keys-quickstart","title":"SSH Keys Quickstart:","text":"<p>For Windows users using MobaXterm, please see this guide on our site in the Mobaxterm section</p> <p>For Mac or Windows users, you can use the following steps.</p> <p>1) Generate your private and public keys on your local system, run \"ssh-keygen -t ecdsa\".  When asked \"Enter file in which to save the key\", you should use the default answer. When asked to enter a passphrase, please choose a secure passphrase/password to securely store your keys.</p> <pre><code>\n[bobsmith@Bobs-Mac ~] $ ssh-keygen -t ecdsa \nGenerating public/private ecdsa key pair.\nEnter file in which to save the key (/Users/BobSmith/.ssh/id_ecdsa): \nEnter passphrase (empty for no passphrase): Passphrase Entered\nEnter same passphrase again: Passphrase Entered\nYour identification has been saved in /Users/BobSmith/.ssh/id_ecdsa\nYour public key has been saved in /Users/BobSmith/.ssh/id_ecdsa.pub\nThe key fingerprint is:\nSHA256:uyy1HdCMcwNMSHLk2cKsc0Tn bobsmith@Bobs-Mac.local\nThe key's randomart image is:\n+---[ECDSA 256]---+\n| *AB.  .o ..     |\n. . .\n|    o .   .  .   |\n|     .   A.o.    |\n+----[SHA256]-----+\n</code></pre> <p>2) Run \"ssh-agent\" and \"ssh-add\" to enable ssh-agent on your local system.  This will enble you to use your keys without having to use your Passphrase every time you use them.  You will need to run these 2 commands each time you reboot your local system.</p> <pre><code>\n[bobsmith@Bobs-Mac ~] $ ssh-agent\nSSH_AUTH_SOCK=/var/folders/xb/8jxv_1495w7f7ym9w89ksg8m0000gn/T//ssh-IAaQtZb85RG5/agent.63154; export SSH_AUTH_SOCK;\nSSH_AGENT_PID=63155; export SSH_AGENT_PID;\necho Agent pid 63155;\n\n[bobsmith@Bobs-Mac ~] $ ssh-add \nEnter passphrase for /Users/bobsmith/.ssh/id_ecdsa: ***Passphrase Entered***\nIdentity added: /Users/bobsmith/.ssh/id_ecdsa (bobsmith@Bobs-Mac.local)\n</code></pre> <p>3) Copy your ssh public key up to the JHPCE cluster.  Please be sure to replace the \"bsmith\" in the example below with your JHPCE login ID.</p> <pre><code>\n[bobsmith@Bobs-Mac ~] $ cat ~/.ssh/id_ecdsa.pub | ssh bsmith@jhpce01.jhsph.edu 'cat &gt;&gt; ~/.ssh/authorized_keys'\n(bsmith@jhpce01.jhsph.edu) Password: Enter your JHPCE Password\n(bsmith@jhpce01.jhsph.edu) Verification code: Enter your Google Authenticator Code\nBobs-Mac$\n</code></pre> <p>4) Now you should be able to login without having to use your JHPCE password and verification code.</p> <pre><code>\n[bobsmith@Bobs-Mac ~] $ ssh bsmith@jhpce01.jhsph.edu \nLast failed login: Mon Apr  7 09:42:08 EDT 2025 from 174.172.134.59 on ssh:notty\nLast login: Mon Apr  7 09:23:33 2025 from 174.172.134.59\nUse of this system constitutes agreement to adhere to all\napplicable JHU and JHSPH network and computer use policies.\n\n. . .\n\n[bsmith@jhpce01 ~]$ \n</code></pre>"},{"location":"access/ssh/#jhpce-python-tool-for-ssh-key-setup","title":"JHPCE Python Tool for SSH Key Setup","text":"<p>Brian Caffo has written some python tools for managing ssh keys in a git repository at https://github.com/jhpce-jhu/jhpce-python. You can install this git repo on your local system and go through the steps at https://github.com/jhpce-jhu/jhpce-python/blob/main/docs/index.md#creating-a-key for setting up your keys. You can also use this python tool to login to the JHPCE cluster and run remote commands there. There is also a Colab notbook with an example for setting up ssh keys within Python at https://colab.research.google.com/drive/1I8VjmDDO86Qj0jJYMmDlZWrAoZwVLrpj?usp=sharing</p>"},{"location":"access/ssh/#ssh-keys-more-info","title":"SSH Keys, more info:","text":"<p>SSH programs make use of something called public-key cryptography. Basically secure communications can be created by splitting a secret into to parts, and placing one part on each end of a link. This can be extended to an optional pair of files you can generate and distribute such that one is located on the JHPCE cluster and the other is on your computer. Or smartphone!</p> <p>This key pair of files is generated once, and protected with a passphrase. You place a copy of the \"public\" key file on JHPCE in a particular place with specific permissions. You keep a copy of the \"private\" key file on your personal device(s). Once you prove to a program (ssh-agent) running on your device that you know the passphrase to your private key file, it will thereafter provide your private key when you run an SSH command.</p> <p>Once configured properly, you can use SSH keys instead of your JHPCE password.</p> <p>SSH Keys described by another cluster</p> <p>This is still a means for using 2 Factor Authentication.  The key files themselves are \"Something you Have\" and these are protected by \"Something you Know\" in the form of the SSH passphrase, as well as the authentication mechanism used to protect your local system (password/fingerprint scanner)</p>"},{"location":"access/ssh/#ssh-keys-setup-more-detailed-info","title":"SSH Keys Setup, more detailed info:","text":"<p>SSH supports different kinds of identity key encryption.  Various algorithms have been developed over the years. As computing power increases and algorithm flaws are discovered, the recommended one to use changes. In the past \"rsa\" was the default.  That is no longer recommended in 2024. We here show \"ecdsa\" but you might want to instead use \"ed25519\". Just be consistent in your use of the string where it shows up in commands or file names.</p> <ul> <li>First, on your local laptop/desktop, open a terminal and <code>cd</code> into your home directory  and invoke the ssh key generator:</li> </ul> <pre><code>ssh-keygen -t ecdsa\n</code></pre> <ul> <li>You will be prompted for a passphrase.  For security reasons, we require that you use a passphrase to protect your key.  This prevents someone who gets access to your private key file from being able to use it!!!!! For the other questions, you can select the default values.</li> <li>The key generator will put two files in the .ssh subdirectory of your home directory, typically  <code>~/.ssh/id_ecdsa</code> and  <code>~/.ssh/id_ecdsa.pub</code>. </li> <li>You should only ever have to run the ssh key generator once on your local host.  If you have already configured passwordless login and you run the key generator a second time, it will overwrite your previous public and private key files. This will break all password-less logins that you set up with your previous keys.</li> <li>Next you want to copy your public key file to the remote host and append it to the authorized_keys file in the <code>.ssh</code> subdirectory of your home directory on the remote host. </li> <li>If there is no <code>~/.ssh</code> directory in the remote host, you will need to login to the remote host and create one. Note, attempting to connect to another host, like <code>ssh github.com</code>, will create one if it isn't there already. </li> <li>You can perform the copy and append operations in one line as follows;</li> </ul> <p><pre><code>cat ~/.ssh/id_ecdsa.pub | ssh &lt;your_userid&gt;@jhpce01.jhsph.edu 'cat &gt;&gt; ~/.ssh/authorized_keys'\n</code></pre> + Where you replace <code>&lt;your_userid&gt;</code> with your JHPCE userid and where you enter your JHPCE password when you are prompted for it by ssh. + To test that everything is working you should be able to log into the remote host from your local host with the following command<code>ssh &lt;your_userid&gt;@jhpce01.jhsph.edu</code>. + When you start ssh, you will be prompted for the passphrase that you used to protect your key.  To avoid having to enter your passphrase every time you use ssh, you can use the <code>ssh-agent</code> program.  To use <code>ssh-agent</code>, run</p> <p><pre><code>ssh-add\nssh-agent\n</code></pre> The ssh-agent will remain active for as long as your desktop or laptop is up and running.  If you reboot your desktop/laptop, you will need to rerun the ssh-add and ssh-agent commands.</p>"},{"location":"access/ssh/#ssh-keys-nitty-gritty-details","title":"SSH Keys Nitty Gritty Details","text":"<p>Authoring Note</p> <p>(((This isn't true if you use <code>srun</code>. Is it even true in JHPCE 3.0 anyway Also, do we want to be suggesting that people log into compute nodes?)))</p> <p>Logging into a cluster node from a login node requires keypairs.   If your private key file is in <code>.ssh/</code> then it should work, since the public key file is in your <code>authorized_keys</code> file. If you do not want the same private key file to be used to log into nodes as the one used to log into the login node, then repeat create a new public private key / public key pair on the cluster.  Append the public key to your<code>authorized_key</code> file. Note, when appending, add the public key, do not overwrite the existing file.</p> <p>Many users set up an alias for the ssh command so they don\u2019t have to type as much to log into the remote host.  You can do this by adding the following line to your <code>~/.bashrc</code>, <code>alias hpc='ssh -X &lt;your_userid&gt;@jhpce01.jhsph.edu'</code>.</p>"},{"location":"access/ssh/#windows-machines","title":"Windows machines","text":"<p>If your desktop/laptop runs Microsoft Windows then you first need to install MobaXterm on your windows machine. If you are using MobaXterm, please use the steps at the bottom of our MobaXterm configuration page.</p>"},{"location":"access/ssh/#permissions-on-ssh-files","title":"Permissions on SSH Files","text":"<p>SSH is very strict about the permissions found on your ssh-related files on both ends of a connection. These files are found on JHPCE in your home directory inside the directory <code>.ssh</code>  Because this directory's name begins with a period, it is not listed when you use the <code>ls</code> program -- you have to use <code>ls -a</code> or <code>ls -ld ~/.ssh</code> to see it.</p> <p>The primary symptom of there being a file permissions problem is that ssh is still asking for a password when you think it should not. Sometimes you get a helpful error message <code>Bad owner or permissions on &lt;a specific ssh-related file&gt;</code></p> <p>These rules are normally found to be broken on the remote side of a connection, but the permissions on your computer also matter. Different SSH programs may keep ssh-related files in different places, especially if they are on Windows computers.</p> <p>ALL OF THESE FILES NEED TO BE OWNED BY YOUR ACCOUNT.</p> <p>This table shows you two forms of the chmod command arguments needed to force permissions to be acceptable by SSH. The most convenient notation used by chmod is an octal (base-8) number. The most readable notation is a comma-separated combination of letters.</p> <p>These two commands are equivalent:</p> <ul> <li><code>chmod 700 $HOME/.ssh</code></li> <li><code>chmod u+rwx,g-rwx,o-rwx $HOME/.ssh</code></li> </ul> File/Directory Octal Human Readable Note $HOME 755 or tighter g-w,o-w Not writable by group or other $HOME/.ssh 700 u+rwx,g-rwx,o-rwx No access by group or other $HOME/.ssh/authorized_keys 600 u+rw,g-rwx,o-rwx Authorized keys file $HOME/.ssh/config 600 u+rw,g-rwx,o-rwx Config file $HOME/.ssh/id_* 600 u+rw,g-rwx,o-rwx Private key files $HOME/.ssh/id_*.pub 644 u+rw,g+r,g-wx,o+r,o-wx Public key files"},{"location":"access/ssh/#ssh-and-x11","title":"SSH and X11","text":"<p>See our document on X11 for instructions on making ssh work to support X11 displays.</p>"},{"location":"access/ssh/#mac-specific-configuration","title":"Mac-specific Configuration","text":"<p>Note</p> <p>You do not need to do ANY configuration for things to work, including running X11. All you need to do is to install the XQuartz package to support X11 window display. (The following material assumes you have installed it, and rebooted afterwards.)</p> <p>You can choose to put configuration information into a file in your home directory to make your life easier by, for example, specifying the username of some remote computer if it differs from your local username.  If you do that, then you no longer have to provide that information for every ssh command. However, you then need to remember that you have made choices in that file when something doesn't work the way you expect.</p>"},{"location":"access/ssh/#config-file-overview","title":"Config File Overview","text":"<p>SSH is a client-server program. SSH client programs like <code>ssh</code> connect over networks to SSH server programs (usually named <code>sshd</code>).  You don't have to worry about <code>sshd</code> configuration unless you are managing a computer that accepts incoming SSH connections.  macOS and Linux computers can run sshd if you enable that service (by checking the box \"Remote Login\" in macOS' Sharing pane of System Preferences).</p> <p>Both client and server have configuration files. There are system-wide ones for each, typically located in /etc/ssh/, and per-user ones, typically located in $HOME/.ssh, e.g. /Users/yourusername/.ssh on a Mac). A tilde symbol used in a shell like bash is a shortcut to \"my home directory\" so $HOME/.ssh and ~/.ssh should be equivalent ways of specifying the same directory. You can read about user SSH configuration files, the order in which they are read, and the meaning of the options, with the command <code>man ssh_config</code>. </p> <p>Do not add anything that you do not understand !! </p>"},{"location":"access/ssh/#changing-your-config-file","title":"Changing Your Config File","text":"You do not need ANY per-user config file for things to work, including running X11 (at least not when tested in macOS 12.6.3). <p>The only line you might need in your ~/.ssh/config file is this:</p> <p><code>XAuthLocation /opt/X11/bin/xauth</code></p> <p>if you are told that xauth is missing. This location has been the standard used by XQuartz -- it might change in the future. The file specified needs to be executable and exist at that location. You can test it by running it with the arguemnt <code>/opt/X11/bin/xauth list</code></p> <p>In your ~/.ssh/config file you may find some of the following options useful. </p> <p>The basic structure of this file is:</p> <ol> <li>some general settings</li> <li>some (optional) host-specific settings, in stanzas</li> <li>some (optional) settings meant to apply to all hosts not previously specified, in a stanza which begins with a line <code>Host *</code></li> </ol>"},{"location":"access/ssh/#general-settings","title":"General Settings","text":"<p>Useful general macOS ~/.ssh/config file settings<pre><code>EnableEscapeCommandline yes # Needed for recent macOS if you want to issue an SSH escape,\n#.      to create or close a tunnel for example\nForwardX11 yes        # Equiv to \"-X\" arg\nForwardX11Trusted yes # Equiv to \"-Y\" arg\nXAuthLocation /opt/X11/bin/xauth  # only include if needed\n# These are for users of public keys\nUseKeychain yes       # look for passphrases for public keys in macOS keychain\nAddKeysToAgent yes    # store passphrases for public keys in macOS keychain\n# These keys are tried in order when logging in, and are loaded by your SSH agent\nIdentityFile ~/.ssh/id_ecdsa\nIdentityFile ~/.ssh/id_rsa\nForwardAgent yes      # Equiv to the \"-A\" arg. For ssh'ing from 1 to 2 to 3. \n</code></pre> Keeping X11 forwarding enabled<pre><code>ForwardX11Timeout 0 # disables the timeout, which is by default 20mins\n                    # you can also set it to a time value like \"336h\" (2 weeks)\n</code></pre></p> Keeping connections alive - method one<pre><code>ServerAliveInterval 15\nServerAliveCountMax 30\n# These values mean that your client ssh program will send a message to the server\n# every 15 seconds, and not decide that a remote server is unresponsive until\n# (15*30)=450 seconds\n</code></pre> Keeping connections alive - method two<pre><code>TCPKeepAlive no\n# TCPKeepAlive defaults to YES, so you have to intentionally disable it.\n# The author of this web page chose to disable TCPKeepAlive at one time (and then\n# used the earlier method for many years happily) because it seemed to be the case\n# that with it enabled, ssh sessions would die if the connection dropped even\n# briefly.\n# It is unclear how long SSH waits if TCPKeepAlive is yes. The output of this\n# command may give clues, at least on Linux computers:\n#     grep -T . /proc/sys/net/ipv4/tcp_keepalive*\n# In any case, normal users cannot change the duration if they use TCPKeepAlive yes\n</code></pre>"},{"location":"access/ssh/#host-specific-settings","title":"Host-specific Settings","text":"<p>Optional.</p> <p>You can specify stanzas for specific hosts in order to create short aliases for those destinations, indicate the username you use there, and control which public key is used for access. (It is recommended to use different keys for different organizations.)</p> Per-host definitions for convenience<pre><code>Host jhpce01 j1 jhpce01.jhsph.edu\n    Hostname jhpce01.jhsph.edu\n    User your-cluster-username\n    ForwardX11 yes\n    IdentityFile ~/.ssh/id_ecdsa.jhpce\n</code></pre> <p>The above stanza allows you to use this command</p> <p><code>ssh j1</code></p> <p>instead of this command:</p> <p><code>ssh -X -i ~/.ssh/id_ecdsa.jhpce your-cluster-username@jhpce01.jhsph.edu</code></p>"},{"location":"access/ssh/#host-related-settings","title":"Host-related Settings","text":"<p>Optional.</p> Host-related definitions for all hosts not explicitly mentioned earlier<pre><code>Host *\n    ForwardX11 no\n    IdentityFile ~/.ssh/id_ecdsa.my-general-key\n</code></pre>"},{"location":"access/x11/","title":"X11/The X Window System","text":"","tags":["in-progress","jeffrey"]},{"location":"access/x11/#what-is-it","title":"What is it?","text":"<p>The X Window System (aka X11, aka X) is the primary GUI system for UNIX computers. X allows a program (an X \"client\") running on a UNIX computer in the cluster to be displayed on a remote computer (running an X \"server\"<sup>1</sup>) over the network. To use it, your local computer needs to have X server software installed on it.</p> <p>SSH provides support for tunnelling X11 over an encrypted connection. You may need to tell SSH that you want that service, by, for example, adding the -X flag to an ssh command in a macOS Terminal.</p> <p>X11 has been (slowly) being replaced by Wayland. Wayland has been the future for many years, but is now (2024) becoming common. However what is implementing X11 is usually invisible to users. It is mentioned here in case you see references to Wayland where you expect to see X11.</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#how-do-you-get-it","title":"How do you get it?","text":"<ul> <li>macOS users need to install XQuartz from xquartz.org.</li> <li>Windows users need to use a program like MobaXterm. We have a page describing installing and using MobaXterm.</li> <li>Linux laptop or desktop users are already using X as their windowing system.</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#example-x11-programs","title":"Example X11 Programs","text":"<p>xterm, SAS, RStudio, xclock, thunar.</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#configuring-ssh-to-support-x11","title":"Configuring SSH To Support X11","text":"<p>Authoringnote</p> <p>The SSH document points to this one for ALL of the information needed to use X11. So this section needs to be complete. Pointing to the MobaXterm document of course, for Windows users.</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#copying-and-pasting-in-x","title":"Copying and Pasting in X","text":"<p>Copying and pasting into and out of X11 environments is often more challenging than doing that using native Windows or macOS programs. You may need to use different key combinations and/or modify the settings of your X11 program (XQuartz or MobaXterm).</p> <p>The historical default meta key used for copying and pasting is the Ctrl key. Windows copied that usage, but macOS users are used to using the Cmd key. Depending on the settings of your X software, you may need to use Ctrl or a different key combination. (And you can often change those settings to something else that you prefer for any reason.)</p> <p>An additional complication is that X11 was developed in a world where you were expected to have a three-button mouse. You could select, copy and paste using different button combinations. That knowledge will help you understand any references you may come across in documentation or program settings about things like \"MIDDLE BUTTON\"</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#x11-forwarding-and-authentication","title":"X11 Forwarding and Authentication","text":"<p>See this excellent article from Teleport (pdf) for quite a lot of good information and diagrams!!! (Saved as a PDF from https://goteleport.com/blog/x11-forwarding/)</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#troubleshooting-x-connections","title":"Troubleshooting X Connections","text":"<p>Authoring Note</p> <p>This section should include pointers to FAQ items or vice versa. We shouldn't rewrite the same information twice!!!</p> <ul> <li>When -X ?</li> <li> <p>When -Y ? Probably only on old macOS systems such as 10.13</p> </li> <li> <p>printenv DISPLAY or echo $DISPLAY</p> </li> <li> <p>[user@login31 ~]$ srun --pty --x11 bash srun: error: No DISPLAY variable set, cannot setup x11 forwarding.</p> </li> <li> <p>How do Windows users configure ForwardX11Timeout? - No - this has only ever been an issue for Mac users  ? https://jhpce.jhu.edu/help/faq/#x11</p> </li> </ul> <p>help/faq/#x11 slurm/slurm-faq/#x11-related-errors</p>","tags":["in-progress","jeffrey"]},{"location":"access/x11/#algorithm","title":"Algorithm","text":"<p>If X11 isn\u2019t working for you, consider taking the following steps, and if you cannot figure out how to resolve the problem, let us know what happened when you followed these steps, so we know what worked and where in the process you reached an error, and exactly what it was.</p> <ol> <li>Log out of the cluster.</li> <li> <p>If you are on a Mac, test that XQuartz works on that Mac at the very most basic level. No network connections, no SSH, no remote authentication, just local X11 functionality.</p> <ol> <li>start XQuartz if not already running</li> <li>start a new <code>xterm</code> window from the Applications menu</li> <li>start a second test program from that <code>xterm</code> like <code>xcalc</code></li> <li>run <code>xauth list</code> to (slowly) list previously-added entries to your <code>~/.Xauthority</code> file. (You can Ctrl+C to cancel the <code>xauth</code>.)</li> <li>If you have any problems doing these things then STOP and find a solution. NOTHING WILL WORK IN TERMS OF X11 ON THE CLUSTER if it won't work at this level on your Mac. If you've just installed XQuartz, did you forget to reboot?</li> </ol> </li> <li> <p>Log back into the cluster: <code>ssh -X USER@jhpce01.jhsph.edu</code></p> </li> <li> <p>On the login node, run <code>printenv DISPLAY</code>  You should see a string like <code>localhost53:0</code> The first number (here \"53\") will vary, as it needs to be unique for all currently-logged in users.</p> </li> <li>If you do not have a DISPLAY variable defined, then you won't be able to run any X11 commands. You need to figure out why this variable is not being set.<ol> <li>One common issue is that an Xauthority lock file can get left on the cluster.  If these files exist, they can be deleted.    <pre><code>$ rm .Xauthority-l\n$ rm .Xauthority-c\n</code></pre></li> </ol> </li> <li>If do you have a DISPLAY variable defined, then:<ol> <li>try to run a simple X11 program to see if it works: <code>xclock</code> should bring up a new clock window on your screen. Close the <code>xlock</code> window by clicking in the right button in the upper left corner of its window or by typing Ctrl+C in the original terminal.</li> <li>run <code>xauth list</code> to (slowly) list previously-added entries to your ~/.Xauthority file.  (You can Ctrl+C to cancel the <code>xauth</code> but you should probably let it finish to ensure that your ~/.Xauthority file is not corrupt.)</li> <li>If you are told \"xauth: Command not found\" then your PATH environment variable is incorrect in an important way. <code>xauth</code> is found in JHPCE at <code>/usr/bin/xauth</code>  Your <code>~/.bashrc</code> file is nonstandard or your <code>module</code> usage has messed things up.  The commands <code>which xauth</code> and/or <code>whereis xauth</code> should show you where in the list of directories contained in the PATH environment variable xauth is found. What is the output of <code>echo $PATH</code>?</li> <li>If you have any problems doing these things then STOP and find a solution. NOTHING WILL WORK IN TERMS OF X11 ON A COMPUTE NODE IN THE CLUSTER if it won't work at this level on the login node.</li> </ol> </li> <li>So now we've established that you have a working X11 environment on the login node.</li> <li>Connect to a compute node asking for X11 forwarding support: <code>srun --pty --x11 bash</code></li> <li>If you receive an error then see the X11-related entries in the SLURM FAQ.</li> <li>On the compute node, run <code>printenv DISPLAY</code></li> <li>If you do not have a DISPLAY variable defined on the compute node, then you won't be able to run any X11 commands on the compute node. You need to figure out why this variable is not being set.</li> <li>If you have a DISPLAY variable defined on the compute node, then run a simple X11 program to see if it works: <code>xclock</code> should bring up a new clock window on your screen.</li> <li>Now close the <code>xclock</code> and try to run your application again.</li> </ol> <ol> <li> <p>Note that X reverses the normal conception of client/server operation, which is that the remote computer is the \"server\". In X11, the \"server\" is the program receiving the keyboard and mouse inputs and displaying the output of remote \"client\" programs.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","jeffrey"]},{"location":"csub/csub-overview/","title":"C-SUB Overview","text":"<p>The CMS SUBcluster (C-SUB) makes use of some of the resources of an existing High Performance Computing cluster called JHPCE.</p>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#motivation-to-create-the-facility","title":"Motivation to create the facility","text":"<p>The Centers for Medicare and Medicaid Services (CMS) is part of the U.S. Department of Health and Human Services. It provides important data for researchers of patients, their conditions, and the American health care system.</p> <p>Acquiring and managing sensitive information from the Federal government is time-consuming, and requires on-going administrative and information technology support by groups with specific expertise.</p> <p>To facilitate research, an effort has been made to create an infrastructure that can provide those resources, as well as a computational facility to store and analyze the data. The C-SUB is built such that it complies with  NIST SP 800-53 protocols, to ensure the protection of this sensitive data. The C-SUB's compliance with NIST SP 800-53 has been reviewed and approved by an external entitiy.</p> <p>Researchers can leverage this existing infrastructure to more quickly and efficiently start and conduct their work.</p> <p>The Health Analytics Research Platform (HARP) was created to implement this vision. It is a collaboration of existing personnel across multiple organizations. Its leaders provide funding and guidance to an IT group which created a computing facility named the C-SUB.</p>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#health-policy-management-hpm-component-of-c-sub","title":"Health Policy &amp; Management (HPM) Component of C-SUB","text":"<p>The Health Analytics Research Platform (HARP) provides data services to HBHI- affiliated and HEADS Center-affiliated investigators to facilitate research collaborations advancing HBHI\u2019s strategic pillars.</p> <p>HPM Personnel</p> <ul> <li>HEADS/HARP Director: Dan Polsky </li> <li>HEADS/HARP Deputy Director: Matt Eisenberg </li> <li>CMS Data Expert: Frank Xu</li> </ul> <p>Contact</p> <p>Please send C-SUB data-specific requests such as</p> <ul> <li>joining the C-SUB</li> <li>exporting files out of the C-SUB</li> <li>data inventory \u2013 current and desired additions or updates</li> </ul> <p>to support@harp-csub.freshdesk.com</p> <p>Further Information</p> <ul> <li> <p>Health Analytics Research Platform (HARP)</p> </li> <li> <p>Hopkins Business of Health Initiative (HBHI)</p> </li> <li> <p>Hopkins Economics of Alzheimer Disease &amp; Services (HEADS)</p> </li> </ul>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#differences-between-jhpce-c-sub-clusters","title":"Differences between JHPCE &amp; C-SUB Clusters","text":"<p>The CMS subcluster (C-SUB) makes use of some of the resources of the original JHPCE cluster. For example, the scientific software such as STATA and SAS.</p> <p>However, in many other ways it operates differently than the rest of the cluster in order to keep CMS data from escaping.</p> <p>Please keep that in mind when reading JHPCE documentation or asking for help via the bitsupport &amp; bithelp mailing lists. Mention that you are a C-SUB user.</p>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#commonalities","title":"Commonalities","text":"<ul> <li>SLURM job scheduler</li> <li>Almost all of the software</li> <li>use of modules</li> <li>except for programs which run out of containers like RStudio Server</li> </ul>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#differences","title":"Differences","text":"<ul> <li>Different user accounts (all C-SUB have the form of <code>c-&lt;JHEDID&gt;-&lt;DUA-NUMBER&gt;</code>)</li> <li>Different login node (C-SUB: jhpcecms01.jhsph.edu, only accessible from JH networks (including the VPN))</li> <li>Different SLURM server, partitions and nodes (cluster name is \"cms\" instead of \"jhpce3\"). So job acounting records differ. Default partition for jobs in \"jhpce3\" is \"shared\" while in \"cms\" it is \"cms\".</li> <li>File transfer in and out is VERY different (see C-SUB orientation pdf)</li> <li>Home directory locations (under <code>/users/&lt;DUA-NUMBER&gt;/</code> instead of just <code>/users/</code>)</li> <li>Common group-writable file sharing areas among DUA members (e.g. /users/55548/shared/)</li> </ul>","tags":["csub","topic-overview"]},{"location":"csub/csub-overview/#getting-help","title":"Getting Help","text":"<p>Many of the pages on this web site will be useful to C-SUB users. However they will need to interpret what they read and think about whether applying it in the C-SUB requires any modification or re-interpretation.</p> <p>Sources of assistance:</p> <ul> <li>This web site - use the search field</li> <li>This web site - see the section titled \"Getting Help\"</li> <li>the orientation slides are improved over time. They contain much information! Because of a lack of staff time, this document will be improved ahead of adding C-SUB-specific information to this web site. There is a version date on the first page. You can use that to compare to the one you used during your orientation.</li> </ul>","tags":["csub","topic-overview"]},{"location":"csub/csub-stub/","title":"stub page for the \"C-SUB\" topic","text":"<p>This is a stub page for the \"C-SUB\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"csub/csub-stub.md\"</p>"},{"location":"csub/new-csub-users/","title":"New C-SUB User or PI?","text":"<p>Please contact a member of HARP via support@harp-csub.freshdesk.com. They can provide information, discuss costs and send requests to the systems administrators as needed to implement the creation of new users and groups.</p> <p>Information about the C-SUB can be found in our overview.</p>","tags":["csub"]},{"location":"files/acl/","title":"ACL - Access Control Lists","text":"<p>Tip</p> <p>Before defining ACLs, you should first read our document about sharing files for necessary background concepts and skills.</p> <p>Traditional Unix file and group permissions can be used to share access to files and directories.  However, there can be times when more fine-grained control of shared access is needed. To accomplish this, Access Control Lists (ACLs) can be used. They add to the normal permissions, not replace them. ACLs and permissions can interact in unexpected ways.</p> ZFS versus Lustre: If you are working in /dcl02/ click here... <p>JHPCE uses two kinds of file systems on its large storage servers: ZFS and Lustre. A different pair of ACL commands is used for each type.</p> <p>As of March 2024, the only Lustre file systems are those which begin with the path <code>/dcl02/</code> For these file systems, use getfacl and setfacl.</p> <p>For everything else, use nfs4_getfacl and nfs4_setfacl. (All of the files originally found on the Lustre file server named DCL01 have been copied off to live on other, ZFS-using file servers. But the name /dcl01 has been preserved, for convenience.)</p> <p>Instructions for Lustre file systems are found later in this document.</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#acl-commands","title":"ACL Commands","text":"<p>An Access Control List is a series of Access Control Entry (ACE) rules associated with a file or directory.</p> <p>These rules are displayed and manipulated by two commands:</p> nfs4_getfacl Display the ACL of a file or directory. nfs4_setfacl Add, remove or modify ACE entries for an ACL on a file or directory. The main arguments you will use: <ul> <li>-a - Add an ACE</li> <li>-x - Remove an ACE (must exactly match the ACE to work)</li> <li>-e - Enter an editor to edit all of the ACEs in the ACL (don't use unless you have defined the environment variable EDITOR<sup>1</sup>)</li> <li>--test - Display the results of applying, but do not actually change</li> <li>-R - Appy the command recursively<sup>2</sup></li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#access-control-entry-ace","title":"Access Control Entry (ACE)","text":"<p>An ACE, or Access Control Entry, is a single control statement, indicating the access of a specific entity (usually a user or group). Thus, an Access Control List (ACL) is a list of ACEs. We here discuss some simple and common options for an ACE, but for a full description see the nfs4_acl(5) man page. A later section of this page gives the meaning for each of the single characters that are combined in ACEs to create cryptic strings like \"rwaDxtTcCy\"</p> <p>We will begin with the structure of an ACE:</p> <pre><code>access type:[flags]:principal:permissions\n</code></pre> <p>All parts are required for every operation, though the <code>[flags]</code> section may be empty. Therefore all of your ACEs will always contain three colons.</p> access type A (for Allow) (A D (for Deny) can be specified, but it is not needed &amp; leads to trouble.) flags g - If present, this means that this ACE applies to group principals, not user principals. fdi - If present, this set of flags mean that the ACE is to be inherited. principal The principal is the entity to which the ACE applies. A key thing to know is that JHPCE uses a Kerberos database to store user and group information, and that our NFSv4 (Network File System, version 4) file systems look to that Kerberos database for ownership and permission information. Kerberos databases can hold information for one or more domains. JHPCE only contains one domain and that is \"cm.cluster\" entity@domain \u2013 Therefore our user and group principals have the form of name@cm.cluster If you fail to provide the full name for a principal, your ACL commands will fail. OWNER@ \u2013 This special principal refers to the owner of the file/directory. It must always be present. This means every file must have an @OWNER ACE in its ACL. GROUP@ \u2013 This special principal refers to the default group of a file. It must always be present. This means every file must have an GROUP@ ACE in its ACL. EVERYONE@ \u2013 This is a special catch-all principal which applies to any entity that is not matched by any of the above. Think of this as equivalent to \u201cother\u201d (aka \u201cworld\u201d) in the traditional UNIX/Linux permissions model. <p>permissions</p> <p>Note</p> <p>Needs to be written</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#important-big-picture-notes-and-suggestions","title":"Important Big-Picture Notes and Suggestions","text":"","tags":["in-progress","jeffrey"]},{"location":"files/acl/#acl-facts","title":"ACL Facts","text":"<ul> <li>ACLs can be used on both files and directories. As with normal permissions, settings on files can have different impacts than those on directories (e.g. the X execute bit or default inheritance settings).</li> <li>You can tell that a file or directory has an ACL defined by the presence of a + symbol in the output of <code>ls -l</code> or <code>ls -ld</code></li> <li>Every file or directory appears to have an ACL if inspected with <code>nfs4+getfacl</code>.  This is just a representation of the basic UNIX ownership and permissions. When we speak in this document about adding an ACL we're talking about adding an ACE to those default values.</li> <li>Making changes with normal commands like <code>chmod</code> and <code>chown</code> will impact what you see if you inspect the ACL afterwards.</li> <li>If you want to change something about the special principals <code>OWNER@</code>, <code>GROUP@</code>, and <code>EVERYONE@</code>you probably want to do that with normal commands, not <code>nfs4_setfacl</code></li> <li>User's umask settings impact the permissions assigned to files and directories being created whether you are using traditional UNIX permissions or ACLs.</li> <li>Setting an ACL on a directory does not change existing files and directories inside it unless you use the <code>-R</code> recursive option to the ACL command.</li> <li>For users to be able to work with a file stored several layer deep in the directory structure, they must be able to get to it. That requires that they need sufficient permissions, via either normal UNIX permissions or ACLs (or both), to \"read\" and \"execute\" (aka \"search\") all of the directories above the final file or directory. For example, if you are wanting to enable access to the directory <code>/dcs07/bob/data/project1/shared</code>, you would need to provide <code>READ-EXECUTE</code> access on <code>/dcs07/bob/data/project1</code>, <code>/dcs07/bob/data</code>, and <code>/dcs07/bob</code>. Use UNIX permissions where appropriate, and ACLs where necessary.</li> <li>ACLs can be set on files and directories located in file systems of types other than NFS. That is not what JHPCE users do. Trying to use an nfs4_*acl command in somewhere like /tmp will throw off an error like this: <code>Operation to request attribute not supported: test1</code></li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#suggestions","title":"Suggestions","text":"<ul> <li>Use normal UNIX permissions where possible. ACLs can be complex to manage. Use ACLs to extend normal permissions.</li> <li>ACLs should use the security notion of \u201cleast privilege\u201d, meaning that ACLs should give only the needed access and nothing more.</li> <li>By default if an ACL does not allow something, that permission is denied. ACEs that start with an \"A\" are \"Allow ACEs\", those that start with a \"D\" are \"Deny ACEs\". Avoid using \"Deny ACEs\" -- they increase the chances of something not working as expected.</li> <li>Like any operation, commands modifying ACL commands on large numbers of files should be run on a compute node and not a login node. Long-running recursive ACL commands on large directory trees may be done via interactive sessions or submitted batch job scripts.</li> <li>Check the values of files and directories before, during, and after changing their permissions or defining ACLs. Use both <code>ls -l</code> and <code>nfs4_getfacl</code> to get the full picture. Test what happens afterwards. Don't change many files at once until you are confident that your commands are correct. You can capture the original configuration using commands like <code>ls -lR directoryname &gt; saved-listing.txt</code> for normal UNIX permissions and <code>nfs4_getfacl --recursive directoryname &gt; saved-acls.txt</code></li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#default-or-inherited-acls-on-directories","title":"Default or inherited ACLs on Directories","text":"<ul> <li>\"Default\" ACLs can be set on a directory, and this ACL will be inherited into the directory structure as new files and subdirectories are created. You might need to create different \"default\" ACLs to control the creation of new files and others to control the creation of new subdirectories.</li> <li>With \"default\" or \"inherited\" ACLs, the individual user\u2019s <code>umask</code> setting is important in assuring that future new files and directories being created have the correct permissions set. The <code>umask</code> setting will take precedence over the ACL, so it must be more permissive than the ACL. For example, if you want to set a default group ACL where the group has write access, you need to make sure that your umask is set to <code>0002</code> rather than <code>0022</code>, as the \u201c2\u201d in the group umask bit will prevent the group write capability.</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#acls-in-zfs-file-systems","title":"ACLs in ZFS File Systems","text":"","tags":["in-progress","jeffrey"]},{"location":"files/acl/#basic-commands","title":"Basic commands","text":"<p>There are 2 commands for dealing with ACLs in file systems like <code>/users</code>, <code>/dcl01</code>, <code>/dcs04</code>, <code>/dcs05</code>, <code>/dcs06</code>, and <code>/dcs07</code>. </p> <ul> <li><code>nfs4_getfacl</code> - display current ACL setting</li> <li><code>nfs4_setfacl</code> - modify ACLs.  </li> </ul> <p>With ACLs you can grant different kinds of access on directories or files to specific users and groups, in addition to the normal group associated with the object, and in addition to \"other\".</p> <p>The simplest permissions to use in ACLs are R for read access, W for write access, and X for execute and directory access. </p> <p>A later section of this page gives the meaning for each of the single characters that are combined in ACEs to create cryptic strings like \"rwaDxtTcCy\"</p> <p>These strings contain so many elements because ACLs can be used to create fine-grained permissions.</p> <p>By default, one\u2019s home directory is only accessible to the owner. This is accomplished with basic UNIX permissions, not ACLs.<sup>1</sup> The original ACL reflects this. For the user alice, the ACL on their home directory would look like:</p> <pre><code>[alice@compute-123 ~]$ pwd\n/users/alice\n[alice@compute-123 ~]$ ls -ld .\ndrwx------ 63 alice users 134 Apr 12 14:36 ./\n[alice@compute-123 ~]$ nfs4_getfacl .\n# file: .\nA::OWNER@:rwaDxtTcCy\nA::GROUP@:tcy\nA::EVERYONE@:tcy\n</code></pre> <p>If you see the string \"tcy\", that means \"no access\".</p> <p>If you see the string \"fdi\", that indicates that the ACE is a \"default\" or \"inherited\" one.</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#read-permisions","title":"Read permisions","text":"<ul> <li>Now, if alice wanted to grant read-only access to their home   directory to the user bob, they would use the <code>nfs4_setfacl</code>   command:</li> </ul> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RX .\n[alice@compute-123 ~]$ getfacl .\n# file: .\nA::OWNER@:rwaDxtTcCy\nA::bob@cm.cluster:xtcy\nA::GROUP@:tcy\nA::EVERYONE@:tcy\nAt this point, bob would be able to access alice\u2019s home directory. Now suppose there is a file that alice wants to let bob update. Alice could use ACLs to grant write access to a particular file:\n\n[alice@compute-123 ~]$ ls -l shared-data.txt\n-rw-r--r-- 1 alice users 79691776 Feb  2 07:06 shared-data.txt\n[alice@compute-123 ~]$ nfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RWX shared-data.txt \n[alice@compute-123 ~]$ nfs4_getfacl shared-data.txt\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#write-permissions","title":"Write permissions","text":"<ul> <li>Now suppose alice wanted to create a directory that bob could write to. </li> <li>Alice could create a directory, and grant bob write access to it:</li> </ul> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A::bob@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\nNow the user bob can copy or save files in the \u201cshared\u201d directory.\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#inherited-defaults","title":"Inherited Defaults","text":"<p>To set an inherited \u201cdefault\u201d ACL that will allow bob access on all new files and directories that get saved into <code>/users/alice/shared</code>, you would need to</p> <ol> <li>first give bob the normal ACL permissions, then</li> <li>add a second set of ACLs that will be inherited using the <code>fdi</code> option to the <code>nfs4_setacl</code> command.</li> </ol> <p>One issue we\u2019ve seen is that, in addition to the ACE you would expect was needed, you need to also add both a <code>@USER</code> and <code>@GROUP</code> ACE need to be explicitly set for permissions to be work as expected. </p> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:bob@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:OWNER@:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:GROUP@:RWX shared\n\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::bob@cm.cluster:rwaxtcy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\nA:fdi:OWNER@:rwaDxtTcCy\nA:fdi:bob@cm.cluster:rwaDxtcy\nA:fdi:GROUP@:rwaDxtcy\nA:fdi:EVERYONE@:tcy\n</code></pre> <p>To set a Group ACL, you need to add the <code>g</code> option to the <code>nfs4_getfacl</code> command.</p> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:g:hpscc@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA:g:hpscc@cm.cluster:rwaDxtcy\nA::EVERYONE@:rtcy\nTo set a Group Inherited ACL, you need to add the \u201cgfdi\u201d option to the \u201cnfs4_getfacl\u201d command. With this inherited group ACL set, all new files and directories will inherit the group settings.\n\n[alice@compute-123 ~]$ nfs4_setfacl -a A:gfdi:swdev@cm.cluster:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:OWNER@:RWX shared\n[alice@compute-123 ~]$ nfs4_setfacl -a A:fdi:GROUP@:RWX shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\n\n# file: shared\nA::OWNER@:rwaDxtTcCy\nA::GROUP@:rwaDxtcy\nA:g:hpscc@cm.cluster:rwaDxtcy\nA::EVERYONE@:rxtcy\nA:fdi:OWNER@:rwaDxtTcCy\nA:fdi:GROUP@:rwaDxtcy\nA:fdig:swdev@cm.cluster:rwaDxtcy\nA:fdi:EVERYONE@:tcy\n</code></pre> <p>Example showing the impact of adding a default (or inherited) ACE to a directory which provides read, write, and execute permissions for the UNIX group \"hpscc\"  Then, depending on the umask in force, files created inside this directory will be created with different permissions.</p> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -a A:gfdi:hpscc@cm.cluster:RWX test1\n[alice@compute-123 ~]$ touch test1/f1\n[alice@compute-123 ~]$ nfs4_getfacl test1/f1\n\n# file: test1/f1\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA:g:hpscc@cm.cluster:rtcy\nA::EVERYONE@:rtcy\n[alice@compute-123 ~]$ umask 0002\n[alice@compute-123 ~]$ touch test1/f2\n[alice@compute-123 ~]$ nfs4_getfacl test1/f2\n\n# file: test1/f2\nA::OWNER@:rwatTcCy\nA::GROUP@:rwatcy\nA:g:hpscc@cm.cluster:rwatcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#removing-an-acl","title":"Removing an ACL","text":"<p>Use the <code>-x</code> option to <code>nfs4_setfacl</code> to remove an ACE from an ACL. Please note that you need to use the full (and exact) ACE, and not the <code>RWX</code> shortcuts.</p> <pre><code>[alice@compute-123 ~]$ nfs4_setfacl -x A::bob@cm.cluster:rwaxtcy shared\n[alice@compute-123 ~]$ nfs4_getfacl shared\nnfs4_getfacl shared-data.txt\n# file: shared-data.txt\nA::OWNER@:rwatTcCy\nA::GROUP@:rtcy\nA::EVERYONE@:rtcy\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#applying-acls-recursively","title":"Applying ACLs Recursively","text":"<p>You can use the <code>-R</code> flag to <code>nfs4_setfacl</code> to apply your changes recursively.</p> <p>Sometimes you want to set a rule only on files or on directories. You can use the <code>find</code> command to generate lists of each type of object which are then passed to the <code>xargs</code> command, which runs the command it is given on each object. By default <code>find</code> starts at the path you give it and, if it is a directory, descends into it recursively.  Note that this example adds arguements to <code>find</code> and <code>xargs</code> which handle the case where files or directory names contain evil, no-good, awful characters like white space, quote marks, or backslashes:</p> <pre><code>cd into-the-top-of-the-directory-tree\nfind . -type d -print0 | xargs --null nfs4_setfacl -a A::bob@cm.cluster:RWX\nfind . -type f -print0 | xargs --null nfs4_setfacl -a A::bob@cm.cluster:R\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#duplicating-existing-acls","title":"Duplicating Existing ACLs","text":"<p>This section under development</p> <p>The following information needs to be revised and reviewed. It is provided in case it is helpful until then.</p> <p>You can copy working ACLs but you must be very careful and check the results. Did your umask contribute an unexpected change to the process?</p> <p>One technique is to save and restore the settings. The second example uses input/output redirection to connect the two commands.</p> <pre><code>nfs4_getfacl -R source_dir &gt; tmpfile1\nsetfacl --restore tmpfile1 destination_dir\n\nnfs4_getfacl -R source_dir |\nsetfacl --restore - destination_dir\n</code></pre> <p>You can copy the ACLs on a directory with rsync. You have to put trailing slashes on BOTH the source and destination.</p> <p><code>rsync -A public-exam1/ /dcs05/somegroup/data/otherdir/</code></p> <p>While -Ar preserves ACLs, it doesn't preserves ownerships and permissions. Use -Arogp (o=owner, g=group, p=permissions) or shorter and more common -Ara (a=archive) for a better copy of your directory.</p> <p>This page has example scripts for saving and restoring ACLs on many files at once.</p> <p>This page has different advice for the same goal.</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#ace-flags-explained","title":"ACE Flags Explained","text":"<p>The ACL on a normal directory looks like this:</p> <pre><code># file: .\nA::OWNER@:rwaDxtTcCy\nA::GROUP@:tcy\nA::EVERYONE@:tcy\n</code></pre> <p>What does \"tcy\" mean? How about \"rwaDxtTcCy\"?  This section contains the answer, with flags grouped by common occurrence. Copied from this page.</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#alias-flags","title":"ALIAS FLAGS","text":"<p>Aliases such as 'R', 'W', and 'X' can be used as permissions. These work simlarly to POSIX Read/Write/Execute. </p> Flag Name Function R Read rntcy W Write watTNcCy (with D added to directory ACE's) X Execute xtcy","tags":["in-progress","jeffrey"]},{"location":"files/acl/#inheritence-flags","title":"INHERITENCE FLAGS","text":"<p>You will see \"fdi\" groups of flags in \"default\" ACLs</p> Flag Name Function f file-inherit New files will have the same ACE minus the inheritence flags d directory-inherit New subdirectories will have the same ACE i inherit-only New files and subdirectories will have this ACE but the ACE for the directory with the flag is null n no-propogate inherit New subdirectories will inherit the ACE minus the inheritence flags","tags":["in-progress","jeffrey"]},{"location":"files/acl/#normal-flags","title":"NORMAL FLAGS","text":"Flag Name Function r read-data (files) / list-directory (directories) Can you read the object's contents? w write-data (files) / create-file (directories) a append-data (files) / create-subdirectory (directories) x execute (files) / pass-through-directory (directories) You don't have to read a directory to cd through it into a subdirectory d delete the file/directory D delete-child : remove a file or subdirectory from the given directory (directories only) t read the attributes of the file/directory T write the attribute of the file/directory n read the named attributes of the file/directory N write the named attributes of the file/directory c read the file/directory ACL C write the file/directory ACL Danger, Will Robinson! o change ownership of the file/directory Danger, Will Robinson!","tags":["in-progress","jeffrey"]},{"location":"files/acl/#acls-in-lustre-file-systems","title":"ACLs in Lustre file systems","text":"<p>Currently, the only Lustre file system we have is <code>/dcl02</code>. So if your files are located anywhere else, you can ignore this section.</p> <p>There are two commands for dealing with ACLs on the JHPCE cluster for directories in Lustre file systems, which start with <code>/dcl02</code>.</p> <p>The <code>getfacl</code> command will display current ACL setting for a file or directory, and the <code>setfacl</code> command is used to modify ACLs.  With ACLs you can grant either read-only access, or read-write access on a directory or file to specific users.</p> <p>Let\u2019s say there is a directory <code>/dcl02/project/data/alice</code> that alice owns. The <code>getfacl</code> command could be used to see the current ACL set on the directory:</p> <pre><code>[alice@compute-123 ]$ pwd\n/dcl02/project/data/alice\n[alice@compute-123 ]$ getfacl .\n# file: .\n# owner: alice\n# group: users\nuser::rwx\ngroup::---\nother::---\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#read","title":"Read","text":"<p>Now, if alice wanted to grant read-only access to <code>/dcl02/project/data/alice</code> to the user bob, they would use the <code>setfacl</code> command:</p> <pre><code>[alice@compute-123 ]$ setfacl -m user:bob:rx .\n[alice@compute-123 ]$ getfacl .\n# file: .\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#write","title":"Write","text":"<p>Now suppose there is a file that alice wants to let bob update. Alice could use ACLs to grant write access to a particular file:</p> <pre><code>[alice@compute-123 ]$ ls -l shared-data.txt\n-rw-r--r-- 1 alice users 79691776 Feb  2 07:06 shared-data.txt\n[alice@compute-123 ]$ getfacl shared-data.txt\n# file: shared-data.txt\n# owner: alice\n# group: users\nuser::rw-\ngroup::r--\nother::r--\n\n[alice@compute-123 ]$ setfacl -m user:bob:rw shared-data.txt \n[alice@compute-123 ]$ getfacl shared-data.txt\n# file: shared-data.txt\n# owner: alice\n# group: users\nuser::rw-\nuser:bob:rw-\ngroup::r--\nmask::rwx\nother::r--\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#creating-a-common-dir","title":"Creating a common dir","text":"<p>Now suppose alice wanted to create a directory that bob could write to. Alice could create a directory, and grant bob write access to it:</p> <pre><code>[alice@compute-123 ~]$ mkdir shared\n[alice@compute-123 ]$ setfacl -m user:bob:rwx shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:rwx\ngroup::r-x\nmask::rwx\nother::r-x \n</code></pre> <p>Now the user bob can copy or save files in the \u201cshared\u201d directory.</p>","tags":["in-progress","jeffrey"]},{"location":"files/acl/#default-acls","title":"Default ACLs","text":"<p>To set an inherited default ACL that will allow bob access on all new files and directories that get saved into <code>shared</code>, you would need to use the <code>-d</code> option to the <code>setacl</code> command:</p> <pre><code>[[alice@compute-123 ~]$ setfacl -d -m user:bob:rwx shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\nuser:bob:rwx\ngroup::r-x\nmask::rwx\nother::r-x\ndefault:user::rwx\ndefault:user:bob:rwx\ndefault:group::--x\ndefault:mask::rwx\ndefault:other::r-x\n</code></pre> <p>If you want to remove and ACL, you can use the \u201c-x\u201d option to setfacl.</p> <pre><code>[alice@compute-123 ]$ setfacl -x user:bob shared\n[alice@compute-123 ]$ getfacl shared\n# file: shared\n# owner: alice\n# group: users\nuser::rwx\ngroup::r-x\nmask::r-x\nother::r-x\n</code></pre> <ol> <li> <p>Not true in the C-SUB, where home directories are protected with ACLs.\u00a0\u21a9\u21a9</p> </li> <li> <p>You may not want to apply some ACEs recursively, because files and directories can need different rules. See this section for a technique to apply changes to only files or only directories.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","jeffrey"]},{"location":"files/archive-files/","title":"Archive Files","text":"<p>It can be advantageous to use archive commands to gather up individual files into archive files.</p> <p>JHPCE users store petabytes of data on our storage servers in billions of files. When data is not being actively used, it can be a good idea to store files in archives.</p> <p>It is easier to deal with fewer files when transferring or storing for the long term.  There are fewer individual items to process, document, run checksums against, or ask programs to process. File systems also work better with fewer files per directory. Creating and altering entries for each file takes time. Network transfers proceed faster when data can be read and manipulated in larger blocks.</p>"},{"location":"files/archive-files/#archive-commands","title":"Archive commands","text":"<p>A variety of programs exist which will add files into and extract files from archives of different formats. The features offered can include: checks for integrity, encryption, compression, Unicode filenames.</p> <p>When choosing a format, consider its popularity. You do not want to learn years later that you can no longer access a program that can work with your old archive file.</p> <p>Also consider which operating systems you may need to use when working with the archive file.</p> <p>Also consider researching any known limitations of a format. Most formats have such limitations!!!  Some of these can bite you. For example, what is the maximum number of characters in a file name? Maximum number of characters in the complete path to the file?  Given how quickly data accumulates and the increase in file sizes over the years, other attributes can be an issue, such as: What is largest file size that can be contained within an archive? The size of the resulting archive? The number of files?  </p> <p>A comparison of archive formats can be found here.</p> <p>Popular programs:</p> <ul> <li>bzip2</li> <li>cpio</li> <li>gzip</li> <li>tar</li> <li>zip</li> <li>7z</li> </ul>"},{"location":"files/archive-files/#compression","title":"Compression","text":"<p>Most archive commands have optional arguments enabling the compression or decompression of files being added to or extracted from archives. Usually those compression algorithms have a range of compression levels, where the CPU required to compress or decompress varies in relation to the amount of file compression achieved.</p> <p>JHPCE uses ZFS file systems with compression enabled for home directories and data stores like /dcs05.</p> <p>Therefore you do not need to spend time or CPU cycles compressing files or archives already stored in the cluster. They are uncompressed by the file server when you open them. </p> <p>Some data compresses more than others. Plain text files compress very well. Binary files holding data read out of lab instruments or images compress less well or are already compressed as a result of the file format used.</p> <p>When is compressing worthwhile?</p> <ol> <li>If you want to reduce files to absolute minimum size, you might enable maximum compression levels, e.g. <code>gzip -9</code>. Test your results -- sometimes compressing files can increase their size.</li> <li>Compressing files may be a worthwhile activity for reducing the size of files or archives before importing them into the cluster. </li> <li>Or if exporting cluster files to another location where space is limited or the network bandwidth is very limited. Remember that ssh programs (ssh, scp, sftp) do compression by default. </li> </ol>"},{"location":"files/archive-files/#do-your-work-on-the-right-computer","title":"Do your work on the right computer","text":"<p>Login nodes are needed by everyone. They should not be bogged down by your work with files. Use a compute node for anything except the smallest file operations. If you aren't sure how much space is in a deep directory tree, don't start a check on a login node!</p>"},{"location":"files/archive-files/#preparing-to-create-an-archive","title":"Preparing to create an archive","text":""},{"location":"files/archive-files/#space","title":"Space","text":"<p>Do you have enough space to create your archive? Do you need to create several smaller archives rather than an enormous single one? (That can have its own advantages.)</p> How much space is available in a file system? <code>df -h .</code> will show you info for your current working directory (CWD) including the hostname of the file server (if you're in an NFS file system) How much space is consumed by a directory? <code>du -sh that-dir</code> will show you disk consumption of the named directory <code>du -sh that-dir/*</code> will show you disk consumption of files and directories inside the named directory"},{"location":"files/archive-files/#data-cleaning-and-preparation","title":"Data cleaning and preparation","text":"<p>You may want to (carefully) prepare a set of files before archiving them. The amount of caution is of course related to the uniqueness and value of the data.</p> <p>This data maintenance can include:</p> <ul> <li>renaming files or directories,</li> <li>deleting unwanted files,</li> <li>setting consistent file permissions,</li> <li>setting consistent user/group ownership,</li> <li>re-organizing files into directories,</li> <li>creating README files describing the contents of the archive, its creation method, and anything else you or someone else might want to know. (It is a great idea to keep a copy of such files both inside and outside of the archive!!)</li> </ul> <p>Some commands like tar can accept arguments indicating that they should exclude certain file names or patterns. You can also create a list of files to archive out of a larger set of files and tell the archive program to only archive those files.</p>"},{"location":"files/archive-files/#precautions","title":"Precautions","text":"<p>You can capture critical information before beginning (and while making changes) with simple commands like a recursive listing of a directory tree's contents such as <code>ls -lR mydir &gt; mydir.contents</code></p> <p>If you are concerned that you might make mistakes during this process, consider:</p> <ul> <li>the backup process that might be protecting the storage area you are working within,</li> <li>if there is a backup done, its frequency,</li> <li>creating a scratch archive of the original material so you can fall back to it if needed,</li> <li>making one containing only the most essential files if you do not have the storage space for one containing everything </li> </ul> <p>While doing prepatory work, consider what commands or command options you can use to check what will happen before implementing changes such as file deletions. </p> <p>A helpful technique can be moving files aside into temporary trash directories before deleting them. If things work as desired, you can then delete the trash directories. You can also rename files instead of deleting them -- e.g. add a DEL suffix.</p>"},{"location":"files/archive-files/#before-deleting-the-original-files","title":"Before deleting the original files","text":"<p>Test your work. Can you extract various sample files? Ones with the longest total path length in characters? Can you generate a list of the archive contents? Does creating a checksum with <code>md5sum archive-name.tar</code> succeed (it will cause every byte of the archive to be read)?</p> <p>Have you created documentation about what the archive contains and the commands used to create it?</p>"},{"location":"files/archive-files/#example-commands","title":"Example commands","text":"<p>There are plenty of tutorials online for working with archives. Our goal with this document is to mention things to consider and some specific example commands.</p>"},{"location":"files/archive-files/#archiving-commands","title":"Archiving commands","text":"<p>Create archive from two directories and a file. The archive file is being created in a different file system, because you knew that the directories contain several terabytes of information (and want to store the archive there).  /dcs05 and /dcs07 come from two different file servers, so reading from one while writing to the other can be faster.</p> <pre><code>cd /dcs05/mygroup/data/trial5/experiment8/\ntar -cf /dcs07/mygroup/my-tar-file.tar some-directory other-dir2 file9\n</code></pre> <p>Create a compressed archive of a directory tree using multiple CPU threads. (Four in this example.) (You need to request four CPU from SLURM to be able to have them to use on the compute node.) By default <code>pigz</code> deletes files after compressing them, so be sure to use the <code>-k</code> \"keep\" argument.</p> <pre><code>cd /dcs05/mygroup/data/trial5/experiment8/\ntar -cf /dcs07/mygroup/my-tar-file.tar --use-compress-program=\"pigz -k -p4\" some-large-directory\n</code></pre> <p>Extract everything from an archive. It is always a good idea to look at the contents of the archive to see what file structure will be created during extraction. You do not want to overwrite other files. There are options like <code>--keep-old-files</code> and <code>--keep-newer-files</code> (and others) which affect extraction behavior. Better safe than sorry!</p> <pre><code>tar -tzf compressed-archive.tgz | less\ntar -xvzf compressed-archive.tgz\n</code></pre> <p>Extract multiple files from an archive. You need to first examine the contents of the archive and look for occurences of the names you want to extract -- there may be multiple items which match. You may need to provide partial paths to indicate which copy of the file you want.</p> <pre><code>tar -tf somearchive.tar &gt; mylisting.txt\nless mylisting.txt\ntar -xvf somearchive.tar \"file1\" \"file2\"\n</code></pre> <p>Extract files matching a wildcard pattern <pre><code>tar -xvf somearchive.tar --wildcards '*.php'\n</code></pre></p>"},{"location":"files/archive-files/#find-command","title":"Find command","text":"<p>The find command is a very useful tool. If you want to modify files or directories found with find, it is recommended to use xargs.</p> <p>Note that file or directory names containing spaces or other such characters cause a world of problems in UNIX. It is extremely highly recommended to strictly avoid creating or retaining such names.</p> <p>Here is an example that finds Perl scripts <pre><code>find /usr/bin/ -exec file {} \\; | grep Perl\n</code></pre></p> <p>Find the size of cache directories inside of \"mydir\" to help you decide whether to delete or exclude them. Store the results in a text file. Inspect it with the less command (press q to quit out of it). The find option -print0 and the xargs option -0 allow searches to deal with PATH NAMES CONTAINING SPACES. <pre><code>find mydir -type d -name .cache -print0 | xargs -0 du -sh &gt; /tmp/cache-list.txt\nless /tmp/cache-list.txt\n</code></pre></p> <p>Find files ending in \".o\" (but not directories) and rename them to \".o.DELETEME\" The asterisk needs to be escaped by the backwards slash to prevent the shell from expanding the wildcard in the current directory before starting the find program. <pre><code>find mydir -type f -name \\*.o -print0 | xargs --null -I{} mv {} {}.DELETEME\n</code></pre></p> <p>Find files or directories modified after a certain date and time (09/11/2019 noon)</p> <pre><code>touch -t 201909111200 911-noon\nfind somedirofmine -newer 911-noon -print0\n</code></pre> <p>This more complex example ignores directories named Library and .dropbox, limits how deep to search <pre><code>find . \\( \\( -type d -name Library -prune \\) -o \\( -type d -name .dropbox -prune \\) \\) -o -newer 911 -depth 4 -print0 \n</code></pre></p>"},{"location":"files/copying-files/","title":"Copying Files Within The Cluster","text":"<p>When transferring files into and out of the cluster, please use the transfer node. The various tools for that work are described in this document.</p> <p>When working with files already within the cluster, please use the information in this document. </p>"},{"location":"files/copying-files/#tldr","title":"TL;DR","text":"<ol> <li>When trying to copy or move files around between file systems inside the cluster, you do not need to use the transfer node or the <code>scp</code> command.</li> <li>Please do your file management work on compute nodes, not login nodes.</li> <li>Understand what you are doing in big-picture terms. Are you trying to move data around within the same file system? Copy data between file systems? How large is the total amount of data that you are manipulating?  For tips on these kinds of things see storage tips</li> <li>Before initiating large copies you should check that you have enough space in the destination. </li> <li>It is safer to copy files between different file systems than to <code>mv</code> them. But copying files between locations within the same file system duplicates files and can lead to confusion.</li> <li>If you need to manage a lot of files, it is well worth your time to learn how to use the <code>rsync</code> command. It can be used to compare directory trees, create log files of its actions, and do other important things not found in more frequently used programs like <code>cp</code> or <code>scp</code></li> </ol>"},{"location":"files/copying-files/#introduction","title":"Introduction","text":"<p>JHPCE users store petabytes of data on our storage servers. Some of it accumulates as work is done. Much is imported from outside the cluster as source material for research.  Moving around large numbers of files can be done with a variety of methods.  Many users default to <code>cp</code> or <code>mv</code></p> <p>All tools have limitations, most of which we, thankfully, don't encounter in day-to-day use. However, when manipulating large numbers of files and files in directory trees many levels deep, these issues can begin to surface. Some of them give you more confidence in the results than others. Some are misleadingly quiet despite failing or producing a result that isn't the same as the source location.</p> <p>The <code>cp</code> command is a good example of a familiar tool that can produce unexpected results. It supports recursion through the <code>-R</code> flag. Depending on the UNIX version, <code>cp</code> may or may not treat symbolic links or hard-linked files the way you expect.</p> <p>What is the state of your copy if <code>cp</code> or <code>mv</code> attempts fail mid-stream? In the middle of a file?</p> <p>For example, consider what happens if a program has a limitation where it cannot handle file paths longer than 1024 characters. You would be unlikely to experience a problem using that program to copy files around unless they are <code>/stored/in/directory/trees/with/many/entries/YYYY-MM-DD-MM-SS/long-filename-with-details-embedded-in-its-name.dat</code>  Such paths are common in the sciences and become more common as time passes and data accumulates.</p>"},{"location":"files/copying-files/#rsync","title":"Rsync","text":"<p>Rsync is a powerful command for copying large numbers of files between a SOURCE (aka SRC) and a DESTINATION (aka DEST), within a single computer or between computers. </p> <p>Note</p> <p>A key benefit of <code>rsync</code> is that it will copy only the material needed to make DEST match SRC. Most other programs, such as <code>cp</code> or <code>scp</code>, will always copy over all of SRC to DEST.</p> <p>We have written an extensive document about using rsync, which we invite you to read here.</p>"},{"location":"files/copying-files/#archive-tools-moving-data-through-a-pipe","title":"Archive tools moving data through a pipe","text":"<p>Rsync is usually the best method. But rsync wasn't always available in the past, or it didn't support copying one or another attribute that more basic tools did.  Different kinds of file permissions, data forks, etc.</p> <p>One method that can be quick and effective is to use an archive program like tar to create an archive file which is passed through an input/output pipe to the same program running in another directory that extracts files into that location. This technique can use available system memory as buffer space, leading to smoother flows of data as disk reads and writes occur with optimal amounts of bytes.</p> <p>This example copies the named directory some-directory from the current working directory to another location. Because tar by default works with blocks of data 512 bytes in size, a higher efficiency is achieved by telling it to create larger blocks to reduce the overhead of doing input/output requests in such small sizes.</p> <p><pre><code>tar -cbf 20480 - some-directory | (cd /destination/place; tar -xbf 20480 -)\n</code></pre> This example extends the technique to copying the data off of your current host to another host: <pre><code>tar -cbf 20480 - some-directory | ssh myaccount@another-host (cd /destination/place; tar -xbf 20480 -)\"\n</code></pre></p>"},{"location":"files/data-security/","title":"Data Security","text":"<p>This is a stub page for the \"Managing Files\" topic.</p> <p>Authoring Note</p> <p>What should users keep in mind about providing the appropriate amount of protection to their data files? (Also, should we have a document that points to outside guides to data management lifecycles? Case Western Reserve has good documentation.)</p> <p>Information about satisfying HIPAA standards can be found here.</p>","tags":["needs-to-be-written"]},{"location":"files/deep-paths/","title":"Navigating Deep Directory Trees","text":"<p>Many researchers use directory and file names to create logical structures of significant complexity to organize their files.  The complexity increases over time as new projects and data sets come along. Long file or directory names are used to make things as self-obvious as possible.</p> <p>It can be a hassle to move around quickly and efficiently between directories you are frequently using. </p> <p>If you have to switch between two directories with long paths, these two techniques can make life better.</p>"},{"location":"files/deep-paths/#a-nest-of-symbolic-links","title":"(A Nest of) Symbolic Links","text":""},{"location":"files/deep-paths/#tldr","title":"TL;DR","text":"<ol> <li>Create a directory with a short name in your home directory</li> <li>Place symbolic links in that directory to files or directories. The target objects can be located all around the file system.</li> <li>Use the tilde symbol (~) and the short directory name to quickly refer to this directory (no matter where you are located in the file system)</li> <li>Now you can edit a far-away file or cd to a far-away directory using a very short path.</li> <li>You can keep the name you use to refer to the object unchanged even if you change the symbolic link to point to a different object, e.g. <code>~/r/current-project</code></li> </ol>"},{"location":"files/deep-paths/#explanation-and-examples","title":"Explanation and examples","text":"<p>A symbolic link is a special kind of file which points at another file. Also known as symlinks. The symbolic link takes up almost no space. It is not a copy of the original file.</p> <p>You can refer to the symbolic link and in most cases the results will be the same as if you specified the original file. (Warning: some commands treat symbolic links in ways you might not expect: cp, rm, rsync, tar Their man pages will discuss how they treat symlinks.)</p> <p>You create a symlink like this:</p> <p><code>ln \u2013s realfile newname</code></p> <p>The resulting files when listed with <code>ls \u2013l</code>:</p> <pre><code>lrwxr-xr-x\u00a0 1 tunison\u00a0 wheel\u00a0 8 Jan 11 10:12 newname@ -&gt; realfile\n-rw-r--r--\u00a0 1 tunison\u00a0 wheel\u00a0 0 Jan 11 10:11 realfile\n</code></pre> <p>So a way to make use of symlinks is to create a directory of them, and refer to those when doing things like changing directories. I use the following scheme myself. You can name your directory whatever you want, such as \u201credirect\u201d instead of the shorter \u201dr\u201d.</p> <pre><code>mkdir ~/r\ncd ~/r\nln \u2013s /cms01/data/puf-free/VERICRED/2019 v2019\nln \u2013s /cms01/incoming/c-myjhed-98765 in\nln \u2013s ~/mycode/thatlanguage/src/yes-i-wrote-it pride-n-joy\n</code></pre> <p>Now you can use the symlinks named \u201cv2019\u201d, \u201cin\u201d \u201cpride-n-joy\u201d instead of the longer real directory names.</p> <p>You can change directory with <code>cd ~/r/in</code> and you will wind up in <code>/cms01/incoming/c-myjhed-98765</code> </p> <p>Or, You can copy downloaded source code into your home directory with</p> <p><code>cp ~/r/in/pkg-3.2.tar ~/mycode/thatlanguage/src/</code></p> <p>Of course, symbolic links can point to files instead of directories, so you can also place in your \"nest\" symlinks to files while still leaving the original in the \"right\" location. This way you don't need to consume the space of having two copies AND you can easily edit the single copy of the file and not have to keep two copies synchronized.</p>"},{"location":"files/deep-paths/#pushd-popd-and-dirs-commands","title":"Pushd, Popd and Dirs Commands","text":"<p>If you\u2019re repeatedly working in several directories and don\u2019t need to open multiple windows to look at them simultaneously, these commands allow you to switch rapidly between them. They use a \u201cstack\u201d data structure. Think of a stack as a pile of plates in a cafeteria. When you want one, you usually take the one from the top. Then a fresh one is exposed. You\u2019ve \u201cpopped\u201d a plate from the stack. If you \u201cpush\u201d three plates onto the stack, then the stack is deeper. You can access the top plate easily, but in this case you can also get at the third plate down.</p> <p>Here is a representation of pushing elements onto a stack, and then popping them off again:</p> <p></p> <p><code>pushd directoryname</code></p> <p>changes your directory from the current one to directoryname, and creates a stack of two directories.</p> <p><code>dirs -v</code></p> <p>will list those directories in the stack.</p> <p><code>pushd</code></p> <p>by itself will switch you between the current directory and the top one in the stack. This is often the main way I use it.</p> <p><code>pushd +2</code></p> <p>will switch you between the current directory and the third down into the stack (third because the index into the stack starts with zero not one).</p> <p><code>popd</code></p> <p>will cd back to the top directory in the stack while removing your current directory from the stack.</p> Create aliases for an even sweeter experience <p>The <code>-v</code> option to dirs is so handy for figuring out what number to use with <code>pushd</code> that you may want to make that the default by adding an alias to your <code>$HOME/.bashrc file</code>. The author uses these aliases to make using these commands even better:</p> <pre><code>alias dirs='dirs -v'\nalias pu='pushd'\nalias po='popd'\n</code></pre>"},{"location":"files/deep-paths/#create-a-default-directory-stack","title":"Create a default directory stack","text":"<p>You can create a stack ahead of time using the \u2013n option to pushd. That option adds the directory to the stack but does not change to it.</p> <p>So if you used the nano text editor to add this to your .bashrc file, it would create a set of directories you can pushd between every time you log in!!!</p> <p>Set up directory stack. Note that they appear in stack in reverse order than listed here!!!</p> <p><code>dirs -v</code> will show their order (and index number)</p> <pre><code>for i in /cms01/data /cms01/outgoing /users/55548 /cms01/incoming; do pushd -n $i 1&gt;/dev/null;done\n</code></pre> <p>Here we use that technique after adding it to our .bashrc file:</p> <pre><code>[~]$ source .bashrc\n\n[~]$ dirs\n~ /cms01/incoming /users/55548 /cms01/outgoing /cms01/data\n\n[~]$ pushd +2\n/users/55548 /cms01/outgoing /cms01/data ~ /cms01/incoming\n\n[55548]$ pwd\n/users/55548\n\n[55548]$ dirs -v\n 0  /users/55548\n 1  /cms01/outgoing\n 2  /cms01/data\n 3  ~\n 4  /cms01/incoming\n</code></pre>"},{"location":"files/encfs/","title":"Encrypted Filesystem with encfs | Joint HPC Exchange","text":"<p>One mechanism available on the JHPCE cluster for implementing an encrypted filesystem or directory is via the use of the FUSE based \u201cencfs\u201d application. \u00a0One key benefit of using \u201cencfs\u201d is that it is completely user based and thus can be set up by any user on the cluster at any time.</p> <p>Initial Setup</p> <p>Before using \u201cencfs\u201d, you will need to be added to a special group on the JHPCE cluster. Please email \u201cbitsupport\u201d and indicate that your would like to use encrypted filesystem.\u00a0Once your account has been configured, you may proceed.</p> <p>To setup your encrypted filespace, you will first need to login to a compute node, and create 2 directories, one that will contain your encrypted data and one directory that will be used as the point of access to your encrypted data.</p> <pre><code>[jhpce01 /users/bob]$ srun --pty --x11 bash\nLast login: Thu Nov  3 14:33:05 2016 from jhpce01.cm.cluster\n[compute-070 /users/bob]$ mkdir ENCRYPTED\n[compute-070 /users/bob]$ mkdir UNENCRYPTED\n</code></pre> <p>Next, load the \u201cencfs\u201d module, and use the \u201cencfs\u201d program to initialize the encrypted directory, and enable access to the encrypted directory through the unencrypted directory. Before running the \u201cencfs\u201d program, you should have a password in mind that you will use to secure the directory. Be sure that you use the full pathnames to the encrypted and unencrypted directories when using the \u201cencfs\u201d command.</p> <pre><code>[compute-070 /users/bob/]$ module load encfs\n[compute-070 /users/bob/]$ \nencfs /users/bob/ENCRYPTED /users/bob/UNENCRYPTED\nCreating new encrypted volume.\nPlease choose from one of the following options:\n enter \"x\" for expert configuration mode,\n enter \"p\" for pre-configured paranoia mode,\n anything else, or an empty line will select standard mode.\n?&gt; (just hit enter here)\n\nStandard configuration selected.\n\nConfiguration finished.  The filesystem to be created has\nthe following properties:\nFilesystem cipher: \"ssl/aes\", version 3:0:2\nFilename encoding: \"nameio/block\", version 3:0:1\nKey Size: 192 bits\nBlock Size: 1024 bytes\nEach file contains 8 byte header with unique IV data.\nFilenames encoded using IV chaining mode.\nFile holes passed through to ciphertext.\n\nNow you will need to enter a password for your filesystem.\nYou will need to remember this password, as there is absolutely\nno recovery mechanism.  However, the password can be changed\nlater using encfsctl.\n\nNew Encfs Password: (enter password)\nVerify Encfs Password: (re-enter password)\n[compute-070 /users/bob]$ df -h\n...\nencfs                  50T   17T   33T  34% /users/bob/UNENCRYPTED\n</code></pre> <p>Please be sure that you remember the password used to set up your encrypted space. If the password is lost, there is no way to access the unencrypted data.</p> <p>Now, you can copy data into your unencrypted directory, and it will become automatically encrypted in your encrypted directory. \u00a0All work should be done in the unencrypted directory.</p> <p>When you have finished using your encrypted space, you can remove access to the unencrypted space by using the \u201cfusermount\u201d command:</p> <pre><code>[compute-070 /users/bob]$ ls ENCRYPTED\n[compute-070 /users/bob]$ ls UNENCRYPTED\n[compute-070 /users/bob]$ cd UNENCRYPTED\n[compute-070 /users/bob/UNENCRYPTED]$ echo \"This is a test\" &gt; file1\n[compute-070 /users/bob/UNENCRYPTED]$ cd ..\n[compute-070 /users/bob]$ ls -l UNENCRYPTED\n-rw-r----- 1 bob mmi 15 Jan 27 16:34 file1\n[compute-070 /users/bob]$ ls -l ENCRYPTED\n-rw-r----- 1 bob mmi   23 Jan 27 16:34 Fng4AKmAzYP6OrpOrCPH3J,x\n[compute-070 /users/bob]$ cat UNENCRYPTED/file1\nThis is a test\n[compute-070 /users/bob]$ cat ENCRYPTED/Fng4AKmAzYP6OrpOrCPH3J,x \n?tS?$??#?q?%o\ub741?\n[compute-070 /users/bob]$ fusermount -u /users/bob/UNENCRYPTED\n[compute-070 /users/bob]$ ls -l UNENCRYPTED\n[compute-070 /users/bob]$ ls -l ENCRYPTED\n-rw-r----- 1 bob mmi   23 Jan 27 16:34 Fng4AKmAzYP6OrpOrCPH3J,x\n[compute-070 /users/bob]$ exit\nConnection to compute-070.cm.cluster closed.\n</code></pre> <p>Once you run the \u201cfusermount\u201d command, your unencrypted access point will be removed, and your data will remain safely encrypted on the system. Other users will not have access to the unencrypted data.</p> <p>Ongoing Usage</p> <p>The next time you need to access your data in an unencrypted state, login to a compute nodes, and rerun the \u201cencfs\u201d command.</p> <pre><code>[jhpce01 /users/bob]$ srun --pty --x11 bash\nLast login: Fri Jan 27 16:10:27 2017 from jhpce01.cm.cluster\n[compute-097 /users/bob]$ ls -l UNENCRYPTED\n[compute-097 /users/bob]$ ls -l ENCRYPTED\n-rw-r----- 1 bob mmi   23 Jan 27 16:34 Fng4AKmAzYP6OrpOrCPH3J,x\n[compute-097 /users/bob]$ module load encfs\n[compute-097 /users/bob]$ \nencfs /users/bob/ENCRYPTED /users/bob/UNENCRYPTED\nEncFS Password: (reenter the password you used to encrypt the data)\n[compute-097 /users/bob]$ ls -l UNENCRYPTED\n-rw-r----- 1 bob mmi 15 Jan 27 16:34 file1\n[compute-097 /users/bob]$ cd UNENCRYPTED\n[compute-097 /users/bob/UNENCRYPTED]$ cat file1\nThis is a test\n</code></pre> <p>As before, when you are done accessing your data, use the \u201cfusermount\u201d command to remove access to the unencrypted space.</p> <pre><code>[compute-097 /users/bob/UNENCRYPTED]$ cd ..\n[compute-097 /users/bob]$ fusermount -u /users/bob/UNENCRYPTED\n[compute-097 /users/bob]$ ls -l /users/bob/UNENCRYPTED\n[compute-097 /users/bob]$ ls -l ENCRYPTED\n-rw-r----- 1 bob mmi   23 Jan 27 16:34 Fng4AKmAzYP6OrpOrCPH3J,x\n[compute-097 /users/bob]$ exit\nConnection to compute-097.cm.cluster closed.\n</code></pre> <p>One final important note. If you do not run the \u201cfusermount -u\u201d command before logging out, the unencrypted directory will remain available on the compute node. While access to it will not be available to other users, this is in all likelihood undesirable. Please be sure to run \u201cfusermount -u\u201d before logging out of the compute node.</p> <p>Sharing Data</p> <p>If you want to grant access to your unencrypted data to other users, it is possible, but you will need to perform a few additional steps. First you will need to set up permissions on the files and directories at the Unix level using either standard group permissions or with ACLs. While it is possible to do this in the encrypted space, it is highly recommended that this be done while you have the space unencrypted so that you can see the true file names that you are working with. Once you have granted access to the files at the Unix level, other users can use the \u201cencfs\u201d command to create their own unencrypted entry point to the encrypted data. They will need to use the same password that was used when the encrypted file space was set up.</p> <p>So, the first step in sharing your data is to grant access to your directory (and directories above it) to another user.</p> <pre><code>[jhpce01 /users/bob]$ srun --pty --x11 bash\nLast login: Fri Jan 27 17:12:11 2017 from jhpce01.cm.cluster\n[compute-108 /users/bob]$ module load encfs\n[compute-108 /users/bob]$ \nencfs /users/bob/ENCRYPTED /users/bob/UNENCRYPTED\nEncFS Password: (reenter the password you used to encrypt the data)\n[compute-108 /users/bob]$ setfacl -m user:chuck:rx .\n[compute-108 /users/bob]$ setfacl -m user:chuck:rwx UNENCRYPTED\n</code></pre> <p>Next, the other user will need to login to a compute node, and run \u201cencf\u201d, however they will need to use a different directory for their unencrypted space.</p> <pre><code>[jhpce01 /users/chuck]$ srun --pty --x11 bash\nLast login: Fri Jan 27 15:44:11 2017 from jhpce01.cm.cluster\n[compute-049 /users/chuck]$ module load encfs\n[compute-049 /users/chuck]$ mkdir UNENCRYPTED\n[compute-049 /users/chuck]$ \nencfs /users/bob/ENCRYPTED /users/chuck/UNENCRYPTED\nEncFS Password: (reenter the password you used to encrypt the data)\n[compute-049 /users/chuck]$ cd UNENCRYPTED\n[compute-049 /users/chuck/UNENCRYPTED]$ ls -l\n-rw-r----- 1 bob mmi 15 Jan 27 16:34 file1\n[compute-049 /users/chuck/UNENCRYPTED]$ echo \"This is chuck\" &gt; file2\n[compute-049 /users/chuck/UNENCRYPTED]$ ls -l\n-rw-r----- 1 bob   mmi 15 Jan 27 16:34 file1\n-rw-r----- 1 chuck mmi 14 Jan 27 17:24 file2\n</code></pre> <p>At this point the original user will also be able to see the changes that the new user made.</p> <pre><code>[compute-108 /users/bob]$ ls -l /users/bob/UNENCRYPTED\n-rw-r----- 1 bob   mmi 15 Jan 27 16:34 file1\n-rw-r----- 1 chuck mmi 15 Jan 27 17:24 file2\n</code></pre> <p>The original user can use remove their unencrypted space, and the second user can continue to use their space. There is no dependency on having the creator active.</p> <pre><code>[compute-108 /users/bob]$ fusermount -u /users/bob/UNENCRYPTED\n[compute-108 /users/bob]$ exit\n</code></pre> <p>Of course, the second user should also use \u201cfusermount -u\u201d on their own directories before logging out.</p> <pre><code>[compute-049 /users/chuck/UNENCRYPTED]$ cd ..\n[compute-049 /users/chuck] fusermount -u /users/chuck/UNENCRYPTED\n[compute-049 /users/chuck] exit\n</code></pre> <p>This process can be extended to several users \u2013 it is not limited to just 2 users.</p>"},{"location":"files/files-overview/","title":"MANAGING FILES","text":"<p>We all spend a lot of our time manipulating files.  This document attempts to provide an overview of, and pointers to, useful knowledge.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#where-files-live","title":"Where files live","text":"<p>We have created a Storage topic in this web site. Please visit the overview and any other pages of interest.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#naming-files","title":"Naming files","text":"<p>Files are created and manipulated with programs. Those programs are written by many different people and organizations. Some of them attempt to adhere to standards like POSIX. Files are stored in file systems. Both programs and file systems have rules and limitations -- and bugs. It is best to be conservative when naming files.</p> <p>UNIX shells like bash, our default, interpret many characters that people have grown used to using in file names on their Windows or Mac computers as having special meanings. Ampersands, parentheses, vertical bars for example. Using other characters, such as spaces, require file names to be enclosed in single or double quotes or escaped with backslashes when provided to UNIX commands. Combining multiple commands which pass file names from one to the other either fail or require the use of special techniques.</p> <p>Instead of space characters, use hyphens or underscores.</p> <p>It is best to use the plain Latin alphabet. Accented characters or those which require Unicode to portray might be stored in the file system, but they may not be handled correctly by programs that try to manipulate them. If those failures are silent, then you may not understand what is happening or what failed to happen. For example, a file may not be added to an archive file, or if it is, might be hard to extract.</p> <p>There are limits to the length of file names (255 characters in most file systems) and program limits to the length of file paths (e.g. 4096 in Linux, 1024 in macOS). Again, exceeding these limits might work in some situations but not others. For example, creating the files might succeed but trying to specify one using a long absolute path might fail. Working with them in one operating system and then transferring them to another might fail.</p> <p>You can look up some limits using the getconf command. Note again that these limits are not guarantees that all of the software you are using will work correctly with things that approach or exceed them.</p> <pre><code>getconf NAME_MAX /\ngetconf PATH_MAX /\n</code></pre> <p>It is HIGHLY RECOMMENDED that you simply DO NOT create files with such names, and that you rename ones you might already have. It will save you a lot of grief over the long run.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#sharing-files-with-other-users","title":"Sharing files with other users","text":"<p>Collaborating with others is a daily activity on the cluster. There are better -- and worse -- ways to do that.</p> <p>We have tried to document relevant information in this document.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#using-acls-to-allow-increased-access","title":"Using ACL's to allow increased access","text":"<p>Traditional Unix file ownership and permissions can be used to share access to files and directories. However, there can be times when more fine-grained control of access is needed. To accomplish this, Access Control Lists (ACLs) can be used. They add to or extend the normal permissions.</p> <p>We have a good document about how to use ACLs here.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#transferring-files-into-and-out-of-the-cluster","title":"Transferring files into and out of the cluster","text":"<p>A variety of tools exist for this task. We have documented many of them in this document.  <code>Rsync</code> is one such program. It has a learning curve and there are some important things to know about using it. We have put that information into the document we wrote about copying data around within the cluster. </p> <p>Before moving large amounts of data in or out, you may want to consolidate it into fewer files using archive tools. We have documented some of the relevant considerations in this document.</p> <p>For transferring files to and from the cluster, you should use <code>jhpce-transfer01.jhsph.edu</code> rather than a login node. This is both significantly faster, as the transfer node has a 40G Ethernet connection to the outside world while the login nodes have 10G connections. In any case, EVERYONE depends on the login nodes, and you should not run ANYTHING on them that occupies them.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#accessing-data-from-compute-nodes","title":"Accessing data from compute nodes","text":"<p>Consider your input/output needs for computations. Which storage location and type provides the best combination of speed, capacity and proximity to CPUs?</p> <p>There may well be different answers for different kinds of files used during your work.</p> <p>Should you stage your data to the fastscratch file system? That document describes how to use chained jobs to get the data there with one batch job and then start the computation with another.</p> <p>Should you stage the data locally to the compute nodes or access it over the network? Compute nodes only have local storage in their /tmp file systems. Everyone else using a node needs to share that space. The operating system of the node itself needs there to be some free space in /tmp.</p> <p>There is a SLURM command which can copy data to assigned nodes. See the sbcast manual page.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#copying-data-around-within-cluster","title":"Copying Data Around Within Cluster","text":"<p>Alternatives to copying (sharing via ACL, using symbolic links).</p> <p>Do it on compute nodes.</p> <p>Example batch jobs for doing so.</p> <p>Rsync argument recommendations.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#best-practices","title":"Best Practices","text":"<p>Think before hitting Enter. Think again. There are few substitutes.</p> <p>Check beforehand that there will be sufficient space in destination before trying to import or create data or copy it around.</p> <p>We protect home directories and project space with redundant arrays of disks. Some of our file systems are backed up daily to other locations. But ultimately you need to ensure that you have backups elsewhere of precious files. Consider creating a GitHub repo to hold programming code -- such files represent a lot of effort. US Navy SEALs have a pessimistic/realistic motto about redundancy: Two is one, one is none.</p> <p>Don't bother compressing files in most cases because we have enabled compression on our ZFS file systems. </p> <p>Archiving old or infrequently-used files can be a good idea. It speeds up directory access (less metadata in directories). It simplifies transferring bulk data and checking on the success or failure. We have a document exploring file archiving here.</p>","tags":["topic-overview"]},{"location":"files/files-overview/#backups-and-restores","title":"Backups and Restores","text":"<p>Home directories are backed up once a day. Users can access the last fourteen days of data themselves. Primary Investigators can request that we back up project spaces. (Only a few have done so.)</p> <p>Further information about this topic can be found in this document</p>","tags":["topic-overview"]},{"location":"files/files-overview/#data-protection-policies","title":"Data Protection Policies","text":"<p>We will be publishing information  about cluster and university policies for the protection of certain classes of information in this document. </p>","tags":["topic-overview"]},{"location":"files/files-overview/#navigating-deep-directory-trees","title":"Navigating Deep Directory Trees","text":"<p>Many researchers use directory and file names to create logical structures of significant complexity to organize their files.  The complexity increases over time as new projects and data sets come along. Long file or directory names are used to make things as self-obvious as possible.</p> <p>It can be a hassle to move around quickly and efficiently between directories you are frequently using. </p> <p>If you have to switch between two directories with long paths, the techniques in this document can make life better.</p>","tags":["topic-overview"]},{"location":"files/files-stub/","title":"stub page for the \"Managing Files\" topic","text":"<p>This is a stub page for the \"Managing Files\" topic.</p> <p>Create a new file with the right contents for the topic header in the nav bar. Then point that header to the new document instead of \"files/files-stub.md\"</p>"},{"location":"files/rsync-itemize-manpage/","title":"Rsync itemized output manual page section","text":"<p>The <code>-i</code> or <code>--itemize-changes</code> flag is especially helpful when trying to compare two directory trees when used with the <code>-n</code> or <code>--dry-run</code> argument. Or to document what was changed during a copy (direct the output into a text file since there can be a lot of info flying past you). </p> <p>However, it produces a cryptic string 11 letters long for each file or directory. We have copied a useful chart from the Internet that you can consult.</p> <p>The general format is like the string YXcstpoguax</p> <p>See also:</p> <ol> <li>Main rsync page</li> <li>A visual key to itemized output</li> </ol> <pre><code>The  \"%i\"  escape  has a cryptic output that is 11 letters long.\nThe general format is like the string YXcstpoguax,  where  Y  is\nreplaced  by the type of update being done, X is replaced by the\nfile-type, and the other letters represent attributes  that  may\nbe output if they are being modified.\n\nThe update types that replace the Y are as follows:\n\no      A  &lt; means that a file is being transferred to the remote\n       host (sent).\n\no      A &gt; means that a file is being transferred to  the  local\n       host (received).\n\no      A  c  means that a local change/creation is occurring for\n       the item (such as the creation  of  a  directory  or  the\n       changing of a symlink, etc.).\n\no      A  h  means  that the item is a hard link to another item\n       (requires --hard-links).\n\no      A . means that the item is not being updated  (though  it\n       might have attributes that are being modified).\n\no      A  * means that the rest of the itemized-output area con\u2010\n       tains a message (e.g. \"deleting\").\n\nThe file-types that replace the X are: f for a file, a d  for  a\ndirectory,  an  L for a symlink, a D for a device, and a S for a\nspecial file (e.g. named sockets and fifos).\n\nThe other letters in the string indicate if some  attributes  of\nthe file have changed, as follows:\n\no      \".\" - the attribute is unchanged.\n\no      \"+\" - the file is newly created.\n\no      \" \"  - all the attributes are unchanged (all dots turn to\n       spaces).\n\no      \"?\" - the change is unknown (when  the  remote  rsync  is\n       old).\n\no      A letter indicates an attribute is being updated.\n\nThe attribute that is associated with each letter is as follows:\n\no      A  c  means  either  that  a regular file has a different\n       checksum (requires --checksum) or that a symlink, device,\n       or  special  file  has a changed value.  Note that if you\n       are sending files to an rsync prior to 3.0.1, this change\n       flag  will be present only for checksum-differing regular\n       files.\n\no      A s means the size of a regular  file  is  different  and\n       will be updated by the file transfer.\n\no      A t means the modification time is different and is being\n       updated to the sender's value (requires --times).  An al\u2010\n       ternate  value of T means that the modification time will\n       be set  to  the  transfer  time,  which  happens  when  a\n       file/symlink/device is updated without --times and when a\n       symlink is changed and the receiver can't set  its  time.\n       (Note:  when  using  an rsync 3.0.0 client, you might see\n       the s flag combined with t instead of the proper  T  flag\n       for this time-setting failure.)\n\no      A p means the permissions are different and are being up\u2010\n       dated to the sender's value (requires --perms).\n\no      An o means the owner is different and is being updated to\n       the sender's value (requires --owner and super-user priv\u2010\n       ileges).\n\no      A g means the group is different and is being updated  to\n       the sender's value (requires --group and the authority to\n       set the group).\n\no      A u|n|b indicates the following information: u  means the\n       access  (use)  time  is different and is being updated to\n       the sender's value (requires --atimes); n means the  cre\u2010\n       ate  time  (newness) is different and is being updated to\n       the sender's value (requires  --crtimes);  b  means  that\n       both the access and create times are being updated.\n\no      The a means that the ACL information is being changed.\n\no      The  x  means  that the extended attribute information is\n       being changed.\n\nOne other output is possible: when deleting files, the \"%i\" will output  the  string  \"*deleting\" for each item that is being removed (assuming that you are talking to a  recent  enough  rsync that  it  logs deletions instead of outputting them as a verbose message).\n</code></pre>"},{"location":"files/rsync-itemize-table/","title":"Meaning of rsync itemized output","text":"<p>See also:</p> <ol> <li>Main rsync page</li> <li>Manual page section for itemized output</li> </ol> <p>The <code>-i</code> or <code>--itemize-changes</code> flag is especially helpful when trying to compare two directory trees when used with the <code>-n</code> or <code>--dry-run</code> argument. Or to document what was changed during a copy (direct the output into a text file since there can be a lot of info flying past you).</p> <p>However, it produces a cryptic string 11 letters long for each file or directory. We have copied a useful chart from the Internet that you can consult.</p> <p>The general format is like the string YXcstpoguax, where:</p> <ol> <li>Y  is replaced  by the type of update being done,</li> <li>X is replaced by the file-type, and</li> <li>the other letters represent what is different/changed.<ul> <li>A + char means file is being created, so this attribute was changed</li> <li>A . char means this attribute was not changed</li> </ul> </li> </ol> <pre><code>  YXcstpoguax  path/to/file\n  |||||||||||\n  `-------------------------- the TYPE OF UPDATE:\n   ||||||||||   &lt;: file is being transferred to the remote host (sent).\n   ||||||||||   &gt;: file is being transferred to the local host (received).\n   ||||||||||   c: local change/creation for the item, such as:\n   ||||||||||      - the creation of a directory\n   ||||||||||      - the changing of a symlink,\n   ||||||||||      - etc.\n   ||||||||||   h: the item is a hard link to another item (requires --hard-links). \n   ||||||||||   \"+\" - the file is newly created\n   ||||||||||   .: the item is not being updated (though it might have attributes that are being modified).\n   ||||||||||   *: means that the rest of the itemized-output area contains a message (e.g. \"deleting\").\n   ||||||||||\n   `----------------------------- the FILE TYPE:\n    |||||||||   f for a file,\n    |||||||||   d for a directory,\n    |||||||||   L for a symlink,\n    |||||||||   D for a device,\n    |||||||||   S for a special file (e.g. named sockets and fifos).\n    |||||||||\n    `--------- c: different checksum (for regular files)\n     ||||||||     CHANGED VALUE (for symlink, device, and special file)\n     `-------- s: Size is different\n      `------- t: Modification time is different\n       `------ p: Permission are different\n        `----- o: Owner is different\n         `---- g: Group is different\n          `--- u: The u slot is reserved for future use.\n           `-- a: The ACL information changed\n</code></pre>"},{"location":"files/rsync/","title":"Rsync: A Key File Management Tool","text":""},{"location":"files/rsync/#tldr","title":"TL;DR","text":"<p>If you need to manage a lot of files, it is well worth your time to learn how to use the <code>rsync</code> command. This program is available on UNIX and Mac computers by default. It is increasingly available on Windows as the Windows Subsystem for Linux (WSL) becomes more common.</p>"},{"location":"files/rsync/#introduction","title":"Introduction","text":"<p>JHPCE users store petabytes of data on our storage servers. Some of it accumulates as work is done. Much is imported from outside the cluster as source material for research.  Moving around large numbers of files can be done with a variety of methods.  Many users default to <code>cp</code> or <code>mv</code></p> <p>All tools have limitations, most of which we, thankfully, don't encounter in day-to-day use. However, when manipulating large numbers of files and files in directory trees many levels deep, these issues can begin to surface. Some of them give you more confidence in the results than others. Some are misleadingly quiet despite failing or producing a result that isn't the same as the source location.</p> <p>The <code>cp</code> command is a good example of a familiar tool that can produce unexpected results. It supports recursion through the <code>-R</code> flag. Depending on the UNIX version, <code>cp</code> may or may not treat symbolic links or hard-linked files the way you expect.</p> <p>What is the state of your copy if <code>cp</code> or <code>mv</code> attempts fail mid-stream? In the middle of a file?</p> <p>For example, consider what happens if a program has a limitation where it cannot handle file paths longer than 1024 characters. You would be unlikely to experience a problem using that program to copy files around unless they are <code>/stored/in/directory/trees/with/many/entries/YYYY-MM-DD-MM-SS/long-filename-with-details-embedded-in-its-name.dat</code>  Such paths are common in the sciences and become more common as time passes and data accumulates.</p>"},{"location":"files/rsync/#rsync","title":"Rsync","text":"<p>Rsync is a powerful command for copying large numbers of files between a SOURCE (aka SRC) and a DESTINATION (aka DEST), within a single computer or between computers. </p> <p>Note</p> <p>A key benefit of <code>rsync</code> is that it will copy only the material needed to make DEST match SRC. Most other programs, such as <code>cp</code> or <code>scp</code>, will always copy over all of SRC to DEST.</p> <p>There are MANY arguments which control how the copying will occur. So you can do things like specify files to exclude, create a log of the actions taken, and provide statistics when done. Rsync uses a variety of tests to compare files and directories. We wrote this page because using rsync properly for complex situations takes some skill and explanation.</p> <p>The most common flag used with <code>rsync</code> is <code>-a</code> for \"archive\". This provides a recursive copy, preserving symbolic links, timestamps, file permissions, user &amp; group ownership. It does NOT preserve ACLs or hard links.</p>"},{"location":"files/rsync/#use-the-appropriate-resources","title":"Use the appropriate resources","text":"<p>When transferring files within the cluster, please use compute nodes when copying more than a few files.</p> <p>When transferring files into and out of the cluster, please use the transfer node. The various tools for that work are described in this document. The rsync information on this page can be useful for such transfers.</p>"},{"location":"files/rsync/#example-rsync-slurm-batch-job","title":"Example rsync SLURM batch job","text":"<p>Is shown here.</p>"},{"location":"files/rsync/#source-and-destination","title":"SOURCE and DESTINATION","text":"<p>Multiple items can be listed as SRC material. Of course there can only be one DEST.</p> <p>Rsync never updates the SRC location. Changes, if any, only occur in the DEST.</p> <p>Files in DEST but not SRC will not be deleted unless you specify one of the delete flags.</p> <p>SRC and DEST can be paths or hostnames with colons and paths or a combination. rsync will use ssh to send files between hosts, so you can even specify a different username for one of the SRC and DEST.</p> <p>Here are the different scenarios and their core syntax:</p> Local copying: <code>rsync [OPTION...] SRC... DEST</code> Access via remote shell: Pull:              <code>rsync [OPTION...] [USER@]HOST:SRC... [DEST]</code> Push:              <code>rsync [OPTION...] SRC... [USER@]HOST:DEST</code>"},{"location":"files/rsync/#rsync-servers","title":"Rsync servers","text":"<p>One can configure an rsync server which is always listening for desired transactions. Very few people do that. JHPCE doesn't run any.  This is being mentioned because some of the language in the manual page can be confusing if you don't know about this.</p>"},{"location":"files/rsync/#mind-the-trailing-slash","title":"Mind the trailing slash!","text":"<p>If the SOURCE represents a directory then adding a trailing forward slash to it will cause the contents of the directory to be copied into DESTINATION. If there is no trailing forward slash, then the SOURCE  directory itself will be copied into DESTINATION.</p> Example If directory /some/source/place contains files a, b, and c <code>rsync -a /some/source/place/  /a/destination/location/</code> will result in the contents of /a/destination/location/ being a, b, and c. If the command was instead  <code>rsync -a /some/source/place  /a/destination/location/</code> will result in the contents being instead /a/destination/location/place <p>This behavior is also found with the old standard <code>cp</code> program when using the <code>-R</code> recursive flag!!!</p>"},{"location":"files/rsync/#rsync-examples","title":"Rsync Examples","text":"<p>It pays to be cautious when running commands which can cause many changes in short order.</p> <p>Rsync can be used to compare two directory trees without updating anything. If both possibly have unique data, then you want to be careful and run preliminary <code>--dry-run</code> commands with a variety of informational flags. </p> <p>Show what would be done but make no changes: <pre><code>rsync -a --verbose --dry-run --stats /local1 /other2\n</code></pre></p> <p>Determine what would be done in more detail (but make no changes): <pre><code>rsync -a --verbose --dry-run --stats --itemize-changes /local1 /other2\n</code></pre></p> <p>Only copy files ending in \".txt\" <pre><code>rsync -avz --include='*.txt' /src /dest\n</code></pre></p> <p>A good combination to consider using. Includes options to try to be complete, safe, and efficient.  <pre><code>rsync -avhAXH --progress --numeric-ids --sparse --one-file-system --stats --delete-after\n</code></pre></p> <p>Tip</p> <p>To compare two directory trees, you can use a number of flags together to see what is different. Remember that rsync is unidirectional, so you would have to run it twice to compare in both directions. It might be true that each directory tree contains many duplicates but also some unique items. Direct the output into a text file for easier viewing and comparing.</p> <p>These include: `--dry-run --itemize-changes --delete --stats</p> <code>--dry-run</code> don't change anything <code>--itemize-changes</code> show what would be modified about each file <code>--delete-after</code> identify any extra files found in DEST that aren't in SRC using the efficient choice for large collections of files <code>--stats</code> flags and produce a summary of file numbers <p>However, <code>--itemize-changes</code> produces a cryptic 11 character string for each file or directory. We have copied a useful chart from the Internet that you can consult. See this page for the key and this page for the manual page section.</p>"},{"location":"files/rsync/#rsync-flags-you-may-want-to-use","title":"Rsync Flags You May Want To Use","text":"<p>Rsync is very flexible. The manual page is long. Here are some useful arguments to know about, organized somewhat by purpose. </p> <p>Most flags have two forms you can choose between, a short one consisting of a single character, or a readable form preceeded by two hyphens, and typically followed by an equals sign and a value. <pre><code>--archive, -a            archive mode; equals -rlptgoD (no -H,-A,-X)\n\n--acls, -A               preserve ACLs (implies --perms)\n--xattrs, -X             preserve extended attributes\n--hard-links, -H                       preserve hard links\n\n--exclude-from={'list.txt'}.        there is a corresponding --include-from\n--exclude={'*.txt','dir3','dir4'}   there is a corresponding --include\n\n--dry-run, -n            perform a trial run with no changes made\n--list-only              list the files instead of copying them\n--ignore-existing        don't overwrite existing files, no matter what\n--max-delete=NUM         don't delete more than NUM files (SET TO 0 OR LOW NUMBER FOR SAFETY)\n--one-file-system, -x    ensure that you stay inside the SRC file system \n\n--atimes -U              preserve access (use) times\n--crtimes, -N            preserve create times (newness)\n--times, -t              preserve modification times\n\n--numeric-ids            don't map uid/gid values by user/group name (more efficient)\n--delete-after           wait until end to process deletes (much more efficient than deleting during)\n--partial                allows you to resume an interrupted transfer\n\n--progress               show which file is being copied (not stored in log file!!)\n--itemize-changes, -i    output a change-summary for all updates \n                         has complex output. (see below for a link)\n--log-file=FILE          useful when using --itemize\n--verbose, -v            increase verbosity\n--info=FLAGS             fine-grained informational verbosity\n--human-readable, -h     output numbers in a human-readable format\n--stats                  provide statistics at the end\n\n--size-only              skip files that match in size (know what you're doing)\n--ignore-times, -I       don't skip files that match size and time (when in doubt...)\n--checksum -c            skip based on checksums, not mod-time &amp; size\n\n--sparse                 turn sequences of nulls into sparse blocks\n--bwlimit                limit the impact of the rsync on the network (in kb/sec)\n\n--chmod=CHMOD            affect file and/or directory permissions\n--usermap=STRING         custom username mapping \n--groupmap=STRING        custom groupname mapping (STRING is not simply\n--chown=USER:GROUP       simple username/groupname mapping \n</code></pre></p>"},{"location":"files/rsync/#archive-tools-moving-data-through-a-pipe","title":"Archive tools moving data through a pipe","text":"<p>Rsync is usually the best method. But rsync wasn't always available in the past, or it didn't support copying one or another attribute that more basic tools did.  Different kinds of file permissions, data forks, etc.</p> <p>One method that can be quick and effective is to use an archive program like tar to create an archive file which is passed through an input/output pipe to the same program running in another directory that extracts files into that location. This technique can use available system memory as buffer space, leading to smoother flows of data as disk reads and writes occur with optimal amounts of bytes.</p> <p>This example copies the named directory some-directory from the current working directory to another location. Because tar by default works with blocks of data 512 bytes in size, a higher efficiency is achieved by telling it to create larger blocks to reduce the overhead of doing input/output requests in such small sizes.</p> <p><pre><code>tar -cbf 20480 - some-directory | (cd /destination/place; tar -xbf 20480 -)\n</code></pre> This example extends the technique to copying the data off of your current host to another host: <pre><code>tar -cbf 20480 - some-directory | ssh myaccount@another-host (cd /destination/place; tar -xbf 20480 -)\"\n</code></pre></p>"},{"location":"files/sharing-files/","title":"Sharing Files","text":"<p>How do you safely collaborate with others in a UNIX environment in an on-going basis?</p> <p>There are different locations where you can store files for the long term. The correct location to store files depends on several factors, including their size, how private they are, whether other users should be able to read or write them, and whether they should remain for your group after you have gone to another organization.</p> <p>Traditional Unix file and group permissions can be used to share access to files and directories.  However, there can be times when more fine-grained control of shared access is needed. To accomplish this, Access Control Lists (ACLs) can be used. They add to the normal permissions.</p> <p>Tip</p> <p>Before defining ACLs, you should first read and understand this document for necessary background concepts and skills. Then you can proceed to read our document about ACLs is here.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#unix-ownership-permissions","title":"UNIX Ownership &amp; Permissions","text":"<p>You must understand the basics of file ownership and permissions to successfully share files. </p> <p>The Wikipedia has a fairly good description of normal UNIX file and group permissions, including the symbolic and numeric notation schemes.</p> <p>Here are some other tutorials you can read:</p> <ul> <li>https://docs.nersc.gov/filesystems/unix-file-permissions/</li> <li>https://www.tutorialspoint.com/unix/unix-file-permission.htm</li> <li>https://www.redhat.com/sysadmin/linux-file-permissions-explained</li> <li>https://kb.iu.edu/d/abdb</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#important-concepts","title":"Important Concepts","text":"<p>Out of the basic information on this topic, these are some particular details you should understand. </p> umask umask is a variable which controls the permissions assigned to files and directories that you create. You have a default umask, which can be changed for future logins if you change your <code>.bashrc</code> file. You can change the current umask at any time before you do run some commands. More guidance is included later in this document. The Wikipedia page for umask is helpful.  re-use of permission bits There are nine basic permission bits for files and directories. Three each for owner, group, and other. As UNIX developed and new capabilities were needed, the authors added one more bit (used for setuid and setgid), then started adding multiple meanings to some bits. This is shown by the letter (and its capitalization) used to represent it in the output of the <code>ls -l</code> command. read and execute bits on directories The role of the read and execute bits on directories is somewhat different than that on files.  A directory is essentially a special kind of file, containing details about its contents. So you need to be able to read the directory in order to list its contents. On a directory, the execute bit means \"search\" or \"traverse\" Users must have appropriate permissions on ALL of the parts of a path needed to reach a final file or directory. They don't need to be able to modify all of the parent directories, but they have to be able to descend through the tree of directories. /dcs07/somegroup/data/src/compile.py An example absolute path composed of four directories and a file. You cannot read the file unless the four directories and the file have appropriate permissions and group memberships. You can only modify the file if you have write permission on it. You can create other files in the same directory only if the <code>src</code> directory has the necessary writable bit enabled.","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#unix-commands-to-know","title":"UNIX Commands To Know","text":"<p>You can learn more about these commands by reading their manual page. You can run the command <code>man command-name</code> to read it locally. Tips for reading manual pages can be found in this PDF document.</p> <p>Only some of the arguments needed are shown in this table.</p> Command Description Notes ls -l List file details Lists both files &amp; dirs ls -ld List dir details <code>-d</code> option says not to list contents of dir umask Display or change umask See footnote<sup>1</sup> id Display your user &amp; grp Specify another user to see their info chmod Change permissions use <code>-R</code> option for recursion chgrp Change grp of file or dir use <code>-R</code> option for recursion chown Change owner of file or dir Ask sysadmins to change ownership newgrp - Create new shell with different group the hyphen option is useful sg Execute cmd with different grp mkdir Make a directory You can make a tree of new dirs with <code>-p newdir1/subdir1/subdir2</code> rmdir Remove an empty directory Use <code>/bin/rm -rf</code> to delete non-empty dirs <p>(To keep table size down, we used abbrieviations: dir for directories, grp for groups, cmd for command.)</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#access-control-lists-acls","title":"Access Control Lists - ACLs","text":"<p>Sometimes you want to give access to an individual or a group in ways that the traditional UNIX permissions and ownership model do not allow. You can use an ACL to grant those permissions, including </p> <p>See this document for details on how to do this.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#where-to-share","title":"Where to share?","text":"","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#project-space","title":"Project Space","text":"<p>The best place for long-term sharing of files, and the only place to share any significant volume of files, is in file systems created for research groups who purchase allocations of one of our storage servers.</p> <p>If your need is temporary or you cannot purchase your own space, you might be able to secure permission from an existing owner to use their space. They will need to create a subdirectory within their allocation for you and change the permissions of the directories above that to allow you and your group to see into those directories. We are happy to create a new UNIX group so you can use it in your permissions scheme. Send the request with the desired group name and a list of member usernames to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#home-directory","title":"Home Directory","text":"<p>Everyone has a home directory. By default this is a private space. Things like SSH can break if the permissions on the home directory are set incorrectly.</p> <p>You don't want to open up your home directory to everyone in the cluster by making your home directory group or world readable or writable. By default all users belong to the same group. It's not just that people you like and trust can see your files -- so can hackers if they get access to anyone's account. Your configuration files reveal details about your account, your accounts elsewhere and, if writable, allow hackers to set traps so they can become you. Then possibly access your home or other computers.</p> <p>If you must use your home directory, please use ACLs to give specific access to specific people. We have written an ACL document to guide you. We are happy to create a new UNIX group so you can use it in your permissions scheme. Send the request with the desired group name and a list of member usernames to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#scratch-space","title":"Scratch Space","text":"<p>If you need to briefly share some files with someone and they are too large to send via email (say 5MB), you might consider placing them in shared scratch space. This is not recommended, but we want you to be informed about this choice before you make it.</p> <p>If you and your recipient are members of some common group other than the default, it is best to make your temporary directory group readable and searchable but remove access for \"other\".</p> /tmp An acceptable place for a quick one-time exchange of small files which don't contain protected info IF YOU ARE CAREFUL. This directory is world-readable and writable. A prime consideration is whether there is enough room in /tmp for your purpose. The operating system and other users depend on there ALWAYS being enough free space in this file system. Delete your files ASAP after the exchange. /fastscratch An acceptable place for exchanging larger files, but not for sharing over times longer than, say, a week. Everyone has a 1TB quota on this fast file system, but there is only 22TB of space. This space is meant to enable fast access to data read or written by jobs on the compute nodes. Files older than 30 days are deleted automatically. This document describes this file system in more detail.","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#group-writable-directories","title":"Group Writable Directories","text":"<p>Authoring Note</p> <p>Most remaining work on this page lies below this point. Above here what is missing is mainly some example command output.</p> <p>Every file or directory has an owner and a group. Every user has a primary group, and can belong to one or more secondary groups.</p>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#group-sticky-bit","title":"Group sticky bit","text":"<p>When you want to make a directory tree group writable but also configure the SGID bit on directories, you cannot use a simple recursive chmod command. Because that will apply the same permission to both files and directories.</p> <p>These commands are examples of how you can recursively set correct permissions. They use the <code>find</code> command to print path names to directories or files, then pass those to xargs to execute the given commands upon.  </p> <pre><code>cd into-the-top-of-the-directory-tree\nfind . -type d | xargs chmod u+rwx,g+rwx,g+s,o-rwx\nfind . -type f | xargs chmod u+rwx,g+rw,o-rwx\n</code></pre> <p>This version adds arguements to <code>find</code> and <code>xargs</code> which handle the case where files or directory names contain evil, no-good, awful characters like white space, quote marks, or backslashes:</p> <pre><code>cd into-the-top-of-the-directory-tree\nfind . -type d -print0 | xargs --null chmod u+rwx,g+rwx,g+s,o-rwx\nfind . -type f -print0 | xargs --null chmod u+rwx,g+rw,o-rwx\n</code></pre>","tags":["in-progress","jeffrey"]},{"location":"files/sharing-files/#creating-a-group-writable-directory","title":"Creating A Group Writable Directory","text":"<p>Raw Material Here</p> <p>The following needs to be rewritten. It was sent as email to a user.</p> <p>The list command, ls, is crucial to figuring out what is possible, where. The long argument, -l, will show you ownership and permission information, as seen in my previous email. Sometimes you also want to use the -d argument to tell ls that you want to see the information about a directory and not what is inside it.</p> <p>So using ls -l  on /users/shared/55548/ will show you that many subdirectories are not group writable. Your failed efforts involve trying to create files or directories inside of directories that are group readable and searchable and sticky but not writable.</p> <p>Every directory in a path to the destination needs to have suitable permissions. Readability: every directory in the path /users/55548/shared/MA_switching needs to be readable by for you to be able to list its contents. Writability: only the last directory in the path needs to be writable for you to be able to modify its contents.</p> <pre><code>jhpcecms01:/users/55548/shared# ls -l\ndrwxrwsrwx  7 c-jlevy33-55548  c-55548  7 Dec  5 11:10 biosim_leverage/\ndrwxr-s---  3 c-eblasco1-55548 c-55548  3 Feb  1 14:00 c-eblasco1-55548/\ndrwxr-s---  7 c-tbrow261-10201 c-55548  9 Dec 20 12:17 Elixhauser/\ndrwxr-s---  3 c-ttotoja1-55548 c-55548  3 Feb 16 16:09 forOwen/\ndrwxrwxr-x 16 c-jxu123-10201   c-55548 22 Jul 15  2023 from-yoda/\ndrwxrwsrwx  7 c-jlevy33-55548  c-55548  7 Feb  7 10:54 hipfracture_adrd/\ndrwxr-s---  7 c-jlevy33-55548  c-55548  7 Dec 20 14:03 levy_partd/\ndrwxr-s---  2 c-aliu63-55548   c-55548  6 Feb 19 16:39 MA_partB/\ndrwxr-s---  2 c-aliu63-55548   c-55548  5 Dec 15 14:03 MA_switching/\ndrwxr-s--- 10 c-rwu32-55548    c-55548 13 Nov  8 10:31 mltss_duals/\ndrwxrws---  4 c-yyang279-55548 c-55548  4 Nov 30 10:13 shen_yang/\ndrwxr-s---  2 c-ykuang6-55548  c-55548  4 Jan 30 12:13 test/\ndrwxr-s---  2 c-jlevy33-55548  c-55548  3 Aug 29 15:19 toofull/\n</code></pre> <p>Only the owner of a directory can change its permissions. So in this case c-aliu63-55548 needs to change the permissions on MA_switching to make it group writable. This command run by them would change the directory: <code>chmod g+w /users/55548/shared/MA_Switching</code> This command run by them would change the directory and everything within it using the recursive flag: <code>chmod -R g+w /users/55548/shared/MA_Switching</code></p> <p>Why are files and directories created with the permissions that they get? A key factor here is the \u201cumask\u201d setting in the user\u2019s environment when files and directories are being created. The command \u201cumask -S\u201d will show you the permissions that will be used for new files and directories. Here you see an example of my inspecting the contents of my umask setting and then changing it to make it such that when I create a file it will be group-writable. If you want such a setting to be used in the future for all of your logins, you would add a line to the bottom of your .bashrc file in your home directory.</p> <p>jhpcecms01:/users/55548/shared# umask -S u=rwx,g=rx,o=rx jhpcecms01:/users/55548/shared# umask u=rwx,g=rwx,o=rx jhpcecms01:/users/55548/shared# umask -S u=rwx,g=rwx,o=rx</p> <p>Many of the people modifying files in the shared directory may want to adjust the permissions of existing files and directories, and change their umask values. If they want directory trees to be group writable and new files or directories to be the same. Or some other set of permissions. Not everything within the shared directory needs to be shared with every other user of the DUA.</p> <ol> <li> <p>There are 2 versions, one built into bash and a standalone one /usr/bin/umask. The latter has a <code>-S</code> option which displays permissions in a more readable fashion. The built-in only works with octal digits. To see the umask manual page which supports -S, use the command <code>man 1p umask</code> To use this version of the umask command, you need to specify the full path to it (/usr/bin/umask) instead of <code>umask</code>.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","jeffrey"]},{"location":"gpu/alphafold/","title":"Alphafold","text":""},{"location":"gpu/alphafold/#alphafold","title":"Alphafold","text":"<p>The Alphfold software is available as a module on the jhpce cluster.</p> <p><pre><code>[login31 /users/mmill116/alpha1]$ srun --pty --x11 --mem=100G --cpus-per-task=8 --partition gpu --gpus=1 bash\n[compute-126 /users/mmill116/alpha1]$ module load alphafold\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ nvidia-smi    (this is to verify that you are assigned a GPU)\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ ls\nquery.fasta\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ cat query.fasta\n&gt;dummy_sequence\nGWSTELEKHREELKEFLKKEGITNVEIRIDNGRLEVRVEGGTERLKRFLEELRQKLEKKGYTVDIKIE\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ run_alphafold.sh -d /legacy/alphafold/data -o . -f query.fasta  -t 2020-05-14 -n 8\nI0305 14:37:51.029889 140001535510336 templates.py:857] Using precomputed obsolete pdbs /legacy/alphafold/data/pdb_mmcif/obsolete.dat.\nI0305 14:37:52.841140 140001535510336 xla_bridge.py:353] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \nI0305 14:37:52.984758 140001535510336 xla_bridge.py:353] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA Host\n\n. . .\n\nI0305 15:27:23.305634 140001535510336 amber_minimize.py:178] alterations info: {'nonstandard_residues': [], 'removed_heterogens': set(), 'missing_residues': {}, 'missing_heavy_atoms': {}, 'missing_terminals': {}, 'Se_in_MET': [], 'removed_chains': {0: []}}\nI0305 15:27:23.643896 140001535510336 amber_minimize.py:500] Iteration completed: Einit 1628.92 Efinal -2082.65 Time 0.49 s num residue violations 0 num residue exclusions 0 \nI0305 15:27:23.721792 140001535510336 run_alphafold.py:277] Final timings for query: {'features': 2523.1148357391357, 'process_features_model_1_pred_0': 2.198369026184082, 'predict_and_compile_model_1_pred_0': 92.24924182891846, 'relax_model_1_pred_0': 5.6730005741119385, 'process_features_model_2_pred_0': 1.1774885654449463, 'predict_and_compile_model_2_pred_0': 95.11810064315796, 'relax_model_2_pred_0': 3.1857430934906006, 'process_features_model_3_pred_0': 0.9210422039031982, 'predict_and_compile_model_3_pred_0': 74.00832223892212, 'relax_model_3_pred_0': 3.2094907760620117, 'process_features_model_4_pred_0': 0.9562208652496338, 'predict_and_compile_model_4_pred_0': 73.54764223098755, 'relax_model_4_pred_0': 3.2828869819641113, 'process_features_model_5_pred_0': 0.9843571186065674, 'predict_and_compile_model_5_pred_0': 72.18165755271912, 'relax_model_5_pred_0': 3.2641286849975586}\n\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ ls\nquery  query.fasta\n(alphafold-gpu) [compute-126 /users/mmill116/alpha1]$ ls query\nfeatures.pkl  ranked_2.pdb        relaxed_model_1_pred_0.pdb  relaxed_model_5_pred_0.pdb  result_model_3_pred_0.pkl  unrelaxed_model_1_pred_0.pdb  unrelaxed_model_5_pred_0.pdb\nmsas          ranked_3.pdb        relaxed_model_2_pred_0.pdb  relax_metrics.json          result_model_4_pred_0.pkl  unrelaxed_model_2_pred_0.pdb\nranked_0.pdb  ranked_4.pdb        relaxed_model_3_pred_0.pdb  result_model_1_pred_0.pkl   result_model_5_pred_0.pkl  unrelaxed_model_3_pred_0.pdb\nranked_1.pdb  ranking_debug.json  relaxed_model_4_pred_0.pdb  result_model_2_pred_0.pkl   timings.json               unrelaxed_model_4_pred_0.pdb\n</code></pre> To use alphafold without a GPU, you would run: <pre><code>[login31 /users/mmill116/alpha1]$ srun --pty --x11 --mem=100G --cpus-per-task=8 bash\n[compute-129 /users/mmill116/alpha2]$ module load alphafold\n(alphafold-gpu) [compute-129 /users/mmill116/alpha2]$ ls\nquery.fasta\n(alphafold-gpu) [compute-129 /users/mmill116/alpha2]$ cat query.fasta\n&gt;dummy_sequence\nGWSTELEKHREELKEFLKKEGITNVEIRIDNGRLEVRVEGGTERLKRFLEELRQKLEKKGYTVDIKIE\n(alphafold-gpu) [compute-129 /users/mmill116/alpha2]$ run_alphafold.sh -d /legacy/alphafold/data -o . -f query.fasta  -t 2020-05-14 -n 8 -g False\nI0305 14:57:32.565061 140585066018624 templates.py:857] Using precomputed obsolete pdbs /legacy/alphafold/data/pdb_mmcif/obsolete.dat.\nI0305 14:57:34.727731 140585066018624 xla_bridge.py:353] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n2024-03-05 14:57:34.729707: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /jhpce/shared/jhpce/core/JHPCE_tools/3.0/lib\n2024-03-05 14:57:34.729740: W external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\nI0305 14:57:34.730122 140585066018624 xla_bridge.py:353] Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n\n. . .\nI0305 16:54:48.980324 140585066018624 amber_minimize.py:408] Minimizing protein, attempt 1 of 100.\nI0305 16:54:49.271893 140585066018624 amber_minimize.py:69] Restraining 574 / 1170 particles.\nI0305 16:54:54.823270 140585066018624 amber_minimize.py:178] alterations info: {'nonstandard_residues': [], 'removed_heterogens': set(), 'missing_residues': {}, 'missing_heavy_atoms': {}, 'missing_terminals': {}, 'Se_in_MET': [], 'removed_chains': {0: []}}\nI0305 16:54:55.044994 140585066018624 amber_minimize.py:500] Iteration completed: Einit 1776.57 Efinal -2114.81 Time 4.96 s num residue violations 0 num residue exclusions 0 \nI0305 16:54:55.143276 140585066018624 run_alphafold.py:277] Final timings for query: {'features': 3216.2297768592834, 'process_features_model_1_pred_0': 6.108574390411377, 'predict_and_compile_model_1_pred_0': 821.0001108646393, 'relax_model_1_pred_0': 12.479053258895874, 'process_features_model_2_pred_0': 2.8280999660491943, 'predict_and_compile_model_2_pred_0': 780.6746006011963, 'relax_model_2_pred_0': 9.498374223709106, 'process_features_model_3_pred_0': 2.3144617080688477, 'predict_and_compile_model_3_pred_0': 732.0320601463318, 'relax_model_3_pred_0': 8.531827449798584, 'process_features_model_4_pred_0': 2.274519920349121, 'predict_and_compile_model_4_pred_0': 732.8238620758057, 'relax_model_4_pred_0': 9.743109226226807, 'process_features_model_5_pred_0': 2.1754395961761475, 'predict_and_compile_model_5_pred_0': 668.7868385314941, 'relax_model_5_pred_0': 8.610054969787598}\n(alphafold-gpu) [compute-096 /users/mmill116/alpha2]$ ls\nquery  query.fasta\n(alphafold-gpu) [compute-096 /users/mmill116/alpha2]$ ls query\nfeatures.pkl  ranked_1.pdb  ranked_4.pdb                relaxed_model_2_pred_0.pdb  relaxed_model_5_pred_0.pdb  result_model_2_pred_0.pkl  result_model_5_pred_0.pkl     unrelaxed_model_2_pred_0.pdb  unrelaxed_model_5_pred_0.pdb\nmsas          ranked_2.pdb  ranking_debug.json          relaxed_model_3_pred_0.pdb  relax_metrics.json          result_model_3_pred_0.pkl  timings.json                  unrelaxed_model_3_pred_0.pdb\nranked_0.pdb  ranked_3.pdb  relaxed_model_1_pred_0.pdb  relaxed_model_4_pred_0.pdb  result_model_1_pred_0.pkl   result_model_4_pred_0.pkl  unrelaxed_model_1_pred_0.pdb  unrelaxed_model_4_pred_0.pdb\n</code></pre></p> <p>In our testing, the GPU version took about 50 minutes to run, and the non-GPU version took nearly 2 hours.</p> <p>You can use \u201crun_alphafold.sh --help\u201d to see all of the options for running alphafold.</p> <p>You do also need a fair amount of RAM.  In the above exmaples, we requested 100GB, and in looking at the \u201csacct\u201d output for these example jobs,  only used about 17GB for the non-gpu run and 34GB for the gpu run.</p> <pre><code>[login31 /users/mmill116]$ sacct -j 2907929 -o JobID,JobName,partition,AllocTres%40,MaxVMSize,MAXRSS,State%20 --units=G\nJobID           JobName  Partition                                AllocTRES  MaxVMSize     MaxRSS                State \n------------ ---------- ---------- ---------------------------------------- ---------- ---------- -------------------- \n2907929            bash     shared     billing=102408,cpu=8,mem=100G,node=1                                  COMPLETED \n2907929.ext+     extern                billing=102408,cpu=8,mem=100G,node=1          0          0            COMPLETED \n2907929.0          bash                               cpu=8,mem=100G,node=1    100.96G     17.47G            COMPLETED \n[login31 /users/mmill116]$ sacct -j 2907783 -o JobID,JobName,partition,AllocTres%40,MaxVMSize,MAXRSS,State%20 --units=G\nJobID           JobName  Partition                                AllocTRES  MaxVMSize     MaxRSS                State \n------------ ---------- ---------- ---------------------------------------- ---------- ---------- -------------------- \n2907783            bash        gpu billing=102410,cpu=8,gres/gpu:tesa100=1+                                  COMPLETED \n2907783.ext+     extern            billing=102410,cpu=8,gres/gpu:tesa100=1+          0          0            COMPLETED \n2907783.0          bash            cpu=8,gres/gpu:tesa100=1,gres/gpu=1,mem+    109.65G     34.30G            COMPLETED \n</code></pre> <p>To submit an alphafold job, you would create a SLURM batch file:</p> <p><pre><code>[login31 /users/mmill116/alphafold]$ cat alpha1.sh\n#!/bin/sh\n#SBATCH --mem=100G\n#SBATCH --cpus-per-task=8\n#SBATCH --gpus=1\n#SBATCH --partition=gpu\n\ndate\nmodule load conda\nconda activate alphafold-gpu\nmkdir /tmp/mm-alphfold1 \nrun_alphafold.sh -d /legacy/alphafold/data -o /tmp/mm-alphfold1 -f ./test.fasta \n-t 2020-05-14 -n 8\ndate\n</code></pre> You would then submit the job with \"sbatch\", perhaps adding additional GPU options <pre><code>[login31 /users/mmill116/alphafold]$ sbatch --gres=gpu:tesa100:1 alpha1.sh\nSubmitted batch job 4679334\n[login31 /users/mmill116/alphafold]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           4679334       gpu alpha1.s mmill116  R       0:02      1 compute-126\n[login31 /users/mmill116/alphafold]$ cat slurm-4679334.out\nThu Apr 18 03:20:24 PM EDT 2024\nI0418 15:22:46.104061 140350337955648 templates.py:857] Using precomputed obsolete pdbs /legacy/alphafold/data/pdb_mmcif/obsolete.dat.\nI0418 15:22:48.993710 140350337955648 xla_bridge.py:353] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n\n . . .\n\nI0418 15:23:01.681938 140350337955648 run_alphafold.py:161] Predicting test\nI0418 15:23:01.682810 140350337955648 jackhmmer.py:133] Launching subprocess \"/jhpce/shared/jhpce/core/conda/miniconda3-23.3.1/envs/alphafold-gpu/bin/jackhmmer -o /dev/null -A /tmp/tmpfcl3sb6v/output.sto --noali --F1 0.0005 --F2 5e-05 --F3 5e-07 --incE 0.0001 -E 0.0001 --cpu 8 -N 1 ./test.fasta /legacy/alphafold/data/uniref90/uniref90.fasta\"\nI0418 15:23:01.998645 140350337955648 utils.py:36] Started Jackhmmer (uniref90.fasta) query\n</code></pre></p>"},{"location":"gpu/gpu-info/","title":"Using GPUs on JHPCE","text":"<p>We have a number of GPU nodes on the JHPCE cluster that are available for general use. Below is the process for accessing the GPU node with an interactive session, and a couple of examples of running programs and submitting jobs that utilizes a GPU.</p> <p>From the JHPCE SLURM cluster login node, you can access a GPU node interactively by using the \u201c\u2013partition gpu\u201d and \u201c\u2013gpus\u201d options to the srun command. You can also supply traditional options to srun, and you may find that you need to request additional system RAM for your program to run. Here is an example of how to request a single GPU on the \u201cgpu\u201d partition. You can run \u201cnvidia-smi\u201d to see the GPU that you\u2019ve been assigned.</p> <pre><code>[login31 /users/mmill116]$ srun --pty --x11 --partition gpu --gpus=1 --mem=20G bash \n[compute-117 /users/mmill116]$ nvidia-smi \n Tue Nov 28 08:39:23 2023       \n +---------------------------------------------------------------------------------------+\n | NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n |-----------------------------------------+----------------------+----------------------+\n | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n |                                         |                      |               MIG M. |\n |=========================================+======================+======================|\n |   0  Tesla V100-PCIE-32GB           On  | 00000000:89:00.0 Off |                    0 |\n | N/A   41C    P0              28W / 250W |      0MiB / 32768MiB |      0%      Default |\n |                                         |                      |                  N/A |\n +-----------------------------------------+----------------------+----------------------+\n +---------------------------------------------------------------------------------------+\n | Processes:                                                                            |\n |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n |        ID   ID                                                             Usage      |\n |=======================================================================================|\n |  No running processes found                                                           |\n +---------------------------------------------------------------------------------------+\n\n<p>\nAs of November 2024, we have the following GPUs available on the gpu partition:</p>\n\n\nNode Name\nDescription\nGPU Count\n\n\n\ncompute-117\nOur first GPU node.2 Intel(R) Xeon(R) Silver 4116 CPUs and 384GB of RAM\n 2 Nvidia V100 GPUs with 32GB RAM 1 Nvidia Titan V with 11GB RAM\n\n\n\ncompute-123\n The first Biostat GPU node2 Intel(R) Xeon(R) Silver 4210R CPUs and 768GB of RAM\n 4 Nvidia V100s GPUs with 32GB RAM\n\n\n\ncompute-126\nOne of the Lieber (Collado) GPU node2 Intel(R) Xeon(R) Gold 5317 CPUs and 512GB of RAM\n4 Nvidia A100 GPUs with 80GB RAM\n\n\n\ncompute-170\n The second Biostat GPU node1 Intel(R) Xeon(R) Silver 4510R CPUs and 1TB of RAM\n2 Nvidia H100s GPUs with 96GB RAM\n\n\n\ncompute-171 - compute-173\nGroup of 3 Biostat GPU nodes, sharing GPUs with the general gpu queue.1 AMD EPYC 7443P CPUs and 1TB of RAM (each node)\n12 Nvidia L40S GPUs with 46GB RAM - 4 GPUs in each node\n\n\n\n\n<p>You can request a particular model of GPU using the \"GRES\" option\n(Generic Resources) to srun and sbatch. The following GRES options are available for\nthe various models of GPUS:</p>\n\nGPU TypeGRES  Option\nNvidia Titan V with 11GB RAMtitanv\nNvidia V100 with 32GB RAMtesv100\nNvidia V100S with 32GB RAMtesv100s\nNvidia A100 with 80GB RAMtesa100\nNvidia H100 with 96GB RAMtesh100\nNvidia L40S with 46GB RAMl40s\n\n\n<p>NVIDIA assigns different \"Computing Capability\" values to each GPU model. These values can be found here, and help one determine what the GPU is capable of doing. Details can be found here. Specific descriptions for each generation as well as some specific CUDA routines to use with each are here: 7.x, 8.x, and 9.x</p>\n\nGPU TypeGRES  OptionComputing Capability\nNvidia Titan V with 11GB RAMtitanv7.0\nNvidia V100 with 32GB RAMtesv1007.0\nNvidia V100S with 32GB RAMtesv100s7.0\nNvidia A100 with 80GB RAMtesa1008.0\nNvidia H100 with 96GB RAMtesh1009.0\nNvidia L40S with 46GB RAMl40s8.9\n\n\n<p>You can see what GPUs are available by running the <code>slurmpic -g</code> command.\nThe GPUS column will show how many GPUs are in use and how many are available\non each node:</p>\n<p></p>\n<p>One commonly used option when using GPUs in a SLURM environment is the <code>--gpu-bind=closest</code>\noption. This will make sure the core/CPU that is assigned to your job is the closest one to the controling bus that the GPU is on. So if you are trying to get the most out of the performance of your GPU code, this option may be helpful.  There is a really good description of GPU/CORE binding at https://pawsey.atlassian.net/wiki/spaces/US/pages/51929056/Example+Slurm+Batch+Scripts+for+Setonix+on+GPU+Compute+Nodes</p>\n<p>You will also notice that there are several partitions with GPUs in them.  By\ndefault all users should only access the \"gpu\" partition. The other partitions\nar PI-specific partitions for groups that have purchases GPUs for the JHPCE\ncluster.</p>\n<p>So, in order request a particular model of GPU you would use the value in the\n\u201cGRES Option\u201d column above to srun or sbatch. In general it is\nbetter to allow the cluster to assign an available gpu to you rather than\nrequesting a particular model, as certain models may not be available at the\ntime you are trying to run your program. In the below example, we are\nrequesting an Nvidia Titan V GPU.</p>\n<pre><code>[login31 /users/mmill116]$ srun --pty --x11 --partition gpu --gres=gpu:titanv:1\u00a0 bash\n[compute-117 /users/mmill116]$ nvidia-smi\n Tue Nov 28 10:50:25 2023\u00a0 \u00a0 \u00a0 \u00a0\n +---------------------------------------------------------------------------------------+\n | NVIDIA-SMI 535.86.10\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Driver Version: 535.86.10\u00a0 \u00a0 CUDA Version: 12.2 \u00a0 \u00a0 |\n |-----------------------------------------+----------------------+----------------------+\n | GPU\u00a0 Name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Persistence-M | Bus-Id\u00a0 \u00a0 \u00a0 \u00a0 Disp.A | Volatile Uncorr. ECC |\n | Fan\u00a0 Temp \u00a0 Perf\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Pwr:Usage/Cap | \u00a0 \u00a0 \u00a0 \u00a0 Memory-Usage | GPU-Util\u00a0 Compute M. |\n | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MIG M. |\n |=========================================+======================+======================|\n | \u00a0 0\u00a0 NVIDIA TITAN V \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 On\u00a0 | 00000000:B1:00.0 Off |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 N/A |\n | 28% \u00a0 35C\u00a0 \u00a0 P8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 26W / 250W |\u00a0 \u00a0 \u00a0 0MiB / 12288MiB |\u00a0 \u00a0 \u00a0 0%\u00a0 \u00a0 \u00a0 Default |\n | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 N/A |\n +-----------------------------------------+----------------------+----------------------+\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n +---------------------------------------------------------------------------------------+\n | Processes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\n |\u00a0 GPU \u00a0 GI \u00a0 CI\u00a0 \u00a0 \u00a0 \u00a0 PID \u00a0 Type \u00a0 Process name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GPU Memory |\n |\u00a0 \u00a0 \u00a0 \u00a0 ID \u00a0 ID \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Usage\u00a0 \u00a0 \u00a0 |\n |=======================================================================================|\n |\u00a0 No running processes found \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\n +---------------------------------------------------------------------------------------+ \n\n<p></p>\n<p>Another option for for requesting a particular GPU model is to use the \"--constraints\" option to request a particular Node Feature defined for the node.  This is especially helpful when you want to select a number of GPU models that would be acceptible to you to use.  For example, one could run the folloing to request a node with either an l40s, tesv100, or titanv.</p>\n<pre><code>\n[mmill116@jhpce01 ~]$ srun --pty -p gpu --gpus=1 --constrain=\"l40s|tesv100|titanv\" bash \n</code></pre>\n\n<p>You can read more about using features at https://jhpce.jhu.edu/slurm/node-features/</p>\n<p>At this point you can start running your GPU specific code. You can either use\ninstall your own GPU-enabled programs, or use the version of python that is\ninstalled on the GPU nodes.</p>\n<p>Below is an example of running an MNIST tensorflow program. The tensorflow and keras python modules have been installed on the GPU nodes, so you can use the default system version of python. This example comes from https://www.tensorflow.org/tutorials/quickstart/beginner</p>\n<pre><code>[login31 /users/mmill116]$ srun --pty --x11 --partition gpu --gpus=1 --mem=10G bash\n[compute-117 /users/mmill116]$ python\nPython 3.9.16 (main, Dec  8 2022, 00:00:00) \n[GCC 11.3.1 20221121 (Red Hat 11.3.1-4)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import tensorflow as tf\n2023-11-28 09:05:16.067766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-28 09:05:16.850666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n&gt;&gt;&gt; mnist = tf.keras.datasets.mnist\n&gt;&gt;&gt; (x_train, y_train), (x_test, y_test) = mnist.load_data()\n&gt;&gt;&gt; x_train, x_test = x_train / 255.0, x_test / 255.0\n&gt;&gt;&gt; model = tf.keras.models.Sequential([\n...   tf.keras.layers.Flatten(input_shape=(28, 28)),\n...   tf.keras.layers.Dense(128, activation='relu'),\n...   tf.keras.layers.Dropout(0.2),\n...   tf.keras.layers.Dense(10)\n... ])\n2023-11-28 09:05:31.942590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -&gt; device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n&gt;&gt;&gt; predictions = model(x_train[:1]).numpy()\n&gt;&gt;&gt; predictions\narray([[-0.745112  ,  0.49414337, -0.10749201, -0.23818162,  0.2159372 ,\n        -0.38107562,  0.8540315 , -0.21077928,  0.04448523,  0.37432173]],\n      dtype=float32)\n&gt;&gt;&gt; tf.nn.softmax(predictions).numpy()\narray([[0.0417023 , 0.14399974, 0.07889961, 0.06923362, 0.10902808,\n        0.06001488, 0.206376  , 0.07115702, 0.09184969, 0.12773912]],\n      dtype=float32)\n&gt;&gt;&gt; loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n&gt;&gt;&gt; loss_fn(y_train[:1], predictions).numpy()\n2.8131628\n&gt;&gt;&gt; model.compile(optimizer='adam',\n...               loss=loss_fn,\n...               metrics=['accuracy'])\n&gt;&gt;&gt; model.fit(x_train, y_train, epochs=5)\nEpoch 1/5\n2023-11-28 09:06:00.300076: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fee20069290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2023-11-28 09:06:00.300099: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n2023-11-28 09:06:00.305647: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-11-28 09:06:00.371150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n2023-11-28 09:06:00.442691: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2023-11-28 09:06:00.535627: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.2931 - accuracy: 0.9150\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1395 - accuracy: 0.9597\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1070 - accuracy: 0.9678\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0877 - accuracy: 0.9729\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0754 - accuracy: 0.9764\n&lt;keras.src.callbacks.History object at 0x7feef9f09550&gt;\n&gt;&gt;&gt; model.evaluate(x_test,  y_test, verbose=2)\n313/313 - 1s - loss: 0.0714 - accuracy: 0.9756 - 600ms/epoch - 2ms/step\n[0.07143650203943253, 0.975600004196167]\n&gt;&gt;&gt; </code></pre>\n\n<p>You can also submit a batch job to the cluster that uses GPUs. In this example\nof submitting a batch job to use a GPU, we are creating 2 files, one containing\nthe python steps that we used above, and the second containing a shell script\nthat will be submitted to SLURM. The Python program looks like:</p>\n<pre><code>[login31 /users/mmill116/gpu-test]$ ls -al\n total 496\n drwxr-xr-x    2 mmill116 mmi   4 Nov 28 11:49 .\n drwxr-x---+ 214 mmill116 mmi 412 Nov 28 11:49 ..\n -rw-r--r--    1 mmill116 mmi 789 Nov 28 11:47 nvidia-test.py\n -rwxr-xr-x    1 mmill116 mmi 343 Nov 28 11:49 test-slurm-gpu.sh\n[login31 /users/mmill116/gpu-test]$ cat nvidia-test.py\n import tensorflow as tf\n print(\"TensorFlow version:\", tf.__version__)\n mnist = tf.keras.datasets.mnist\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n x_train, x_test = x_train / 255.0, x_test / 255.0\n model = tf.keras.models.Sequential([\n   tf.keras.layers.Flatten(input_shape=(28, 28)),\n   tf.keras.layers.Dense(128, activation='relu'),\n   tf.keras.layers.Dropout(0.2),\n   tf.keras.layers.Dense(10)\n ])\n predictions = model(x_train[:1]).numpy()\n predictions\n tf.nn.softmax(predictions).numpy()\n loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n loss_fn(y_train[:1], predictions).numpy()\n model.compile(optimizer='adam',\n               loss=loss_fn,\n               metrics=['accuracy'])\n model.fit(x_train, y_train, epochs=5)\n model.evaluate(x_test,  y_test, verbose=2)\n</code></pre>\n\n<p>The script that will be used to submit to SLURM looks like. At this point you\nneed to include the LD_LIBRARY_PATH environemnt variable if you will be using\nthe system version of python:</p>\n<pre><code>&gt;[login31 /users/mmill116/gpu-test]$ cat test-slurm-gpu.sh\n!/bin/sh\nSBATCH --partition=gpu\nSBATCH --gres=gpu:titanv:1\n\necho $CUDA_VISIBLE_DEVICES\ncd $HOME/gpu-test\nnvidia-smi\nexport LD_LIBRARY_PATH=/jhpce/shared/jhpce/core/JHPCE_tools/3.0/lib:/usr/local/lib/python3.9/site-packages/nvidia/cudnn/lib:/jhpce/shared/jhpce/core/conda/miniconda3-23.3.1/envs/cudatoolkit-11.8.0/lib\n\npython nvidia-test.py\n\n<p></p>\n<p>You can now use \u201csbatch\u201d to submit the job, and examine the results.</p>\n<pre><code>\n[login31 /users/mmill116/gpu-test]$ sbatch test-slurm-gpu.sh\n Submitted batch job 915238\n [login31 /users/mmill116/gpu-test]$ squeue --me\n              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             915238       gpu test-slu mmill116  R       0:06      1 compute-117\n\n[login31 /users/mmill116/gpu-test]$ ls\n nvidia-test.py\u00a0 slurm-915238.out\u00a0 test-slurm-gpu.sh\n [login31 /users/mmill116/gpu-test]$ squeue --me\n \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JOBID PARTITION \u00a0 \u00a0 NAME \u00a0 \u00a0 USER ST \u00a0 \u00a0 \u00a0 TIME\u00a0 NODES NODELIST(REASON)\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 915238 \u00a0 \u00a0 \u00a0 gpu test-slu mmill116\u00a0 R \u00a0 \u00a0 \u00a0 0:21\u00a0 \u00a0 \u00a0 1 compute-117 \n[login31 /users/mmill116/gpu-test]$ squeue --me\n              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n[login31 /users/mmill116/gpu-test]$ ls\n nvidia-test.py  slurm-915238.out  test-slurm-gpu.sh\n [login31 /users/mmill116/gpu-test]$ cat slurm-915238.out\n 0\n Tue Nov 28 11:55:08 2023       \n +---------------------------------------------------------------------------------------+\n | NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n |-----------------------------------------+----------------------+----------------------+\n | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n |                                         |                      |               MIG M. |\n |=========================================+======================+======================|\n |   0  NVIDIA TITAN V                 On  | 00000000:B1:00.0 Off |                  N/A |\n | 28%   35C    P8              26W / 250W |      0MiB / 12288MiB |      0%      Default |\n |                                         |                      |                  N/A |\n +-----------------------------------------+----------------------+----------------------+\n +---------------------------------------------------------------------------------------+\n | Processes:                                                                            |\n |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n |        ID   ID                                                             Usage      |\n |=======================================================================================|\n |  No running processes found                                                           |\n +---------------------------------------------------------------------------------------+\n 2023-11-28 11:55:08.645381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n 2023-11-28 11:55:09.449932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n 2023-11-28 11:55:10.803612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10696 MB memory:  -&gt; device: 0, name: NVIDIA TITAN V, pci bus id: 0000:b1:00.0, compute capability: 7.0\n TensorFlow version: 2.13.0\n Epoch 1/5\n 2023-11-28 11:55:11.914159: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff35c066ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n 2023-11-28 11:55:11.914504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n 2023-11-28 11:55:11.920545: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var <code>MLIR_CRASH_REPRODUCER_DIRECTORY</code> to enable.\n 2023-11-28 11:55:11.985098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n 2023-11-28 11:55:12.057397: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n 2023-11-28 11:55:12.148968: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n 1875/1875 [==============================] - 5s 2ms/step - loss: 0.2948 - accuracy: 0.9142\n Epoch 2/5\n 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1433 - accuracy: 0.9574\n Epoch 3/5\n 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1078 - accuracy: 0.9675\n Epoch 4/5\n 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0884 - accuracy: 0.9725\n Epoch 5/5\n 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0751 - accuracy: 0.9769\n 313/313 - 1s - loss: 0.0713 - accuracy: 0.9780 - 600ms/epoch - 2ms/step\n [login31 /users/mmill116/gpu-test]$ \n\n<p></p>"},{"location":"help/cost-calculator/","title":"Cost Calculator","text":"<p>Should this document be renamed to something more general, like Billing or The Cost of Computing?</p> <p>Refer to policies at: https://jhpce.jhu.edu/joinus/policies/</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#why","title":"Why?","text":"","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#billing-process","title":"Billing Process","text":"","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#how-do-we-calculate-charges","title":"How do we calculate charges?","text":"<p>Requested not used, except for duration.</p> <p>CPU requested</p> <p>RAM requested</p> <p>Job duration</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#if-something-changes","title":"If something changes","text":"<p>Who to contact with what information?</p> <ul> <li>People leave</li> <li>People work for different PIs</li> <li>People work for multiple PIs</li> <li>PIs leave</li> </ul>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#cost-calculator_1","title":"Cost Calculator","text":"<p>Are we going to create a form? Or just give some examples?</p>","tags":["needs-to-be-written","mark"]},{"location":"help/cost-calculator/#seeing-your-past-usage","title":"Seeing Your Past Usage","text":"<p>The sacct command is used to view information about completed jobs. Here is our sacct document with tips for using that command.</p>","tags":["needs-to-be-written","mark"]},{"location":"help/external/","title":"External Guides","text":"<p>The World Wide Web has been of incalculable benefit to humanity. We owe Sir Tim Berners-Lee an enormous debt of gratitude.</p> <p>Tip</p> <p>If you have suggestions for this page, please send the link(s) and your reasons for nominating it/them to bitsupport.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#caveat-emptor","title":"CAVEAT EMPTOR","text":"<p>You need to consider whether what you're reading on an external web site applies to JHPCE. Policies and practices differ between clusters, as does available software and their versions.  Don't copy-and-paste blindly. Nonetheless, there is a wealth of information out there that you can apply to your work.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#slurm-web-sites-of-note","title":"SLURM Web Sites of Note","text":"<p>We are accumulating this information on this page.</p> <p>We also have a document with links to not only the vendor's documentation for each common command, but also pages we've written with advice and examples for some important commands.</p>","tags":["in-progress","jeffrey"]},{"location":"help/external/#user-tutorials","title":"User tutorials","text":"<ul> <li>R/Bioconductor-powered Team Data Science - this site has a wealth of information for improving your data science and teamwork skills. A real gem for people new to the field!</li> <li>Lieber Institute RStats club</li> <li>Leo Collado Torres YouTube Channel including JHPCE playlist - note that the JHPCE playlist starts off with videos featuring the Sung Grid Engine job scheduler, which has been replaced by SLURM in 2023. The data science contents of the SGE-containing videos may still be of interest.</li> <li>Lieber Institute module config and Lieber Institute module source code - Lieber has generously built and maintains many software modules for JHPCE users</li> <li>Youtube videos from Lieber RSTATS club on laptop setup:<ul> <li>macOS: https://youtu.be/8tuI-ZAlZ1Y</li> <li>winOS: https://youtu.be/qtn3eTwFlZI</li> </ul> </li> <li>Another Lieber video about JHPCE https://twitter.com/lcolladotor/status/1779209787340681678</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"help/external/#jhpce-tutorials","title":"JHPCE Tutorials","text":"<ul> <li>JHPCE 3.0 Orientation Slides -- SLURM scheduler &amp; Rocky 9</li> <li>C-SUB JHPCE Orientation Slides 2023</li> </ul>","tags":["in-progress","jeffrey"]},{"location":"help/external/#obsolete","title":"Obsolete","text":"<p>JHPCE 2.0 Orientation Slides -- 2023 -- Sun Grid Engine</p>","tags":["in-progress","jeffrey"]},{"location":"help/faq-old/","title":"FAQ","text":"<p>There is a dedicated SLURM FAQ document.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#why-does-bash-report-that-it-cant-find-the-module-command","title":"Why does bash report that it can\u2019t find the module command?","text":"Click to expand answer <p>If you receive a message like</p> <pre><code>bash: module: command not found\n</code></pre> <p>The module is a shell function that is declared in <code>/etc/bashrc</code>. It is always a good idea for <code>/etc/bashrc</code> to be sourced immediately in you <code>~/.bashrc</code>.  Edit your <code>.bashrc</code> file so that the first thing it does is o execute the system bashrc file, i.e. your <code>.bashrc</code> file should start with the following lines:</p> <pre><code>if [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#my-script-is-giving-odd-error-messages-about-r-or-m","title":"My script is giving odd error messages about <code>\\r</code> or <code>^M</code>.","text":"Click to expand answer <p>Windows and Unix use different characters to indicate a new line.  If you have uploaded your script from a Windows machine, it may have the Windows newline characters.  These need to be replaced by the Unix newline characters.  To do this, you can run the \u201cdos2unix\u201d command on your script <code>dos2unix myscript.sh</code>. This will strip out all of the Windows newlines and replace them with the Unix newlines.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#what-is-the-safe-desktop","title":"What is the SAFE desktop?","text":"Click to expand answer <p>The SAFE desktop is a virtual Windows computer that you can use to run scientific software and access JHPCE. For more information see this item.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#ssh-bad-owner-or-permissions","title":"SSH \"Bad owner or permissions\"","text":"Click to expand answer <p>If you receive a message like \"Bad owner or permissions on ~/.ssh/config\" or continue to have to provide your password when ssh'ing to JHPCE when you think you have configured things to not need one, your file owner or permissions may be incorrect. See this document for answers.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#im-getting-x11-errors-when-using-rstudio-with-putty-and-xming","title":"I\u2019m getting X11 errors when using rstudio with Putty and Xming","text":"<p>Obsolete</p> <p>As of 20240220 vcxsrv is not installed on the cluster. This FAQ item needs to be reviewed and probably removed. JRT</p> <p>We\u2019ve had issues reported by users of Putty with Xming. One solution we\u2019ve found is to use the (vcxsrv))[https://sourceforge.net/projects/vcxsrv/] instead Xming newlines.  (This commmand is on all nodes.)</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#how-can-i-add-packages-into-emacs-on-the-cluster","title":"How can I add packages into emacs on the cluster?","text":"<p>Make sure to <code>module load emacs</code> to get a later version of emacs. Then one needs to edit their <code>.emacs</code> file in their home director to include the package repos     <pre><code>(require 'use-package)\n(add-to-list 'package-archives '(\"gnu\" . \"http://elpa.gnu.org/packages/\") t)\n(add-to-list 'package-archives '(\"melpa\" . \"http://melpa.org/packages/\") t)\n</code></pre>     After restaring emacs then one can do <code>M-x package-list-packages</code> and follow the GUI.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#xauth-error-messages-from-macos-sierra-when-using-x11-forwarding-in-ssh","title":"Xauth error messages from MacOS Sierra when using X11 forwarding in SSH","text":"<p>With the upgrade to MacOS Sierra, the \u201c-X\u201d option to ssh to enable X11 forwarding may not work.  If you receive the message: <code>untrusted X11 forwarding setup failed: xauth key data not generated</code> , you can resolve the issue by add the line <code>ForwardX11Trusted yes</code> to your <code>~/.ssh/config</code> file on your Mac. You may still see the warning: Warning: <code>No xauth data; using fake authentication data for X11 forwarding.</code> To eliminate this warning, add the line <code>XAuthLocation /usr/X11/bin/xauth</code> to your <code>~/.ssh/config</code> file on your Mac.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#when-running-sas-an-error-dialog-pops-up-about-remote-browser","title":"When running SAS, an error dialog pops up about Remote Browser","text":"<p>When running SAS, you may need to specify options to indicate which browser to use when displaying either help or graphical output. We recommend using the Chromium browser.</p> <p>See our SAS usage document about how to resolve this issue.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#im-on-a-mac-and-the-c-command-to-interrupt-an-ssh-session-isnt-working","title":"I\u2019m on a Mac, and the <code>~C</code> command to interrupt an ssh session isn\u2019t working","text":"<p>It used to, but I upgraded MacOS and now it does not work.*  Newer versions of MacOS have disabled by default the ability to send an SSH Escape with <code>~C</code> (~ Shift+C).  To reenable this, on you Mac, you need to set the <code>EnableEscapeCommandline</code> option.  You can do this by either running <code>ssh -o EnableEscapeCommandline=yes . . .</code> or by editing your <code>~/.ssh/config</code> file, and at the top of that file add the line:</p> <pre><code>EnableEscapeCommandline=yes\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#how-do-i-get-the-rstudio-program-to-work-on-the-cluster","title":"How do I get the Rstudio program to work on the cluster?","text":"<p>See our core R support document about this and other R usage.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#my-x11-forwarding-stops-working-after-20-minutes","title":"My X11 forwarding stops working after 20 minutes","text":"<p>This error comes from the <code>ForwardX11Timeout</code> variable, which is set by default to 20 minutes.  To avoid this issue, a larger timeout can be supplied on the command line to, say, 336 hours (2 weeks):</p> <pre><code>$ ssh -X username@jhpce01.jhsph.edu -o ForwardX11Timeout=336h\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#how-do-i-copy-a-large-directory-structure-from-one-place-to-another","title":"How do I copy a large directory structure from one place to another.","text":"<p>Please do not copy or move anything except a small set of files on the login nodes.</p> <p>As an example, to copy a directory tree from <code>/home/bst/bob/src</code> to <code>/dcs07/bob/dst</code>, first, create a cluster script, let\u2019s call it <code>copy-job</code>, that contains the line <code>rsync -avzh /home/bst/bob/src/ /dcs07/bob/dst/</code>. Next, submit this script as a batch job to the cluster. An example SLURM batch job can be found here.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#my-app-is-complaining-that-it-cant-find-a-shared-library-eg-libgfortranso1","title":"My app is complaining that it can\u2019t find a shared library, e.g. <code>libgfortran.so.1</code>","text":"<p>Nine times out of ten, the allegedly missing library is there. The problem is that your application is looking for the version of the library that is compatible with the old system software. It will not help to point your application to the new libraries. They are more than likely to be incompatible with the new system. The correct solution is to reinstall your software. If the problem persists after the reinstallation, then please contact us and we will install standard libraries that are actually missing.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#ssh-gave-a-scary-warning-remote-host-identification-has-changed","title":"ssh gave a scary warning: <code>REMOTE HOST IDENTIFICATION HAS CHANGED</code>","text":"<p>Go into the <code>~/.ssh</code> directory of your laptop/desktop and edit the known_hosts file.  Search for the line that starts with the host that you ssh\u2019d to. Delete that line (it is probably a long line that wraps). Then try again</p>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#why-arent-slurm-commands-or-r-or-matlab-or-available-to-my-cron-job","title":"Why aren\u2019t SLURM commands, or R, or matlab, or\u2026 available to my cron job?","text":"<p>Authoring note</p> <p>This info needs to be re-written for upgraded cluster. And moved to the SLURM FAQ. Should the <code>scrontab</code> command be mentioned?</p> <p><code>cron</code> jobs are not launched from a login shell, but the module commands and the JHPCE default environment is initialized automatically only when you log in. Consequently, in a cron job, you have to do the initialization yourself. Do this by wrapping your cron job in a bash script that initializes the module command and then loads the default modules. You bash shell script should start with the following lines:</p> <pre><code>#!/bin/bash\n\n# Source the global bashrc\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\nmodule load JHPCE_DEFAULT_ENV\n</code></pre>","tags":["needs-review","in-progress"]},{"location":"help/faq-old/#ive-deleted-files-but-my-quota-hasnt-changed","title":"I've deleted files but my quota hasn't changed","text":"<p>See this document.</p>","tags":["needs-review","in-progress"]},{"location":"help/faq/","title":"FAQ","text":""},{"location":"help/faq/#slurm-faq","title":"SLURM FAQ","text":"<p>Questions about using SLURM are covered in their own FAQ here.</p>"},{"location":"help/faq/#orientationcluster-basics","title":"Orientation/Cluster Basics","text":"<p>You can access our latest orientation slides at This Link (C-SUB users go here instead (pdf).)  We update these periodically. The version date can be found on the first page in the lower right corner.</p>"},{"location":"help/faq/#compatibility","title":"Compatibility","text":"<ul> <li>My script is giving odd error messages about <code>\\r</code> or <code>^M</code>.</li> </ul> Click to expand answer <p>Windows and Unix use different characters to indicate a new line.  If you have uploaded your script from a Windows machine, it may have the Windows newline characters.  These need to be replaced by the Unix newline characters.  To do this, you can run the \u201cdos2unix\u201d command on your script <code>dos2unix myscript.sh</code>. This will strip out all of the Windows newlines and replace them with the Unix newlines.</p> <ul> <li>My app is complaining that it can\u2019t find a shared library, e.g. libgfortran.so.1</li> </ul> Click to expand answer <p>Nine times out of ten, the allegedly missing library is there. The problem is that your application is looking for the version of the library that is compatible with the old system software. It will not help to point your application to the new libraries. They are more than likely to be incompatible with the new system. The correct solution is to reinstall your software. If the problem persists after the reinstallation, then please contact us and we will install standard libraries that are actually missing.</p>"},{"location":"help/faq/#file-transfer","title":"File Transfer","text":"<ul> <li>I cannot connect with SFTP in MobaXterm or other GUI program</li> </ul> Click to expand answer <p>Because we use multifactor authentication (MFA), you need to configure your GUI program to either expect a prompt for a one time password (OTP) or be able to provide public key information. In MobaXterm, when configuring an SFTP session, you need to check the \"2-steps authentication\" box under the Advanced Settings tab.</p> <ul> <li>How do I copy a large directory structure from one place to another? Within the cluster? Into the cluster? Out of the cluster?</li> </ul> Click to expand answer <p>We have a document about transferring data into or out of the cluster here. We have a document with advice about copying data around within the cluster here. (This also includes good tips for using the rsync program for any data copying.)</p> <p>As an example, to copy a directory tree from /users/bob/src to /dcs08/bob/dst, first, create a cluster script, let's call it \"copy-job\", that contains the line <pre><code>#!/bin/bash\n\nrsync -avzh /users/bob/src /dcs08/bob/dst\n</code></pre> Next, submit a batch job to the cluster <pre><code>sbatch --mail-type=FAIL,END --mail-user=bob@jhu.edu copy-job\n</code></pre> This will submit the \"copy-id\" script to the cluster, which will run the job on one of the computer nodes, and send an email when it finishes.</p> <p>Warning</p> <p>Please do not copy or move any larger files on the login nodes. Use the transfer node for internal/external transfers and compute nodes for transfers between cluster storage locations.</p>"},{"location":"help/faq/#login","title":"Login","text":"<ul> <li>SSH gave a warning: REMOTE HOST IDENTIFICATION HAS CHANGED</li> </ul> Click to expand answer <p>Go into the ~/.ssh directory of your laptop/desktop and edit the known_hosts file. Search for the line that starts with the host that you ssh\u2019d to. Delete that line (it is probably a long line that wraps). Then try again</p> <ul> <li>SSH \"Bad owner or permissions\"</li> </ul> Click to expand answer <p>If you receive a message like \"Bad owner or permissions on ~/.ssh/config\" or continue to have to provide your password when ssh'ing to JHPCE when you think you have configured things to not need one, your file owner or permissions may be incorrect. See this document for answers.</p> <ul> <li>How do I delete saved passwords in MobaXterm?</li> </ul> Click to expand answer <p>When using MobaXterm you should not save your password when prompted to do so. MobaXterm will save your password, and then inadvisedly try to use that as your \u201cVerification Code:\u201d, which means that when you first connect to the cluster in MobaXterm, you are prompted for \u201cPassword:\u201d, and which you will need to press to be prompted for \u201cVerification Code:\u201d If you accidentally saved your password, you can remove the saved password by the following steps.  </p> <p>1) In MobaXterm, go to \"Settings-&gt;Configuration\" </p> <p>2) On the next screen, select \"MobaXterm Password Management\" </p> <p>3) This will display a list of saved passwords, and you should delete all of the entries that reference \u201cjhpce\u201d </p> <p>Once these entries are deleted, you should be prompted for \u201cVerification Code:\u201d when you connect to the cluster via MobaXterm.</p>"},{"location":"help/faq/#modules","title":"Modules","text":"<ul> <li>Why does bash report that it can\u2019t find the module command?</li> </ul> Click to expand answer <p>If you receive a message like</p> <pre><code>bash: module: command not found\n</code></pre> <p>The module is a shell function that is declared in <code>/etc/bashrc</code>. It is always a good idea for <code>/etc/bashrc</code> to be sourced immediately in you <code>~/.bashrc</code>.  Edit your <code>.bashrc</code> file so that the first thing it does is o execute the system bashrc file, i.e. your <code>.bashrc</code> file should start with the following lines:</p> <pre><code>if [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n</code></pre>"},{"location":"help/faq/#openssl","title":"OpenSSL","text":"<ul> <li>OpenSSL version mismatch error (e.g. Built against 30000070, you have 30100050)</li> </ul> Click to expand answer <p>Please unload the conda_R module before doing your <code>git clone</code> or other ssh-related command. That module contains its own OpenSSL libraries which conflict with the system-provided one.</p>"},{"location":"help/faq/#resources","title":"Resources","text":"<ul> <li>What is the SAFE desktop?</li> </ul> Click to expand answer <p>The SAFE desktop is a virtual Windows computer that you can use to run scientific software and access JHPCE. For more information see this item.</p>"},{"location":"help/faq/#rstudio","title":"RStudio","text":"<ul> <li>Why do I see a mass of error messages about \"GL\" and \"GPU\" every time I launch RStudio?</li> </ul> Click to expand answer <p>Those errors are normal. RStudio expects to be run on a computer with an attached display driven by a local graphics processing card and supported by a set of graphics software libraries and device drivers. We don\u2019t have such cards in our rack-mounted systems, nor the software that goes with them.</p>"},{"location":"help/faq/#sas","title":"SAS","text":"<p>We have a SAS usage document. Please read it for useful tips.</p> <ul> <li>When running SAS, an error dialog pops up about Remote Browser</li> </ul> Click to expand answer <p>When running SAS, you may need to specify options to indicate which browser to use when displaying either help or graphical output. We recommend using the Chromium browser.</p> <p>See our SAS usage document about how to resolve this issue.</p> <ul> <li>When starting SAS in an interactive session with browser support enabled, a stream of error messages appear</li> </ul> Click to expand answer <p>These can be ignored because the web browser wants to run on a local system with a graphics card, and the our compute nodes don't have such cards or the software to support them.</p> <p>See our SAS usage document about redirecting output to resolve this issue.</p> <p>Without redirecting standard output and standard error you will see messages similar to ones below: <pre><code>[2970887:2970887:1024/152818.092311:ERROR:chrome_browser_cloud_management_controller.cc(163)] Cloud management controller initialization aborted as CBCM is not enabled.\n    [2970887:2971086:1024/152818.161869:ERROR:login_database.cc(922)] Password store database is too new, kCurrentVersionNumber=35, GetCompatibleVersionNumber=39\n    [2970887:2971087:1024/152818.164247:ERROR:login_database.cc(922)] Password store database is too new, kCurrentVersionNumber=35, GetCompatibleVersionNumber=39\n    [2970887:2971086:1024/152818.167534:ERROR:login_database_async_helper.cc(59)] Could not create/open login database.\n    [2970887:2971087:1024/152818.170351:ERROR:login_database_async_helper.cc(59)] Could not create/open login database.\n    [2970887:2970887:1024/152818.626429:ERROR:object_proxy.cc(590)] Failed to call method: org.freedesktop.portal.Settings.Read: object_path= /org/freedesktop/portal/desktop: org.freedesktop.portal.Error.NotFound: Requested setting not found\nlibGL error: No matching fbConfigs or visuals found\nlibGL error: failed to load driver: swrast\n</code></pre></p>"},{"location":"help/faq/#slurm","title":"Slurm","text":"<ul> <li>There is a dedicated SLURM FAQ document</li> <li>Commands related to SLURM can be found in this document </li> </ul>"},{"location":"help/faq/#ssh","title":"SSH","text":"<ul> <li> <p>For a variety of questions and information about ssh - please see our ssh document.</p> </li> <li> <p>I\u2019m on a Mac, and the ~C command to interrupt an ssh session isn\u2019t working</p> </li> </ul> Click to expand answer <p>New versions of MacOS have disabled the ability to send an SSH Escape with <code>~C</code> (~ Shift+C). To reenable this, on you Mac, you need to set the <code>EnableEscapeCommandline</code> option. You can do this by either running <code>ssh -o EnableEscapeCommandline=yes . . .</code> or by editing your <code>~/.ssh/config</code> file, and at the top of that file add the line: <pre><code>EnableEscapeCommandline=yes\n</code></pre></p> <ul> <li>SSH \"Bad owner or permissions\"</li> </ul> Click to expand answer <p>If you receive a message like \"Bad owner or permissions on ~/.ssh/config\" or continue to have to provide your password when ssh'ing to JHPCE when you think you have configured things to not need one, your file owner or permissions may be incorrect. See this document for answers.</p>"},{"location":"help/faq/#storage","title":"Storage","text":"<ul> <li>How much space does a directory use? How much is available?</li> </ul> <p>See this page for information about evaluating disk space consumption.</p> <ul> <li>I've deleted files but I'm still restricted by disk quota</li> </ul> <p>See this document.</p>"},{"location":"help/faq/#x11","title":"X11","text":"<ul> <li>There is a dedicated document about X11 and troubleshooting connections here</li> <li>X11 Forwarding and Authentication - See this excellent article from Teleport (pdf) for quite a lot of good information and diagrams!!! (Saved as a PDF from https://goteleport.com/blog/x11-forwarding/)</li> <li> <p>If you are using MobaXterm and NOT using its Sessions to do SSH or SFTP, STOP using the built-in terminal window and START using an SSH or SFTP Session. The MobaXterm terminal window doesn't work reliably. See the MobaXterm document for instructions.</p> </li> <li> <p>My X11 forwarding stops working after 20 minutes </p> </li> </ul> Click to expand answer <p>This error comes from the <code>ForwardX11Timeout</code> variable, which is set by default to 20 minutes on some versions of MacOS.  To avoid this issue, you can specify a value of 0 which completely disables the timeout. Or you can set a larger timeout to, say, 336 hours (2 weeks). This value can be supplied by changing your SSH configuration (see our SSH document) or for each connection on the command line like this:</p> <p><pre><code>$ ssh -X username@jhpce01.jhsph.edu -o ForwardX11Timeout=336h\n</code></pre> Alternatively, you can add the line ForwardX11Timeout=0 to your ~/.ssh/config file <pre><code>$ head -1 .ssh/config\nForwardX11Timeout=0\n</code></pre></p> <ul> <li>Xauth error messages from MacOS Sierra when using X11 forwarding in SSH</li> </ul> Click to expand answer <p>With the upgrade to MacOS Sierra, the \u201c-X\u201d option to ssh to enable X11 forwarding may not work. If you receive the message: untrusted X11 forwarding setup failed: xauth key data not generated , you can resolve the issue by add the line ForwardX11Trusted yes to your ~/.ssh/config file on your Mac. You may still see the warning: Warning: No xauth data; using fake authentication data for X11 forwarding. To eliminate this warning, add the line XAuthLocation /usr/X11/bin/xauth to your ~/.ssh/config file on your Mac.</p>"},{"location":"help/faq/#2-factor-authentication","title":"2 Factor Authentication","text":"<ul> <li>My winscp program stopped working!</li> </ul> Click to expand answer <p>With 2 Factor Authentication, you need to include you username in the definition for your session</p> <ul> <li>The Authy Chrome plugin works on one machine but not on another</li> </ul> Click to expand answer <p>Check that the time is set correctly on your machine. The six-digit One Time Passwords (OTP) are time-based and are only valid for 30 seconds. So if the time is off on your machine it will not generate the correct OTP for the 30 second interval.</p> <ul> <li>I use CyberDuck to transfer files from my mac to the cluster. Now it doesn't work anymore.</li> </ul> Click to expand answer <p>We have had several people that have still had issues with Cyberduck even after applying the fixes below. We recommend using  Filezilla  instead.  You can also try to update your cyberDuck to the latest version.  Please make sure that your password is not saved in the Cyberduck connection.</p> <ul> <li>I use Filezilla to transfer files from my mac to the cluster. Now it doesn't work</li> </ul> \"Click to expand answer\"     Update your Filezilla to the latest version.  In Filezilla, create a new Site:     <ol> <li>Enter the \"Hostname\" (eg. jhpce-transfer01.jhsph.edu)</li> <li>Set the \"Protocol\" to \"SFTP\"</li> <li>Set the \"Logon Type\" to \"Interactive\"</li> <li>Set the \"User\" to your JHPCE UserID. Do not touch the \"Password\" and \"Account\" fields.</li> <li>Click \"Connect\". You will be prompted for your \"Verification Code\", which is the 6 digit number from Google Authenticator, and then \"Password\", which is your JHPCE Password.</li> </ol>"},{"location":"help/faq/#sun-grid-engine","title":"Sun Grid Engine","text":"<p>We transitioned from the Sun Grid Engine to SLURM in 2024. We wrote this document for users familiar with SGE who needed to learn some SLURM basics.</p>"},{"location":"help/general-advice/","title":"General Tips/Requests","text":"","tags":["needs-major-revision"]},{"location":"help/general-advice/#the-cluster-is-a-shared-resource","title":"The cluster is a shared resource","text":"<p>Generally expected knowledge + Take care not to use too many resources on the login nodes. Anything CPU, RAM, or input/output intensive should be done on compute nodes rather than one of the login nodes. + Anything more than a quick <code>ls</code> including: copying large files, recursively changing permissions, creating or extracting tar or zip archives, running a <code>find</code> should be done in a session on a compute node. + Data transfers of files larger about 1GB should be done through <code>jhpce-transfer01.jhsph.edu</code> rather than a login node. + Try to avoid having directories with more than 100 files in them.  + Try to avoid storing programs and scripts in data directories like <code>DCL*</code>. + Most storage on the cluster is raided but not backed up.  + We do back up home directories and a few other select directories on the DCS and DCL systems for groups that have requested backups.  We do have additional backup storage capacity available for a small fee. + Make use of your 1 TB of fastscratch storage for IO intensive job + Please remember that DCS and DCL stand for \u201cDirt Cheap Storage\u201d and   \u201cDirt Cheap Lustre\u201d, and were designed with cost-effectiveness as a   primary driving factor over performance. + Sharing data can be done in several ways on the cluster: i.) traditional Unix file permissions and groups and ii.) Access Control Lists (ACLs). + Sharing files with external collaborators can be done via Globus.</p>","tags":["needs-major-revision"]},{"location":"help/general-advice/#best-practices-passwords-and-authentication","title":"Best practices: passwords and authentication","text":"<ul> <li>Do not share your password with ANYONE. You are responsible for the use of your account.</li> <li>Choose a \"strong\" password.</li> <li>Use the \"kpasswd\" command to choose a new password. It requires your new password to have three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</li> <li>It would be best if your password was unique and not the same     password you use on other systems.</li> <li>If you believe your password or your computer have been compromised     please alert us via email to bitsupport@lists.jh.edu. Then reset your password using a different device. Visit     https://jhpce-app02.jhsph.edu/\u00a0 to reset your password.\u00a0 This web     site is only available on campus, so if you are outside of the     school network, you will need login to the JHU VPN first. You will     log into that page with your JHED ID and password.</li> <li>Hopkins staff will *NEVER* send you an email message asking for     your password or login credentials</li> <li>NEVER give out your password and login ID to anyone in an email     message or on a web page.</li> </ul> <p>If you have never used a Linux or Unix system before, we strongly     recommend going through the Unix Command     Line     tutorial. The cluster is entirely Linux based. Our Orientation is     about using the cluster, not using Linux. This tutorial should     only take 30 minutes or so to go through.</p>","tags":["needs-major-revision"]},{"location":"help/general-advice/#new-c-sub-user-orientation","title":"New C-SUB user orientation","text":"","tags":["needs-major-revision"]},{"location":"help/general-advice/#what-to-do-before-the-c-sub-jhpce-orientation-session","title":"What to do BEFORE the C-SUB JHPCE Orientation Session","text":"<p>There is a lot of material for us to cover and you to absorb. It is vital for your success that you complete a number of steps PRIOR to attending the\u00a0Orientation Session for the CMS Subcluster of the JHPCE (pronounced by its letters J-H-P-C-E) cluster.</p> <ul> <li> <p>Download a copy of the slides from the Orientation     from: [latest-csub-orient.pdf.] (https://jhpce.jhu.edu/orient/images/latest-csub-orient.pdf) </p> </li> <li> <p>In order to access the JHPCE cluster and make use of the     applications on the cluster, you may need to install additional     software on your smart phone and laptop.</p> </li> <li> <p>Install the 2 Factor Authentication program. \u00a0The JHPCE cluster     makes use of \"Google Authenticator\" to provide enhanced security.     \u00a0You can choose to either install an app on your smartphone or, if     you do not have an Apple or Android based smart phone, you can     install an extension to the Google Chrome browser.\u00a0 Prior to the     Orientation Session, you will only need to download the     GoogleAuthenticator app on your smart phone, or install the Authy     Chrome extension. We will be configuring Google Authenticator     during the Orientation Session. </p> </li> <li>Install required client software.\u00a0 You may need to install a     couple of programs on your laptop or desktop in order to access the     JHPCE Cluster.\u00a0 You will need 1) an SSH client for logging in, 2) an     SFTP client for transferring files to and from the cluster, and 3)     an X11 client for displaying graphics back from the JHPCE cluster.     \u00a0The SSH client is a requirement -- the SFTP and X11 clients are     preferable but optional.<ul> <li>Microsoft Windows We have found that the easiest program to use for accessing the     JHPCE cluster is MobaXterm as it combines the functionality of     all 3 software packages (SSH, SFTP, and X11) in 1 program.\u00a0     Install MobaXterm by following the first few steps of     https://jhpce.jhu.edu/access/mobaxterm.md     .\u00a0 Alternatively, if you already use an SSH client, (such as     putty     or Cygwin) and an SCP client\u00a0 (such as     WinSCP),     you can continue using that software.</li> <li>Apple Macintosh There are built in command line tools for ssh and scp that     can be run from a Terminal window. The Terminal program can be     found in \"Applications -&gt; Utilities\". From a Terminal window,     you would type:\\     <code>ssh &lt;username&gt;@jhpcecms01.jhsph.edu</code>and then login with     the login id and the password we provided to you.- In order to     run graphical programs on the cluster and have them displayed on     your Mac, you will need to install XQuartz from     http://xquartz.macosforge.org/landing/.- Optionally, you can     also install a GUI based SFTP program such as     \"Filezilla\". One note about     Filezilla -- if you download the package from the default link     on SourceForge, you may be be blocked by your MalWare/Virus     Scanner, or prompted to install Potentially Unwanted Programs     (PUPs) during installation.\u00a0 We recommend you follow the     alternative download link     here to     download a clean copy of the program.</li> </ul> </li> </ul>","tags":["needs-major-revision"]},{"location":"help/glossary-manual/","title":"Glossary","text":"<p>Terms you might wonder about. If you don't find what you're looking for, please use Google. Italicized terms indicate that there is a glossary entry for that item.</p> Absolute path A path which starts with a forward slash. Your shell (such as bash) interprets this location as starting with the root of the file systems on a computer. See also: relative path ACE Access Control Entry. A component of an ACL. See our ACL document. ACL Access Control List. A collection of ACEs associated with a file or directory. Used to extend the normal UNIX permission scheme. See our ACL document. bash An example of a shell, which are programs which accept and process commands entered via text with a certain syntax, or set of rules. (Wikipedia page) CLI Command Line Interface - a program like bash which provides a means to enter commands via text strings and see responses. (Wikipedia page.) Cluster A collection of computers (aka \"nodes\") managed as a group to provide services. Usually uses a job scheduler. Compute node Computers within a cluster used to execute jobs Computer You're using one right now. Aren't they marvelous? Core A component of a CPU which processes threads of program instructions. Cores each have their own collections of components which are needed to execute instructions, such as registers for mathematical operations, memory stores to cache information. CPUs can contain many cores. Cores can have support for hyperthreading. (Wikipedia page. SLURM support for cores/threads.) CPU Acronym for Central Processing Unit. The main processor in a computer. Contains one or more cores. There may be more than one CPU within a computer. See information about the CPUs on your UNIX computer with the command <code>less /proc/cpuinfo</code> (press q to quit). (Wikipedia page) Current working directory Expansion of acronym CWD. Your current (aka present) location within a file system. Can be expressed as a path. The command <code>pwd</code> will return the path to your CWD. EMACS A super-complex but powerful and VERY extensible text editor written in Lisp, an abomination of a computer language. I called it \"Eight Megabytes And Constantly Swapping\" back in the day when a megabyte of RAM cost thousands and thousands of dollars. Like Doctor Who, proponents can be tiresome to others. See also: vi File system A data storage service which allows for the storage and retrieval of information. File systems are basic units of storage. File system services include holding metadata about files, such as when they were created and who owns them. The command <code>df -h .</code> will return information about the file system of your current working directory. (Wikipedia page) GUI Acronym for Graphic User Interface. Antonym of CLI. A means of interacting with a user via windows. Operating systems such as macOS and Windows rely on GUI for its ease-of-use. X11 is a protocol which provides for the display of GUI windows over networks. Some of the windows in a GUI might be terminals offering CLI programs, such as bash shells.  Hyperthreading Modern CPU cores normally have some additional circuitry which store information used by (normally two) threads. This allows the core to easily switch its attention between two threads so work can continue if one of the threads needs to wait for information to be brought into the core from RAM. (Wikipedia page) Job A unit of work given to an operating system by a scheduler on behalf of a user Job scheduler Application which controls the execution of interactive or batch jobs on a cluster. SLURM is a job scheduler. (Wikipedia page) Linux A flavor of UNIX. Don't let them convince you that they invented it. (They will try. It's sad, really.) locality The proximity of data stored in memory to the processing circuitry. You want to keep data as close as possible when creating jobs. Memory General term for anything that can store information. You have a memory. Computers use many types of memory. People often mean RAM when they say memory. Sometimes they mean hard drive space when they say memory, which is understandable but confusing to those trying to help them. Say \"disk space\" or \"disk storage\" instead. Thanks. Memory hierachy A hierarchy of types of memory used by a computer. Each type has different speed of access, capacity, and cost. Fast memory is expensive and usually provides for limited capacity. Each is therefore usually used for certain purposes. A register inside of a core inside of a CPU is the fastest memory, but small. Next is one or more caches inside of a core. Next (cheapest, fastest, most capacious) is RAM. After RAM comes hard drives on the same computer. After hard drives on the local computer comes hard drives on file servers which have to pass their information across the network. Next come USB sticks, then floppy disks, then humans typing into keyboards. The closer to a register you can keep the information needed for your program, the faster it can be processed. This distance from the processor is called locality. Pane (Associate with window) A subdivision within a window where information is displayed, perhaps alone or perhaps alongside other panes Path A string of characters used to identify a location in a directory structure inside of a file system. That location represents either a file or directory. Examples: <code>./my-script</code> or <code>/dcs07/a-groups-data/some/place/down/deep</code> See also: absolute path, current working directory, relative path. Node Any computer within a cluster Nodes can be grouped by the services they provide, such as offering computation or file services. Usually used to indicate a compute node. RAM Acronym for Read Access Memory. Part of a memory hierarchy. A form of computer memory that can be read or written in any order. Contrast with data stored on a tape. You cannot read any data from the tape without moving the tape to reach the portion where your desired information is stored. Some of us had to use data on tapes. We were lucky we didn't have to use data stored on punchcards. RAM is fast and capacious. Most RAM used in cluster nodes is volatile -- it cannot store information without continuous electrical power. (Wikipedia page) Relative path A path which does not start with a forward slash. Your shell (such as bash) interprets this location starting with your current working directory. See also: absolute path Scheduler A component of a job scheduler application which decides which jobs to start running, when, and on which cores on which CPUs on which nodes. (Operating systems also contain schedulers, which do the same things for any process or thread, whether or not they are part of a job.) (SLURM CPU allocation process described here. SLURM has multiple other documents in the Slurm Scheduling section of this page.) SSH Acronym for Secure SHell. Technically a protocol for multiple network services. Usually meant to refer to a program which implements that protocol to provide a CLI. See our SSH document. (Wikipedia page) Thread The smallest sequence of programmed instructions that can be managed independently. Part of a process (which may have one or more threads). It takes time for the information needed by a thread to run to be loaded into a CPU core. So there is an expense (delay) to switching between threads on the same core. Hyperthreading attempts to reduce that cost. (Wikipedia page. SLURM support for cores/threads.) UNIX A family of operating systems initially brought to you by Bell Telephone, a monopoly in the US. UNIX has fundamentally always used a CLI because, well, GUIs didn't exist for many years after UNIX got started. UNIX has a steep learning curve, but has only grown in use over the decades because it is powerful and extensible. If you're struggling to learn UNIX, remember that many people had to learn UNIX without the Internet or Google. Modern macOS and Windows (since Windows NT) rely on UNIX code. Linux is not UNIX. Linux is a flavor of UNIX. Anyone who tells you otherwise is a poser. (Wikipedia page) User You Vi The best text editor. Anyone who tells you otherwise is uninformed. Window (Associate with pane) One or more panes displayed to a user by a software application."},{"location":"help/good-query/","title":"Helpful Hints For Asking Questions or Reporting Problems","text":"<p>This page describes how to write a good help request email.</p>"},{"location":"help/good-query/#a-good-request","title":"A Good Request","text":"<ol> <li> <p>Write a descriptive summary.</p> <ul> <li>Put in a short summary into the Subject.</li> <li>Which cluster are you on? We support two -- JHPCE and C-SUB. We assume JHPCE but specify \"C-SUB\" if that is where you are working.</li> </ul> </li> <li> <p>Expand on this in a first paragraph. Try to answer the following questions:</p> <ul> <li>Please give us your user name on the cluster.</li> <li>What are you trying to achieve?</li> <li>When did the problem start?</li> <li>Did it work before or is this the first time you are trying to do this?</li> <li>Which steps did you attempt to achieve this?</li> </ul> </li> <li>Additional advice:<ul> <li>Put enough details in the details section.</li> <li>Send us screenshot images of what you did -- often we can notice little details you might not have thought to mention.</li> <li>Please give us the exact commands you type into your console.</li> <li>What are the symptoms/is the error message</li> <li>Have you tried to google for the error message?</li> <li>Never put your password into the ticket.</li> <li>In the case that you handle person-related data of patients/study participants, never write any of this information into the ticket or subsequent email.</li> </ul> </li> </ol>"},{"location":"help/good-query/#specific-questions-for-common-issues","title":"Specific questions for common issues","text":""},{"location":"help/good-query/#problems-connecting-to-the-cluster","title":"Problems Connecting to the Cluster","text":"<ul> <li>From which machine/IP do you try to connect (ifconfig on Linux/Mac, ipconfig on Windows)?</li> <li>Did it work before?</li> <li>What is your cluster user name? This is NOT your JHED ID.</li> <li>Please send us the output of <code>ssh-add -l</code> and add <code>-vvv</code> to the SSH command that fails for you.</li> <li>What is the response of the server?</li> </ul>"},{"location":"help/good-query/#problems-submitting-jobs","title":"Problems Submitting Jobs","text":"<ul> <li>Please give us the directory that you ran things in.</li> <li>Please send us the submission script that you have problems with.</li> <li>If the job was submitted, Slurm will give you a job ID. We will need this ID.</li> <li>Please send us the output of scontrol show job  or sacct --long -j  of your job."},{"location":"help/help-basics/","title":"Help","text":""},{"location":"help/help-basics/#ways-to-seek-help-and-support","title":"Ways to Seek Help and Support","text":"Search this website: There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Search/Email for \"bithelp\" advice: <code>bithelp@lists.johnshopkins.edu</code>. This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users, and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. You should check to see if your question has been asked and answered before in the list archives. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff vi \"bitsupport\": <code>bitsupport@lists.johnshopkins.edu</code>. This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty. You should check to see if your question has been asked and answered before in the list archives. Please don't hesitate to reach out to us if you have any questions or concerns. <p>Tip</p> <p>When contacting us, please provide enough information so we can promptly diagnose the problem or answer your question.</p> <p>Here is a page describing the information we need.</p>"},{"location":"help/list-archives/","title":"Using the bithelp &amp; bitsupport email list archives","text":"<p>You can find the answers to questions that people have asked before in the archived list contents. </p> <p>Those archives are stored and presented in a format oriented at individual months of traffic. So to look into the past you have to do a few extra steps.</p> <p>Please note that the \"https://lists.jh.edu\" web site is only available on the JHU campus networks, or when connected via JHU VPN.</p>"},{"location":"help/list-archives/#direct-links-to-the-list-archives","title":"Direct links to the list archives:","text":"<ul> <li>https://lists.jh.edu/sympa/arc/bithelp</li> <li>https://lists.jh.edu/sympa/arc/bitsupport</li> </ul>"},{"location":"help/list-archives/#creating-your-search-query","title":"Creating Your Search Query","text":"<ol> <li>Log into https://lists.jhu.edu with your JHED</li> <li>Search for the list \u201cbithelp\u201d or \"bitsupport\" or go to the list archive directly with the links above.</li> <li>If you know that you want to search in only a single specific month, you can click on that month if it has a blue month number. </li> <li>Otherwise, click on the \u201cAdvanced search\u201d button</li> <li>Select additional months of material beyond the current one by clicking on them in the \u201cextended search term\u201d field at the bottom center. You can add them one at a time by clicking on them or use normal item selection techniques for your operating system (e.g. select multiple months by holding down the Shift key while clicking or all of them using the Cmd+A key combination on a Mac).</li> <li>Enter your search term in the search field.</li> <li>Make any other search adjustments (e.g. case-sensitive, which terms need to be found, etc)</li> <li>Hit Enter</li> </ol>"},{"location":"joinus/","title":"Index","text":"","tags":["topic-overview"]},{"location":"joinus/#overview-for-new-pis-and-users","title":"Overview for new PIs and users","text":"<p>The JHPCE cluster is a Linux based HPC environment, and is designed for running loosely coupled parallelizable jobs, or programs needing large amounts of  CPUs, RAM, or disk space.  We have a number of genomics and statistical tools installed on the cluster, but most of the analysis is done using programs  written in languages like R, python, Stata, or SAS.</p> <p>The JHPCE Cluster operates as a non-profit service center run out of the  Biostats department of the Bloomberg JHU Bloomberg School of Public Health. As  such we do charge fairly nominal fees for compute and storage, and all users on the cluster need to have a sponsoring PI that will fund their usage.</p>","tags":["topic-overview"]},{"location":"joinus/#information-for-new-sponsoring-pis","title":"Information for New Sponsoring PIs","text":"<p>As mentioned above, we do operate as a non-profit JHU Service Center, and do charge for compute and storage usage on the cluster.  </p> Compute Charges Our fees for compute time are roughly 1 penny per hour for a job using 1 core  and 5GB of RAM.  Costs scale linearly with time and cpu+mem usage, so a job  running for 24 hours that used 8 cores and 40GB of RAM would cost about $2.00.  Storage Charges    Costs for storage are broken into home directory storage and project storage space.   1. Home directory Storage: All users are given a personal home directory with a 100GB quota.  For home directory space, we charge $0.45 per GB per year, so this cost would max out at $45 for a year if a user used their entire 100GB of space.  2. Project Storage: If you need several TB of space for storing large amounts of data, you can purchase an allocation on one of our large  storage arrays.  Every 12 months or so we purchase a new large storage array  for the JHPCE cluster, and sell allocations on that array. The cost for an  allocation will be based on the actual cost of the storage, but has been  decreasing over time.  Our latest storage build worked out to be about $30  per TB per year.  There is typically a 10TB minimum buy-in for new storage  purchases.  <p>We have more information about becoming a Sponsoring PI as both a Stakeholder and Non-Stakeholder at New PI Page If this sounds like a good fit for your lab, then the first step in accessing the cluster would be for you or someone on your team to complete the  New Project form.  This will help us gather the relevant contact and financial information for your lab.</p>","tags":["topic-overview"]},{"location":"joinus/#information-for-new-users","title":"Information for New Users","text":"<p>Once the New Project form is completed, the Project will be added as a Sponsoring Organization on the New Users form at which point the members of your team that will be performing the analysis can sign up for a user account.  Once the New User form is completed, an email will be sent to the requestor with some introductory information about the cluster, as well as a link to sign up for an upcoming JHPCE Orientation Session.</p>","tags":["topic-overview"]},{"location":"joinus/#orientation","title":"Orientation","text":"<p>All new users are required to attend one of our JHPCE Orientation Sessions, during which their account will be set up on the cluster.  The Orientation is typically held every other Wednesday afternoon, and lasts for about 2 hours.  Prior to attending the orientation, you should review the What to do before attending the orientation session page.  The slides for the orientation can be found here</p>","tags":["topic-overview"]},{"location":"joinus/hipaa/","title":"HIPAA","text":""},{"location":"joinus/hipaa/#basics","title":"Basics","text":"<p>HIPAA has two relevant standards that must be satisfied.  The HIPAA Security Rule Standards and the HIPAA Privacy Rule Standards. The JHPCE adheres to the HIPAA Security Rule Standards. Adhering to the HIPAA Privacy standards is the responsibility of the PI.  BSPH is not part of the Johns Hopkins covered entities (As of 1/26/06). Therefore, insofar as data on the JHPCE is concerned, the PI is responsible for ensuring that his or her research is conducted in compliance with HIPAA Privacy Rule Standards. Fortunately, this is not difficult. Formally, PIs are restricted to either deidentified datasets or to limited datasets (see links below for details). Examples of limited data sets include dbGaP datasets (assuming a data use agreement is in place) and deidentified insurance claim data. The latter can include dates such as admission, discharge, service, DOB, DOD; and location information such as city, state, five digit or more zip code; and ages in years, months or days or hours.</p>"},{"location":"joinus/hipaa/#using-phi-or-pii-on-jhpce","title":"Using PHI or PII on JHPCE","text":"<p>The JHPCE cluster may be able to meet the requirements specified by the data provider for the handling of PHI data.  There may be additional steps that the data analysts will need to go through in order to access the data, but this is typically a matter of running a few extra steps to access the data.  If you have a dataset with sensitive data, please reach out to us at bitsupport@jhu.edu and we can review the data handling requirements to assess whether the JHPCE cluster can meet them.</p>"},{"location":"joinus/hipaa/#csub","title":"CSUB","text":"<p>The JHPCE maintains a small sub-cluster for handling CMS Medicare and Medicaid claims data. This sub-cluster has a number of additional security features in place to ensure the security of this more sensitive data. If you are interested in accessing the CMS data, please email support@harp-csub.freshdesk.com for more information. We have additional information in our CSUB Section.</p>"},{"location":"joinus/hipaa/#links","title":"Links","text":"<p>The following links from the JHPSH IRB, \u00a0SOM IRB and the US Department of Health &amp; Human Services (HHS) provide details on how to de-identify your data so that you are in compliance with the HIPAA Privacy Rule Standards.</p> <ul> <li>JHSPH IRB HIPAA page</li> <li>JHMI IRB definition of de-identified data</li> <li>JHMI definition of \u201cLimited data set\u201d</li> <li>HHS Guidance Regarding Methods for De-identification of PHI </li> </ul>"},{"location":"joinus/leaving/","title":"Leaving JHPCE as a User or PI","text":"<p>So, you're leaving JHU.</p>"},{"location":"joinus/leaving/#departing-studentsresearchers-or-pis-of-departing-studentsresearchers","title":"Departing Students/Researchers or PIs of Departing Students/Researchers","text":"<p>If you are a student or researcher that is either leaving JHU or will no longer be using the JHPCE cluster, please be sure to have your sponsoring PI reach out to us. We will work with your PI to close out your account on the JHPCE cluster and either delete your data from the cluster, or migrate it to another location. If you have data on the cluster that you are authorized to keep, please be sure to transfer the data to a location outside of the cluster so that you can access it once you account is closed. We have some tips on transferring data to and from the cluster here</p> <p>We can keep your account open for a short time after you leave JHU if your PI agrees to continue to sponsor your usage. Please have your PI reach out to us at bitsupport@lists.jhu.edu so that we can arrange your ongoing access to the cluster.</p>"},{"location":"joinus/leaving/#departing-pisdepartments","title":"Departing PIs/Departments","text":"<p>If you are a sponsoring PI and you are leaving JHU, or will no longer be using the JHPCE cluster, please reach out to us at bitsupport@lists.jhu.edu so that we can coordinate closing out your project. There will need to be a number of decisions made about your account and data.</p> <ul> <li>Any users under your project will that will continue to use the JHPCE cluster will need to be moved under another PI's project. We will need an \"OK\" from the adopting PI in order to move anyone under their Project.</li> <li>All users that are not picked up by another sponsoring PI will have their accounts closed and home directories deleted.</li> <li>If you have data stored in a larger DCL or DCS storage allocation, the allocation will need to be either moved under another project, or deleted. As with users, you will need to identify a new PI/Project that will assume ownership of your storage allocation.</li> <li>Please be sure to transfer any data you will need to retain to a location outside of the cluster so that you can access it once you account is closed. </li> </ul>"},{"location":"joinus/new-pi-form/","title":"New PI Form","text":"<p>For information about becoming a Sponsoring PI, please see Our PI Information Page</p> Loading\u2026"},{"location":"joinus/new-pi/","title":"Are you a Primary Investigator wanting to join JHPCE?","text":"<p>This page is for new PIs who are interested in joining the JHPCE as well as for PIs who have previously registered with us but need to create a new project, collaboration or organization.</p>"},{"location":"joinus/new-pi/#stakeholder-vs-non-stakeholder-pis","title":"Stakeholder vs. Non-Stakeholder PIs","text":"<p>As a PI, you can join as either a Stakeholder or Non-Stakeholder. We define Stakeholder PIs as those that have contributed computing resources to the  facility.  Non-Stakeholder PIs are those that sponsor the computing for their lab or department, but do not purchase computing resources for the cluster. Most PIs will start as Non-Stakeholders, but will move up to stakeholders when they either acquire computing resources they want to contribute to the cluster, or purchase computing resources for the cluster either through a grant or through their normal budgeting cycle. </p> <p>As a Stakeholder, you agree to share any unused capacity on your compute nodes with the broader JHPCE community by placing them on the \"shared\" partition or queue on the cluster. For non-Stakeholders, the only access to computing resources is via those shared by the Stakeholders.  This model has historically worked very well for the JHPCE community.  While there is no guarantee of computing resources to Non-Stakeholders, there is generally sufficient capacity on the \"shared\" partition to meet the needs of the users sponsored by Non-Stakeholders. </p> <p>This model is often referred to as a \u201ccommon pool resource\u201d in the sense formalized by E. Ostrom and others. Users affiliated with stakeholders have  the right of priority access to the resources that they \u201cown\u201d. All users, whether they are members of a stakeholding organization or not, are otherwise treated identically. Excess computing capacity is made available to the entire community via a low priority \u201cshared queue\u201d. The shared partition also provides surge capacity to stakeholders in addition to providing access to HPC resources to non-stakeholders.</p> <p>The JHPCE service center will charge a monthly management fee to the Stakeholder PI that has purchased computing nodes for the cluster.  This fee is roughly $500 per month for a compute node (which works out to $0.01/hr for a node with 64 cores and 512GB of RAM), and covers the charges for space, power, and cooling, as well as the salaries of those supporting the equipment.  These Stakeholder fees though are defrayed to the extent that their capacity is used by others. So if the users sponsored by a Stakeholder PI do not use their compute nodes for a month, that monthly fee is paid for entirely by its usage on the \"shared\" partition.</p> <p>In this way the system has been able to meet the needs of both the stakeholders with the greatest HPC consumption, and the needs of the smaller users in the long tail of HPC consumption (where the bulk of the science is performed). All of this is accomplished with light-weight polices and light-weight organizational infrastructure (&lt; 3FTEs) , which is a major reasons that we are able to keep our costs low.</p>"},{"location":"joinus/new-pi/#how-to-register","title":"How to register","text":"<p>If you are a Principal Investigator registering a new project or organization, please fill out this Form. If you have never registered a project and budget number with the JHPCE, we request that you contact the director of the JHPCE to arrange a 1/2 hour orientation (either in person or via telephone).</p>"},{"location":"joinus/new-pi/#becoming-a-stakeholder","title":"Becoming a stakeholder","text":"<p>There are many benefits to becoming a Stakeholder in the JHPCE cluster.</p> <p>As a Stakeholder, you will be given a dedicated partition on the node that you purchase for the cluster.  The partition will only be accessible by members of your group, thus avoiding delays when the shared partition becomes busy. We also allow you to pull your node off the the shared partition if you need dedicated access to your computing resources for a time.  Bear in mind though that while the node is off of the shared partition, you will be paying the entire management fee for the node.</p> <p>If you are interested in purchasing nodes to add to the JHPCE cluster to become a Stakeholder, please email the JHPCE Director at jhpce@jhu.edu. We can help you with sizing an appropriate solution for your needs, and coordinate with vendors to get quotes.</p>"},{"location":"joinus/new-users-form/","title":"New user form","text":"Loading\u2026"},{"location":"joinus/policies/","title":"Policies","text":""},{"location":"joinus/policies/#1-introduction-to-the-commons","title":"1. Introduction to the commons","text":"<p>The JHPCE service center operates a High Performance Computing (HPC) facility as a Common Pool Resource (CPR) Hierarchy, with rights to specific resources based on stakeholder ownership of specific resources. \u00a0A formal computational model of the HPC resources provides a starting point for the self governance of the the commons, first by defining a hierarchy of resources that are governed locally by individual stakeholders, second, by distributing common and resource-specific expenses throughout the hierarchy, third by calculating fees in proportion to actual measured consumption of specific resources and fourth, by providing a data based method that incentivizes sharing of excess resource capacity. \u00a0CPR principles have provided a framework that has helped us address issues of overuse, congestion, and sustainability. The strategy has harnesses the intellectual and grant writing skills of the entire community, for the benefit of the entire community, while enabling the growth of an HPC facility that is up-to-date and very matched to the needs of its PIs. This contrasts with traditional governance models that rely on central planning, F&amp;A and a handful of rainmakers. Our experience is that the CPR model benefits not just the largest and best funded stakeholders, but also the much larger numbers of smaller stakeholders and non-stakeholders who make up the tail of the HPC consumption distribution \u2013 where the bulk of the science is produced.</p>"},{"location":"joinus/policies/#2-computing-resources","title":"2. Computing Resources","text":""},{"location":"joinus/policies/#21-background","title":"2.1 Background","text":"<p>All user are permitted to submit jobs to the shared queue. The shared queue provides low-priority access to unused capacity throughout the cluster. Capacity on the shared queue is provided on a strictly \u201cas-available\u201d basis and serves two purposes. First it provides\u00a0surge capacity to stakeholders who temporarily need more compute capacity than they own, and second, it gives provides non-stakeholders access to computing capacity. Scheduling polices attempt to harvest unused capacity as efficiently as possible while mitigating the impact on the rights of stakeholders to use their resources. The service center does not guarantee that stakeholders will provide sufficient excess capacity to meet non-stakeholders needs, however in practice the cluster is rarely operating at full capacity so there is usually ample computing capacity on the shared queue. The JHPCE service center provides the following computing services and capabilities to all PIs and their users:</p>"},{"location":"joinus/policies/#22-computing-privileges-and-services-afforded-to-all-pis","title":"2.2 Computing privileges and services afforded to all PIs","text":"<ol> <li>Low-priority access to cluster-wide excess computing capacity via the shared queue.</li> <li>Detailed quarterly reports of monthly usage and charges, stratified user.</li> <li>System-level support from the System Administrators</li> <li>Access to a download queue that is used for file transfer to and from the facility</li> <li>Access to a low-priority \u201cinteractive queue\u201d for small tasks</li> </ol> <p>2.3 Computing privileges and services afforded to compute-system stakeholders</p> <p>The JHPCE service center is organized to promote the research agendas of its stakeholders. We provide the following computing services and capabilities to stakeholders who purchase compute nodes:</p> <ol> <li>Professional system administration and support of stakeholder-owned compute nodes</li> <li>Management of service contracts, warranties and interaction with vendors (e.g. repair tickets).</li> <li>System-level support from the system administrators</li> <li>Priority access to owned compute nodes via individual stakeholder queues.</li> <li>A stable maintenance fee that is assessed on each compute node on a quarterly basis.</li> <li>Discounts on maintenance fees in exchange for providing excess compute capacity to\u00a0other users.</li> <li>Access to cluster-wide excess computing capacity via the shared queue.</li> <li>Detailed quarterly reports of monthly usage and charges, stratified by queue, host and user.</li> <li>Freedom to remove owned nodes from the cluster at any time.</li> </ol>"},{"location":"joinus/policies/#3-storage-resources","title":"3. Storage Resources","text":""},{"location":"joinus/policies/#31-background","title":"3.1 Background","text":"<p>There are numerous storage spaces available for long term storage of data on the JHPCE cluster.  There are spaces available for all users of the cluster, and also provide that opportunity to buy into a large storage purchase Every 12 - 18 months.</p>"},{"location":"joinus/policies/#32-storage-privileges-and-services-afforded-to-all-pis","title":"3.2 Storage privileges and services afforded to all PIs","text":"<p>All users have home directory space (currently a 100 GB quota). The charge for storage on home directories is proportional to actual usage. Backup of home directories is mandatory and is charged separately.</p>"},{"location":"joinus/policies/#33-storage-privileges-and-services-afforded-to-storage-system-stakeholders","title":"3.3 Storage privileges and services afforded to storage system stakeholders","text":"<p>The bulk of JHPCE storage is dedicated to specific research projects and is owned by stakeholders. Currently the service center has essentially no storage to hand out to users. To meet the continually growing storage requirements, we periodically build large, shared, low-cost, low-power storage arrays, and provide all PIs the opportunity to purchase space on these arrays. The JHPCE provides the following services/capabilities to stakeholders who have purchased a share on a storage device:</p> <ol> <li>A dedicated, fixed allocation on shared devices that stakeholders can manage as they see fit.</li> <li>Professional system administration and support of stakeholder-owned storage</li> <li>Management of service contracts, warranties and interaction with vendors (e.g. repair tickets).</li> <li>Support from the system administrator, including configuration of top-level directories,\u00a0quotas and root-level operations, e.g. on ZFS devices stakeholders can request compressed\u00a0shares, thereby doubling their usable storage.</li> <li>Stakeholders may share their space with collaborators, either for free or in exchange for sharing\u00a0the maintenance charge.</li> <li>Once a storage allocation is purchased, stakeholders maintain access to their space by paying a\u00a0stable quarterly charge that covers operating expenses and amortization of capital costs.</li> <li>Buy-in and operating charges are calculated in proportion to the total capital costs and the total\u00a0operating expenses of the storage device.</li> <li>Due to the fact that ownership of a storage device is shared, it is not possible for a stakeholder\u00a0to remove their share of a device.</li> <li>We expect storage devices to have a 5 year lifetime. Beyond that we may discontinue operation\u00a0of the device. The industry standard is to include 3 year service and support contracts in the\u00a0purchase cost of storage devices. Thus years 4 and 5 may incur higher operating charges due to\u00a0the addition of year 4-5 service contracts.</li> </ol>"},{"location":"joinus/policies/#4-cost-recovery-principles","title":"4. Cost Recovery Principles","text":""},{"location":"joinus/policies/#41-the-jhpce-facility-is-a-common-pool-resource-hierarchy","title":"4.1 The JHPCE facility is a Common Pool Resource Hierarchy","text":"<p>The facilities in the JHPCE are managed as a Common Pool Resource (CPR) hierarchy. The amount of cost that we recover from each resource in the hierarchy is determined by assigning expenses to individual resources in the CPR via a rigorous graph-theoretic time-dependent model of the entire CPR. Expenses are distributed throughout the hierarchy to determine the costs that must be assigned to specific resources, e.g. specific nodes or specific storage partitions.</p>"},{"location":"joinus/policies/#42-compliance-with-omb-regulations","title":"4.2 Compliance with OMB regulations","text":"<p>OMB regulations require that service centers do not make a profit or loss. Year-end deficits or surpluses must be carried forward to the next fiscal year. OMB regulations do not require that rates be estimated and fixed at the beginning of each fiscal year. Instead the requirement is that either rates be fixed periodically OR a well defined systematic charge-back methodology be applied uniformly to all users. We employ the latter strategy. Expense sharing and rate calculations are performed with a software tool that creates a hierarchical model of the HPC facility and incorporates institutional subsidies, expenses and depreciation schedules to calculate the operating costs assigned to individual resources. The software tool then combines the operating costs with the usage data from system logs to calculate rates and chargebacks for individual resources and users. The JHPCE bills quarterly. To guarantee that the service center exactly recovers all it\u2019s quarterly expenses, the JHPCE uses retrospectively calculated rates which are based on actual quarterly expenses and actual quarterly usage (see discussions below). This differs from the conventional approach of setting rates at the beginning of each fiscal year. To appreciate the advantages of this approach we first consider the disadvantages of the conventional approach.</p>"},{"location":"joinus/policies/#43-difficulties-associated-with-the-conventional-approach-to-setting-rates","title":"4.3 Difficulties associated with the conventional approach to setting rates","text":"<p>With the conventional approach, budgets are typically reconciled once per year to: 1) account for the deficit/surplus that has accumulated over the previous year and 2) account for changes in the services and configuration of service center and 3) guess the projected resource usage. By construction, the resulting rates lag the realities of the service center, i.e. the actual expenses, resource usage and configuration. This lag leads to the buildup of deficits (and occasionally surpluses). The resulting uncertainty causes organizational inertia that resists change and innovation. PIs demand and deserve predictable costs. They ask for the current \u201crate schedule\u201d. Unfortunately, setting fixed rates one year ahead actually leads to more variability than rates that are adjusted quarterly in response to \u201crealities on the ground\u201d. With the conventional approach, rates change (by a difficult to predict amount) at the beginning of each fiscal year. There is an annual jump in rates of unknown sign and magnitude. This is not a formula for a financially stable service center or for the predictability that PIs need.</p>"},{"location":"joinus/policies/#44-allocation-and-smoothing-of-expenses-and-subsidies","title":"4.4 Allocation and smoothing of expenses and subsidies","text":"<p>Expenses are spread over time. Capital expenses are depreciated across 3-5 year intervals as determined by their asset class. Non-capital expenses are spread across 12 months, even if the interval spans two fiscal years. Thus the impact of large expenses is spread across individual fiscal years. Expenses are allocated to the appropriate sub-hierarchy of the resource hierarchy according to the part of the resource hierarchy that incurs the expense. This is performed with the aid of a formal graphical model of the facility. For example, expenses related to storage would be assigned to the storage-related sub-hierarchy. Thus the storage expense would not directly impact compute-related rates. In general, this allows expenses to be allocated to the subset of users who actually need and use particular resources. As resources are added and removed from the JHPCE facility, in response to waxing and waning research initiative and grant funding of particular stakeholders and users, the costs associated with those changes are shared with the appropriate subset of the JHPCE community. Finally, subsidies are applied at the level of the hierarchy that is required by the donor. For example, institutional and departmental subsidies are typically for staff salaries, and are therefore applied at the staff salary level of the hierarchy.</p>"},{"location":"joinus/policies/#45-retrospective-rate-calculations","title":"4.5 Retrospective rate calculations","text":"<p>Rates in the JHPCE are calculated retrospectively at the end of every quarter and are based on that quarter\u2019s actual configuration, actual expenses, actual institutional support, and actual usage. While the precise rates are not known until the end of the quarter, 7 years of experience has shown that rates have been stable from quarter-to-quarter with smooth and gradual changes. The only exception has been when major system upgrades have occurred. To PIs that are initially uncomfortable with retrospective rates, we point out that charges are the product of two terms: rates and usage. The latter is the dominant source of uncertainty in budget estimation. Our system allows PIs to use historical rates from recent quarters to estimate their computing budgets in grant applications. By smoothing and appropriate allocation of expenses, we are able to create highly stable rates and to substantially reduce quarterly fluctuations to only a few percent. The final advantage of the retrospective system is that since the software reconciles the budget on a quarterly basis, the service center knows on a quarterly basis, exactly where it stands financially. This is the recipe for a successful service center.</p>"},{"location":"joinus/policies/#45-computing-charges-from-rates-and-usage","title":"4.5 Computing charges from rates and usage","text":"<p>Charges to individual users and stakeholders are calculated strictly in proportion to their share of usage of particular resources. Users with dedicated (i.e. unshared) resources are charged based on 100% usage (i.e. independently of actual usage). Thus stakeholders who purchase a compute node (or a fixed allocation in a shared storage device) will see a fixed charge (subject to aforementioned quarterly fluctuations). In addition, stakeholders who share their resources (e.g. by sharing excess compute cycles) see reduced cost in proportion to the amount of usage that is shared with others.</p>"},{"location":"joinus/policies/#5-current-rate-examples-spring-2024","title":"5 Current rate examples (Spring 2024)","text":"Computing queue rate shared $0.0015/GB-hour sas $0.0224/GB-hour Storage device rate rate home $0.0159/GB-month $190.99/TB-year dcl02 (Lustre allocation) $0.0021/GB-month $25.20/TB-year dcs04 (ZFS allocation) $0.0019/GB-month $23.000/TB-year dcs07 (ZFS allocation) $0.0016/GB-month $19.50/TB-year <p>Allocations may be purchased on custom devices whenever we build them. Allocations are paid off over a 5 year amortization schedule, where the cost includes the hardware costs for the storage plus the quarterly JHPCE Managed Storage charge.</p>"},{"location":"joinus/policies/#51-historical-rate-examples-summer-2021","title":"5.1 Historical rate examples (Summer 2021)ComputingStorage","text":"queue rate shared $0.0024/GB-hour sas $0.0160/GB-hour device rate rate home $0.0243/GB-month $291.99/TB-year dcs01 (ZFS allocation) $0.0031/GB-month $37.90/TB-year dcl01 (Lustre allocation) $0.0025/GB-month $29.60/TB-year"},{"location":"joinus/policies/#52-historical-rate-example-spring-2015","title":"5.2 Historical rate example (Spring 2015)ComputingStorage","text":"queue rate shared $0.0017/GB-hour sas $0.0139/GB-hour device rate rate home $0.0631/GB-month $757.65/TB-year dcs01 (ZFS allocation) $0.0055/GB-month $66.38/TB-year dcl01 (Lustre allocation) $0.0049/GB-month $59.39/TB-year"},{"location":"orient/orientation-overview/","title":"About orientation","text":"<p>All new cluster users are required to attend a JHPCE orientation session. These sessions are generally held every other Wednesday afternoon via Zoom.</p> <p>Before signing up for a sesssion, please be sure that you have completed the New User Form and that your PI has approved your account. During the signup process, you will be able to select a time for your orientation session.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#documentation-used-in-orientation","title":"Documentation used in orientation","text":"<p>You can download the orientation slides at This Link</p> <p>We transitioned from the Sun Grid Engine to SLURM in 2024. We wrote this document for users familiar with SGE who needed to learn some SLURM basics.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#what-to-do-before-the-jhpce-orientation-session","title":"What to do before the JHPCE Orientation Session","text":"<p>Prior to attending the Orientation Session for the JHPCE cluster, you may need to install some additional applications on you laptop or smart phone.</p> <p>Please install this software prior to attending the JHPCE cluster orientation session.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#install-the-2-factor-authentication-program","title":"Install the 2 Factor Authentication program","text":"<p>The JHPCE cluster makes use of \u201cGoogle Authenticator\u201d to provide enhanced security. \u00a0You can choose to either install an app on your smartphone or, if you do not have an Apple or Android based smart phone, you can install an extension to the Google Chrome browser.\u00a0 Prior to the Orientation Session, you will only need to download the Google Authenticator app on your smart phone, or install the Authy Chrome extension. We will be configuring Google Authenticator during the Orientation Session.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#install-required-ssh-client-software","title":"Install required SSH client software","text":"<p>You may need to install a couple of programs on your laptop or desktop in order to access the JHPCE Cluster. You will need:</p> <ol> <li>an SSH client for logging in,</li> <li>an SFTP client for transferring files to and from the cluster, and</li> <li>an X11 server for displaying graphics back from the JHPCE cluster.</li> </ol> <p>The SSH client is a requirement; the SFTP and X11 clients are optional. For more information about SSH, see our page here</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#microsoft-windows-users","title":"Microsoft Windows Users","text":"<p>We have found that the easiest program to use for accessing the JHPCE cluster is MobaXterm as it combines the functionality of all 3 software packages (SSH, SFTP, and X11) in 1 program.\u00a0 Prior to the Orientation session, you should install MobaXterm by following the first few steps of https://jhpce.jhu.edu/access/mobaxterm/ .\u00a0 Alternatively, if you already use an SSH client, (such as putty or Cygwin) and an SCP client\u00a0 (such as WinSCP), you can continue using that software.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#apple-macintosh-users","title":"Apple Macintosh Users","text":"<ul> <li>SSH Client - There are built in command line tools for ssh and scp that can be run from a Terminal window.\u00a0 The Terminal program can be found in \u201cApplications -&gt; Utilities\u201d.\u00a0 From a Terminal window, you would type: <code>ssh &lt;username&gt;@jhpce01.jhsph.edu</code> and then login with the login id and the password we provided to you.</li> <li>Graphical Programs, X11 server - In order to run graphical programs on the cluster and have them displayed on your Mac, you will need to install XQuartz from http://xquartz.macosforge.org/landing/.</li> <li>File Transfer - MacOS has a built in <code>sftp</code> text-based program for doing file transfers to and from the JHPCE cluster. Optionally, you can also install a GUI based SFTP program such as Cyberduck</li> </ul>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#the-jhpce-cluster-is-linux-based","title":"The JHPCE cluster is Linux Based","text":"<p>If you have never used a Linux or Unix system before, we strongly recommend going through the Unix Command Line tutorial offered at the Digital Ocean site at\u00a0https://www.digitalocean.com/community/tutorials/a-linux-command-line-primer\u00a0. The cluster is entirely Linux based, so some exposure to the Linux command line environment is recommended before attending the Orientation Session. The tutorial should only take 30 minutes or so to go through.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#best-practices-passwords-and-authentication","title":"Best practices - passwords and authentication","text":"<p>Do not share your password with ANYONE. Choose a \u201cgood\u201d password using special characters and letters and digits. It would be best if your password was unique and not the same password you use on other systems. If you believe your password or your computer have been compromised please let us know immediately so we can reset your password. If you store passwords on your computer (not recommended), please let us know immediately if your computer is lost or stolen.</p>","tags":["topic-overview","in-progress"]},{"location":"orient/orientation-overview/#finally","title":"Finally","text":"<p>Hopkins staff will NEVER send you an email message asking for your password or login credentials NEVER give out your password and login ID to anyone in an email message or on a web page.</p>","tags":["topic-overview","in-progress"]},{"location":"ourtools/features/","title":"WEB SITE TOOLS/ENABLED FEATURES WORTH KNOWING HOW TO USE","text":"<p>Let's create a visually appealing web site using some of these features!! Use a WYSIWYG editor like MacDown and \"mkdocs serve\" to quickly edit.</p> <p>Material for MkDocs is packed with many great features that make technical writing a joyful activity.</p> <p>Please consult the Tips and Conventions page for authoring guidance and tips.</p> <p>Jeffrey likes:</p> <ul> <li>admonitions -- a lot!!</li> <li>highlighting text</li> <li>keyboard meta keys (like Ctrl)</li> <li>Details (collapsed blocks of text)</li> </ul> <p>Running the mkdocs package installed via python allows you to develop web pages on your local computer. See the recipe</p> <p>There is another document containing wishlist items that we might want to enable/configure.</p>"},{"location":"ourtools/features/#critical-material-for-mkdocs-reference-material","title":"Critical Material for MkDocs reference material","text":"<p>Look here for information about these and other features!!! Just keep in mind that ones marked \"insiders\" are not available for our use. Materials for MkDocs reference section.</p> <p>It isn't clear how much caution one should use in consulting MkDocs documents and people's solutions for it. JRT thinks that Material for MkDocs differs enough that one should definitely keep in mind whether your google search has turned up something about MkDocs.</p>"},{"location":"ourtools/features/#modifying-text-patterns-across-the-site","title":"Modifying text patterns across the site","text":"<p>Example of changing all markdown files in a copy of the git repo:</p> <pre><code>cd jhpce_mkdocs\nfind . -type f -name \\*.md -exec grep -il \"www.jhpce.jhu.edu\" {} \\;\nfind . -type f -name \\*.md -exec sed -i '' \"s/www.jhpce.jhu.edu/jhpce-old.jhu.edu/g\" {} \\;\nfind . -type f -name \\*.md -exec grep -il \"jhpce-old.jhu.edu\" {} \\; | xargs git add\n</code></pre>"},{"location":"ourtools/features/#diagramming-with-mermaid","title":"Diagramming with Mermaid","text":"<p>mkdocs.yml contains code enabling the use of a JavaScript tool called Mermaid. If you want your local <code>mkdocs serve</code> program to be able to display it, you need to <code>pip install mkdocs-mermaid2-plugin</code> (see recipe at bottom of this page)</p> <p>Material for MkDocs Diagrams documentation</p> <p>Diagram syntax from the mermaid people</p> <p>A live editor at the mer-people site!!! You can copy the resulting code to your buffer or save the image as (png,svg).</p> <pre><code>graph LR\n   A[Your computer] --&gt; B[Login nodes]\n   B[Login nodes] --&gt; C[(Compute nodes)]</code></pre> <p>Supported types: flowchart (aka graph), sequenceDiagram, stateDiagram-v2,classDiagram, erDiagram</p> <p>JRT finds these types interesting: timeline, user journey</p> <p>Numerous others, including pie, bar and line charts (xychart-beta)</p> Only some types officially supported by Material for MkDocs <p>Besides the diagram types listed above, Mermaid.js provides support for pie charts, gantt charts, user journeys, git graphs and requirement diagrams, all of which are not officially supported by Material for MkDocs. Those diagrams should still work as advertised by Mermaid.js, but we don't consider them a good choice, mostly as they don't work well on mobile. While all Mermaid.js features should work out-of-the-box, Material for MkDocs will currently only adjust the fonts and colors for flowcharts, sequence diagrams, class diagrams, state diagrams and entity relationship diagrams.</p>"},{"location":"ourtools/features/#details","title":"Details","text":"<p>Like an admonition but makes pages more readable by collapsing content. Documentation here and also explained in the detail below.</p> Psst: Click To Expand <p>You can have it be open by default, too. (Add a + after the opening ?+?+?). The following \"detail\" was set in the Markdown to display opened up.</p> Syntax to use <p>Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class keyword (like \"tip\" or \"warning\") or classes (separated with spaces) and the summary contained in double quotes. Content is placed below the header and must be indented with FOUR SPACES.</p> <p>Another detail can be nested inside by adding another blank line and another detail header line and content block. But this header line needs to start with the word \"multiple\" So ??? multiple class \"Title\"</p> <p>You can probably include code blocks inside of details like you can with admonitions.</p>"},{"location":"ourtools/features/#frontmatter-in-document-files","title":"Frontmatter (in document files)","text":"<p>Tags are the primary use of frontmatter I think we should use at this point. This may not be a complete list of directives that one can optionally add within a document. But the basics are that you can add to the top of the document a stanza to set the title of the document, a description of it, a status indicator such as new or deprecated. See this page for how to define an icon for the page.</p> <p>Apparently multiple frontmatter elements, like both tags and a page title, need to exist together inside one pair of three dashed lines. See example:</p> <pre><code>---\ntags:\n  - a tag\ntitle: some docs need explicit titles set b/c they can't be correctly guessed\n---\n</code></pre>"},{"location":"ourtools/features/#tags","title":"Tags","text":"<p>An example of frontmatter is the code to add tags to documents. 20240211 I tested adding a tag and it works. I also specified in the nav section a page for Material for MkDocs to automatically list tags and the pages they are found on.</p> <p>A lot of optional tag-related settings/capabilities seem to be reserved for paying sponsers as of 20240201. See this page. Can users search for documents by tags in the search field?</p> <p>The tags I envision using at the outset are shown below, so we can try to use them to figure out which pages need attention and possibly who is assigned to finish it.</p> <p>The tag \"needs-improvement-later\" could be added to a page which has \"done\" to indicate that what we have is able to be published but needs more work.</p> <p>The tag \"contains-refs-to-old-site\" came to me because some pages, such as the R page, contain screenshots which contain the URL of the old web site.</p> <p>The tag \"last-revised-YYYYMMDD\" could be used on a page which also has \"done\" or \"needs-improvement-later\" so you can tell that information by looking at the tags page. As opposed to having to go look at the repository. The wishlist document mentions adding a plugin or extension which allows the last-revised date to be automatically generated and listed at the bottom of each page.</p> <p>Place lines like this at the very top of the document, before the document title, to add the tags mentioned. Tags are strings but I am hoping to avoid spaces or underscores. (Underscores suck b/c they require the shift key. And you can't always see them depenind on how text renders.)</p> <p>Code blocks can be numbered or not by default (given settings in mkdocs.yml). THEY ARE NOW OFF BY DEFAULT. See this section for instructions on adding titles to code blocks and enabling or disabling line numbering.</p> <pre><code>---\ntags:\n  - done\n  - needs-review\n  - in-progress\n  - needs-improvement-later\n  - contains-refs-to-old-site\n  - last-revised-20240210\n  - jeffrey\n  - mark\n  - jiong\n  - adi\n  - brian\n---\n</code></pre>"},{"location":"ourtools/features/#internal-links","title":"Internal links","text":"<p>From this mkdocs page JRT learned that you can specify anchor points to document sections by knowing that they are converted to lowercase and white space is replaced by dashes. So this very section, named \"Internal links\" can be specified as a link to \"../ourtools/features.md#internal-links\" (in a different document) or \"#internal-links\" (inside the same document).</p>"},{"location":"ourtools/features/#working-with-image-files","title":"Working with image files","text":"<p>Key information for doing things like aligning, adding captions, ...</p> <p>https://squidfunk.github.io/mkdocs-material/reference/images/</p> <p>There is no center alignment. You can fake it by adding a caption to the image (see example in above page)</p> <p>Alight to the left or right like this<pre><code>![Image title](https://dummyimage.com/600x400/eee/aaa){ align=left }\n</code></pre> Hmmm, if Jeffrey tries to use that syntax then the rest of the text in this document is placed to the right of the image, even including the next section (\"Keyboard meta keys\"). How do you indicate a break between the text that you want wrapped on the right and the rest of the document?</p> <p>This first example places the image to the left (without any align option)</p> <p><pre><code>![Image title](../files/images/dir-stack-push-pop.png)\n</code></pre> </p> <p>This second example places the image to the left (without any align option), puts the text I want to the right of it (but no more text!!!)</p> <pre><code>![Image title](../files/images/dir-stack-push-pop.png) The text that you want to the right side of image.\n</code></pre> <p> The text that you want to the right side of image.</p> <p>This third example results in the whole rest of the document wrapped up to the right of the image. (So I didn't actually put the live code in the document).</p> <pre><code>![Image title](../files/images/dir-stack-push-pop.png){align=left} The text that you want to the right side of image.\n</code></pre>"},{"location":"ourtools/features/#symbols-for-keyboard-keys","title":"Symbols for keyboard keys","text":"<p>(enabled by the pymdownx.keys extension)</p> <p>You can create images of keyboard keys, such as meta keys like Ctrl or Cmd by surrounding a keyword such as \"ctrl\" with two plus characters</p> <p>+ + ctrl + + will create Ctrl</p> <p>This can be useful when trying to draw attention to important small characters like back ticks, exclamation marks or vertical pipe symbols. You can also use them as tags like Mail some-address@bob.edu or Help some-URL</p> <p>DO NOT USE ANY CAPITAL LETTERS or the symbol will not display, only the word you typed.  \"Ctrl\" will fail, \"ctrl\" will succeed.</p> <p>EXAMPLES: Menu Up [ ] Caps Lock Eject Fn F4</p>"},{"location":"ourtools/features/#key-sequences","title":"Key sequences","text":"<p>You can create a meta-key sequence by adding a single + between the keywords. (So you surround the sequence with two plus symbols but only use a single plus symbols in between the keywords.)</p> <p>Ctrl+Alt+Del is created with + + ctrl + alt + delete + +</p>"},{"location":"ourtools/features/#list-of-keywords","title":"List of keywords","text":"<p>Example keywords are: bar, pipe, enter, shift, dblquote, ctrl, esc, tab, space, del, arrow-up, single-quote, brace-left, bracket-right, cmd, windows.</p> <p>A backtick ` is called a grave, by the way. As in the French \"accent grave\" </p> <p>All of the symbols you could desire are listed in the following web page, (except for some reason the vertical \"pipe\" or \"bar\" symbol -- use + + pipe + + to create that):</p> <p>https://facelessuser.github.io/pymdown-extensions/extensions/keys/</p>"},{"location":"ourtools/features/#custom-keys","title":"CUSTOM \"KEYS\"","text":"<p>You can create a key (or key combo) with any wording you want by adding double quotes around your text, for example</p> <p>+ + \" Your Wording Here \" + + to create Your Wording Here</p> <p>This can be useful to make a series of steps users need to follow more clear. Do this first+Then do this second</p>"},{"location":"ourtools/features/#abbreviations","title":"Abbreviations","text":"<p>Abbreviations can be defined by using a syntax similar to URLs and footnotes, starting with an asterisk immediately followed by the term to be associated in square brackets.</p> <p>This code creates the sentence that follows. You can hover over \"HTML\" and see the definition appear.</p> <pre><code>The HTML specification is maintained by the W3C.\n*[HTML]: Hyper Text Markup Language\n</code></pre> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"ourtools/features/#glossary","title":"Glossary","text":"<p>There's a way to create a document which is automatically updated when people define abbrieviations. See the wishlist document for details. Currently we have a manually written glossary, because that was quicker, and allows for the explanation to be longer than many page authors would probably provide in their individual pages.</p>"},{"location":"ourtools/features/#definition-list","title":"Definition List","text":"<p>You can create an indented block of text using a colon followed by FOUR space characters.</p> <p>Example code and result: <pre><code>`a sample term to define`\n:    The definition you are seeking. (But not the droids.)\n</code></pre></p> <code>a sample term to define</code> The definition you are seeking. (But not the droids.)"},{"location":"ourtools/features/#admonitions","title":"Admonitions","text":"<p>These are sweet! We should use them frequently. But be aware that they are not rendered correctly in MacDown.app. This is where it is good to be running \"mkdocs serve\" on your local machine. So you can verify that they are properly constructed before you push up your modified markdown file.</p> <p>Consider whether or not you want the material collapsed or expanded when users first visit a page. If you want the information collapsed, create a \"detail\" instead of an \"admonition\" by using question marks instead of exclamation points. Details are documented above.</p> <p>Admonition documentation</p> <p>You add an admonition by</p> <ol> <li>starting a line with three explanation marks, a space, and a keyword (called a \"type qualifier\") such as note, danger, example, info, tip, warning. Here is a list. Certain colors are used for known keywords. If you use your own word or phrase, the color is maybe out of your control.</li> <li>You can add an optional title for your admonition by adding a double quoted string after the type qualifier.</li> <li>on the next line(s) start with FOUR spaces. You can have multiple paragraphs inside your admonition by continuing to indent the first line by FOUR SPACES.</li> </ol> <p>Note</p> <p>Some text using the \"note\" type qualifier</p> <p>Example</p> <p>admonitions allow setting off info inside colored boxes, e.g. note, tip, warning, danger, example. https://squidfunk.github.io/mkdocs-material/reference/admonitions/#usage</p> <p>All lines indented four spaces are included in your admonition, including fenced code blocks.</p> <p>Code block inside of an admonition</p> Show my failed jobs between noon and now<pre><code>sacct -s F -o \"user,jobid,state,nodelist,start,end,exitcode\" -S noon -E now\n</code></pre>"},{"location":"ourtools/features/#open-urls-in-new-tabs","title":"Open URLs in new tabs","text":"<p>(Adi has configured the server to always open URLs in new tabs.)</p> <p>JRT thinks there might be plugins which make this easier than what you have to do othwerwise, which is to use HTML instead of the Markdown notation. In normal HTML you add a space and a string to the end of the URL: <code>target=\"_blank\"</code></p> <pre><code>&lt;a href=\"https://squidfunk.github.io/mkdocs-material/reference/admonitions/\" target=\"_blank\"&gt;About admonitions&lt;/a&gt;\n</code></pre>"},{"location":"ourtools/features/#footnotes","title":"Footnotes","text":"<p>This<sup>1</sup> is a reference to the feature's description.</p> <p>You add a footnote by entering</p> <p><code>[^1]</code></p> <p>in the midst of your text. Anywhere in the document afterwards you create the footnote contents by</p> <ul> <li>after a blank line</li> <li>placing at the start of a line</li> <li>the corresponding numbered entry using the same syntax</li> <li>but adding a colon and a space character after the closing square brace, then</li> <li>adding your wording for the footnote.</li> </ul> <p><code>[^1]: Wording of footnote</code></p> <p>Numbering: It seems to be the case that you can use whatever number you want in your Markdown and Materials for MkDocs will generate the correct numbering in the resulting HTML files. In other words, you could give each footnote the number \"1\" Perhaps this depends on the ordering of the use of a footnote and its definition. (The only algorithm available to the processing is going to be to associate the first use with the first definition, the second use with the second definition, ...)</p>"},{"location":"ourtools/features/#data-tables","title":"Data tables","text":"<p>Tables are easily constructed out of vertical pipe symbols |, hyphens - and text. Optional colons can be used to align column contents.</p> <p>They don't render unless you also include the line of hyphens under the line containing the column titles.</p> <p>A simple table is created with these characters:</p> <p>| Column1 Title | Column2 Title |</p> <p>| ---------- | ---------- |</p> <p>| Contents C1R1 | Contents C2R1 |</p> <p>| Contents C2R1 | Contents C2R2 |</p> <p>Table Tips:</p> <ul> <li>Number of pipe symbols per line must match.</li> <li>Number of hyphens in the second line do not have to match any column width.</li> <li>Alignment is done by placing a colon to the left, right, or on both sides of the hyphens in your dividing line.<ul> <li>First column aligned left, second aligned right:  | :---------- | ----------: | </li> <li>First column no alignment, second aligned center:  | ---------- | :----------: | </li> </ul> </li> </ul>"},{"location":"ourtools/features/#sortable-tables","title":"Sortable tables","text":"<p>This is now implemented.</p> <p>https://squidfunk.github.io/mkdocs-material/reference/data-tables/#sortable-tables</p>"},{"location":"ourtools/features/#import-csv-or-excel-file","title":"Import CSV or Excel file","text":"<p>See https://timvink.github.io/mkdocs-table-reader-plugin/</p>"},{"location":"ourtools/features/#icons-and-emojois-examples-kinds-of-check-marks","title":"ICONS and EMOJOIS: Examples: Kinds of check marks","text":"<p>JRT found that these three kinds of check marks are examples of \"icons\" and \"emojis\".</p> Example Markdown text <code>:material-check:</code> <code>:material-close:</code> <code>:material-check-all:</code> <p>They would not render until I added these three lines to mkdocs.yml: <pre><code>  - attr_list\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n</code></pre></p> <p>See this page for more information and links to the \"icon sets\" as well as other pages that display a bazillion emojis: https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/</p> <p> works if you enter <code>:smile:</code></p> <p>STILL UNKNOWN - CAN ANYONE FIGURE IT OUT AND UPDATE THIS PAGE???</p> <p>But these other things didn't work. For some of them I downloaded an .svg file for them and placed them in both a top-level .icons/ directory and a docs/.icons/ directory. So read more if you want to be able to put all kinds of cool symbols in our pages. DO YOU NEED TO HAVE .ICONS DIRECTORIES AT ALL?</p> <p>:page-facing-up:</p> <p>:mdiDatabaseSettingsOutline:</p> <p>:mdiOrderAlphabeticalAscending:</p> <p>:fa-regular fa-envelope:</p>"},{"location":"ourtools/features/#fenced-code-blocks","title":"Fenced code blocks","text":"<p>Note that you can set off a block of text using three preceding and three following backtick characters. If you provide a valid keyword immediately after the backticks (no space char), then your code block will have a set of formatting choices applied to them. That keyword could be, for example, python.</p> <p>Code blocks can be located inside of details and admonitions and probably other things (because of settings added to mkdocs.yml)</p> <p>There are MANY options for code blocks. All kinds of syntaxes can be used to mean different things. Here is the main document for code blocks.</p>"},{"location":"ourtools/features/#code-block-line-numbers-highlighting","title":"code block line numbers &amp; highlighting","text":"<p>Code blocks can be numbered or not by default (given settings in mkdocs.yml). THEY ARE NOW OFF BY DEFAULT. See this section for instructions on adding titles to code blocks and enabling or disabling line numbering.</p> <p>If line numbers are enabled by default, you can disable them for a specific code block by adding after the beginning three back ticks a space, then a <code>linenums=\"0\"</code></p> <p>If line numbers are disasbled by default, you can enable them for a specific code block by adding after the beginning three back ticks a space, then a <code>linenums=\"N\"</code> where N is a (?positive?) integer other than zero.</p> <p>If you have multiple code blocks and want the line numbers to continue in second and later blocks, you can replace that \"N\" with a specific number.</p> <p>You can highlight specific line numbers within the block by adding after the beginning three back ticks a space, then a <code>hl_lines=\"2 3\"</code></p>"},{"location":"ourtools/features/#code-block-titles","title":"code block titles","text":"<p>Add a title by following leading 3 backticks with a space and <code>title=\".browserslistrc\"</code></p>"},{"location":"ourtools/features/#code-block-formatting-by-programming-language","title":"code block formatting by programming language","text":"<p>!!! note:     The language keywords are not the same as in github -- there is overlap but also differences. See this list for the  language keywords for the Pygments Python syntax highlighter used by Material for MkDocs.</p> <p>Text written before learning about the above:</p> <p>Like in Github you can specify a programming language keyword immediately following the leading three backticks to cause the text to be formatted in that language's notation. </p> <p>There is a directory full of examples you can simply click on.</p> <p>Useful keywords include Awk, checksums, DNS Zone, Jupyter Notebook, Python, R, Regular Expression, Rich Text Format, SAS, Shell, sed, SSH Config/filenames, stata, YAML</p> <p>A python example</p> <pre><code>def fn():\npass\n</code></pre>"},{"location":"ourtools/features/#highlighting-text","title":"Highlighting text","text":"<p>https://squidfunk.github.io/mkdocs-material/reference/code-blocks/</p> <p>https://squidfunk.github.io/mkdocs-material/setup/extensions/python-markdown-extensions/#highlight</p> <p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p> <p>I could not figure out how to quote plain text correctly so it would be displayed instead of rendered. Here I'm going to use the key symbols to defeat that challenge.</p> <p>Highlight a passage by starting with</p> <p>{ = =</p> <p>and end with </p> <p>= = }</p> <p>Delete text by starting with</p> <p>{ - - </p> <p>and ending with</p> <p>- - }</p> <p>Add Replacement text by starting with</p> <p>{ + +</p> <p>and ending with</p> <p>+ + }</p> <p>This can also be combined using tildes and greater-than characters. See the markdown source code onea single.</p> <p>and comments can be added inline by starting with</p> <p>{ &gt; &gt;</p> <p>and ending with</p> <p>&lt; &lt; }</p>"},{"location":"ourtools/features/#snippets-including-other-files","title":"Snippets: Including other files","text":"Required to enable this feature <p>Added stanza to mkdocs.yml: <pre><code>  - pymdownx.snippets: # adds the ability to embed content from arbitrary files into a document\n      check_paths: false # if true, if snippets cannot be found, the site cannot be rebuilt!!!\n      base_path: docs\n</code></pre></p> <p>When Snippets is enabled, HTML or Markdown content from other files (including source files) can be embedded by using the <code>--8&lt;--</code> (scissors) notation directly from within a code block to pull in a file via a relative path (starting inside of docs/, not relative to the document doing the including), in this case <code>ourtools/includes/sample-bashrc</code></p> <p>It can be extremely frustrating figuring out what path to use to make a snippet actually display. If you get it wrong in certain ways it will prevent the rebuilding of the whole site.</p> <p>sample bashrc<pre><code># System-wide .bashrc file for interactive bash(1) shells.\nif [ -z \"$PS1\" ]; then\n     return\nfi\n</code></pre> For more details about snippets, including ability to pull in only certain sections of other documents, or to append a file to every page in the web site, see https://facelessuser.github.io/pymdown-extensions/extensions/snippets/</p> <p>This warning says to not enable snippets for security reasons. I cannot discern the threat level to us in our usage. https://github.com/facelessuser/pymdown-extensions/security/advisories/GHSA-jh85-wwv9-24hv</p>"},{"location":"ourtools/features/#recipe-for-running-mkdocs-locally","title":"Recipe for Running Mkdocs Locally","text":"<p>As of 20240401 these steps are needed to build a local Material for MkDocs server that will run a browser at <code>http://127.0.0.1:8000/</code></p> <p>Warning</p> <p>Because our <code>mkdocs.yml</code> contains some certain material, JRT thinks, <code>mkdocs serve</code> spits out errors if it doesn't find Git supporting files/directories. But you're supposed to be able to create stand-alone web pages outside of Git so it would be nice to understand the interdependency.</p> <pre><code>cd ~/Documents/GitHub/\n\ngit clone https://github.com/jhpce-jhu/jhpce_mkdocs\nor you can use the ssh-key method:\ngit clone git@github.com:jhpce-jhu/jhpce_mkdocs\n\ncd jhpce_mkdocs\n\npip3 install mkdocs-material\npip3 install mkdocs-git-revision-date-localized-plugin\npip3 install mkdocs-open-in-new-tab\npip3 install mkdocs-mermaid2-plugin\npip3 install mkdocs-git-authors-plugin\npip3 install mkdocs-extract-listings-plugin\nmkdocs build\nmkdocs serve\n</code></pre>"},{"location":"ourtools/features/#errors-you-might-run-into-running-mkdocs-locally","title":"Errors You Might Run Into Running Mkdocs Locally","text":"<p>2025-04-09 - MM found insalling and running \"mkdocs serve\" using instructions above on Mac gave constant stream of errors: <pre><code>INFO    -  DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since\n           version 4.0.0.\n             File\n           \"/Users/mmill116/Library/Python/3.9/lib/python/site-packages/mkdocs_extract_listings_plugin/html_parser.py\",\n           line 18, in parse_listings_from_html\n               for pre in soup.findAll('pre'):\n             File \"/Users/mmill116/Library/Python/3.9/lib/python/site-packages/bs4/_deprecation.py\", line 56,\n           in alias\n               warnings.warn(\n</code></pre> The fix was to edit <code>..../Library/Python/3.9/lib/python/site-packages/mkdocs_extract_listings_plugin/html_parser.py</code> and change the single instance of \"findAll\" to \"find_all\".</p> <p>It can sometimes be useful to Ctrl+C the mkdocs serve program and restart it. Usually this involves significant changes to <code>mkdocs.yml</code> and those will stop over time. However when in doubt give it a try.</p> <p>JRT added some instructions to the mkdocs.yml file causing warnings to be issued. JRT has found them very useful. <pre><code># https://www.mkdocs.org/user-guide/configuration/#validation\nvalidation:\n  omitted_files: warn\n  absolute_links: warn\n  unrecognized_links: warn\n</code></pre></p> <p>The <code>mkdocs serve</code> program will spew out a number of warnings and error messages as you change files. Most of them are important but a few of them are going to recur and are harmless. For example warnings about files in the docs/ tree which are not mentioned in the nav bar. Such files are only going to grow in number. Perhaps there is a way to exclude known cases?</p> This error means you have an error in frontmatter YML code somewhere<pre><code>TypeError: '&lt;' not supported between instances of 'NoneType' and 'str'\nERROR   -  [14:46:28] An error happened during the rebuild. The server will appear\n           stuck until build errors are resolved.\n</code></pre>"},{"location":"ourtools/features/#macdown-wysiwyg-editor","title":"MacDown WYSIWYG Editor","text":"<p>Jeffrey has found that the free MacDown editor is VERY HELPFUL in authoring. It is a MarkDown editor for the Macintosh which displays the source code in one pane and the rendered document in another pane opposite.</p> <p>It renders most of Material for MkDocs material correctly, but not all. This is another reason why it is useful to run \"mkdocs serve\" locally, so you can see in a web browser the results of your edits.</p> <p>There are preferences that are worth enabling. This page lists the MarkDown elements which are enabled and disabled by default. This page discusses extended syntax and the site has other interesting MarkDown reference information.</p> <p>One handy feature: If you copy a web URL, highlight some text in your source code, then click on the link insert symbol in the toolbar, it will automatically paste in the URl as it inserts the square brackets and parentheses to create a URL.</p> <p>Does automatic pattern matching for syntax. Includes a command line program to use to open documents (Jeffrey configured his Mac using Get Info on an .md file to use MacDown by default so he can say in Terminal \"open file.md\" and it opens up the GUI). Supports a variety of themes for those who like dark mode.</p> <p>One flaw in this program is that sometimes the source code pane is blank when you open existing documents. The solution is to quit the program and launch it again. The fast workaround is to grow and shrink the document width and the dividing line between the source and rendered document panes.</p> <p>You can install it by download via the web or with Homebrew with <code>brew install --cask macdown</code> Stats on its page indicate some popularity.</p>"},{"location":"ourtools/features/#page-status","title":"Page Status","text":"<p>You can add icons to pages in two ways by modifying the frontmatter of a page.</p> <p>Basic documentation for this: https://github.com/squidfunk/mkdocs-material/blob/master/docs/reference/index.md</p> <p>These show up in the nav bar. Whereas page tags as we are currently using in 04/2004 do not. Would be nice to signal more clearly to users that something was under heavy construction (because they don't necessarily understand the import of the tags, so unless you put in an authoring note at the top...)  Tags are nice to have, too, because they create a sorted list of pages to visit via simple clicks.</p>"},{"location":"ourtools/features/#page-icons","title":"Page Icons","text":"<p>One is by defining an \"icon\"in the frontmatter for each document</p> <p>This appears to the left of the page title in the nav bar. I'd rather it be on the right!!!</p> <p>There are MANY interesting icons you can use.</p>"},{"location":"ourtools/features/#page-status_1","title":"Page Status","text":"<p>One is by defining a \"status\" in the frontmatter for each document But the only three icons available are LAME LAME LAME in both resolution and appeal</p> <p>You can define a new icon to go with a status, but how to do that in the css file isn't clear from the </p> <p>docs/stylesheets/extra.css</p> <p>This page goes the furthest to document exactly how to do this, although they used this to define the word BETA rather than using an icon. https://www.starfallprojects.co.uk/blog/material-custom-status/#enable-the-feature</p> <p>https://squidfunk.github.io/mkdocs-material/customization/#additional-css</p>"},{"location":"ourtools/features/#tabs","title":"Tabs","text":"<p>(These don't seem to render correctly in MacDown)</p> <p>Jeffrey doesn't quite understand where we would use these yet. But they're very neat. They make web pages more compact, because you can cover a multipart topic in the same amount of vertical space, and it's intuitive to click on what you need.</p> <p>tabs documentation</p> <p>An example idea is to divide up some information into Linux, Mac, and Windows sections.</p> <p>Tabs are created with the same syntax as an admonition or detail but use equal signs.</p> <p>There has to be a blank line before the beginning of (admonition, detail, tab)</p> <p>Everything has to be indented FOUR SPACES (not a tab char) inside of (admonition, detail, tab)</p> <p>You MUST enclose the title of the tab in double quotes even if it is a single word (it seems to Jeffrey)</p>"},{"location":"ourtools/features/#simple-example-of-tabs","title":"Simple example of tabs","text":"<p>This example contains one or two lines of text. The default tab is the left-most, and whichever one you click on becomes rendered in bold font. I fooled around and made the second tab italicized, for contrast.</p> Mac WayOther WayPencil and Paper Way <p>More intuitive and elegant</p> <p>The lesser way</p> <p>Respect your elders. It was all we had for a LONG TIME.</p> <p>Really, a long time. At least we had calculators instead of slide rules.</p>"},{"location":"ourtools/features/#tabs-inside-of-admonitions-or-details","title":"Tabs inside of admonitions or details","text":"<p>Everything has to be indented FOUR SPACES (not a tab char) inside of the tab you are creating</p> <p>So when inside of an (admonition, detail, tab) therefore tab contents are all indented EIGHT spaces. (Yes, you can have tabs inside of tabs.)</p> <p>These examples contain code blocks.</p> Details are collapsed (by default) Mac WayWindows Way <pre><code>Contents for Mac users\n</code></pre> <pre><code>Contents for Mac users\n</code></pre> <p>Admonitions are always expanded</p> <p>Each tab contains a block of text. Here we've added programming language specifier keywords.</p> C++PythonBash <p>Contents for C++ code <pre><code>#include &lt;iostream&gt;\n\nint main() {\nstd::cout &lt;&lt; \"Hello World!\";\nreturn 0;\n}\n</code></pre></p> <pre><code>print('Hello, world!')\n</code></pre> <p>Discussion about bash code <pre><code>usage()\n{\necho \"Usage: $0 [-h] \"\necho \"Displays all defined QOS with nice formatting.\"\necho \"Caution, this script may not show all fields relevant/used in future.\"\necho \"    -h     |--help        display usage\"\n}\n</code></pre></p> <ol> <li> <p>https://squidfunk.github.io/mkdocs-material/reference/footnotes/#adding-footnote-references\u00a0\u21a9</p> </li> </ol>"},{"location":"ourtools/tags/","title":"Tagged Files","text":"<p>Tags and the files they are mentioned in are listed here, automatically generated by Material for MkDocs.</p> <p>While the web site is under development, they guide site authors to documents which need help and notify users how to approach the information found on tagged pages.</p> <p>After the web site content stabilizes, the tags that remain will primarily consist of keywords.</p>"},{"location":"ourtools/tips-conventions/","title":"Tips and Conventions","text":"<p>We aim to create a great web site for our users. Consistency contributes to that result. Here are some conventions and time-saving tips.</p> <p>PLEASE read through the Features page for ideas about ways to present information in the best manner. Materials for MkDocs has so many tools for creating attractive and useful pages!!!</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#conventionschecklist","title":"Conventions/Checklist","text":"<p>When editing pages on our site, please keep these things in mind.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#update-tags","title":"Update tags","text":"<p>as needed to reflect current document status. Example <code>needs-to-be-written</code> becomes <code>in-progress</code> as the document is fleshed out and becomes somewhat useful to users.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#remove-authoring-notes","title":"Remove authoring notes","text":"<p>as their guidance is fulfilled by your modifying the document.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#write-information-once-then-refer-to-it","title":"Write information once, then refer to it","text":"<p>Instead of placing versions of the same information in multiple places, put one authoritative version in the right document and then refer to it in other documents where needed. Example: In the FAQ, the answer to a question/issue may be a simple \"See this document\"</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#refer-to-specific-locations-within-documents","title":"Refer to specific locations within documents","text":"<p>Jeffrey enabled \"permalink\" so each section of each document can have its own URL.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#images-live-near-their-documents","title":"Images live near their documents","text":"<p>Each topic subdirectory under docs/ has an images/ directory to hold images for documents in that directory. This aids in web site maintenance, as it is more clear over time which of many dozen image files on the web site are used where.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#include-live-near-their-documents","title":"Include live near their documents","text":"<p>Each topic subdirectory under docs/ has an includes/ directory to hold files pulled into documents in that directory. This aids in web site maintenance, as it is more clear over time which of many dozen include files on the web site are used where.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#tips","title":"Tips","text":"","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#linking-to-documents-within-web-site","title":"Linking to documents within web site:","text":"<ul> <li>Because documents are divided up between directories by topic, any references you make to them need to use correct relative paths.</li> <li>Links need to include the \".md\" file name suffixes. These are not shown in the URL on the web site, but are required for links to be make correctly.</li> </ul>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#symbolic-link-image-files-which-change","title":"Symbolic Link Image Files Which Change","text":"<p>We have some documents, such as Orientation PDF, which are updated. During the updates their file names often change in order to embed date versioning info.</p> <p>Instead of embedding links in our web pages to the actual PDF file, and having to find and update all of the links every time the file name changes, Jeffrey has found that creating symbolic links with well-chosen static file names to the variable file names allows the links to remain constant.</p> Make a target named latest-orient.pdf<pre><code>cd docs/orient/images\nln -s JHPCE-Overview-CMS-2023-12-2.pdf latest-orient.pdf\nUPDATE - The orientation slides are at [This Link](https://docs.google.com/presentation/d/1elMSTUdKws7FLVFK7vVV_AErA4brNSPX/pub){:target=\"_blank\"}\n</code></pre>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#blank-lines-are-required-for-some-features-to-work","title":"Blank lines are required for some features to work:","text":"<p>Some elements, such as lists like this one, rely on there being a blank line above the first item. Same is true for admonitions and details.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#frontmatter-indentation","title":"Frontmatter Indentation:","text":"<p>Items in YAML at the top of many pages has to be indented according to YAML rules, or things break.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#indentation-for-block-content","title":"Indentation for block content:","text":"<p>FOUR spaces is what you need to put in front of each paragraph you want to be included in something like an admonition or detail. FOUR, no more, no less. Four spaces is also needed to nest list items.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#inspect-example-documents","title":"Inspect example documents","text":"<p>if you want to see how something was done in practice. The features page contains many such examples.</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#line-numbers-in-code-blocks","title":"Line numbers in code blocks","text":"<p>They had been enabled by default until 20240310, when Jeffrey disabled them. When enabled, you got rid of them for a cod block by adding <code>linenums=\"0\"</code> after the opening three backticks and a space, i.e. ` ` ` Space linenums=\"0\"</p> <p>Now that they have been disabled, if you want line numbering, you should add <code>linenums=\"1\"</code></p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#add-a-title-too","title":"Add a title, too","text":"<p>to code blocks by adding <code>\"title words\"</code> after the opening three backticks and a space, i.e. ` ` ` Space \" My title \" Space linenums=\"0\"</p>","tags":["in-progress"]},{"location":"ourtools/tips-conventions/#2-ways-of-embedding-code-blocks","title":"2 ways of embedding code blocks:","text":"<ul> <li>With HTML:</li> </ul> <pre><code>[test@compute-107 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0\n</code></pre> <ul> <li>With Markdown: <pre><code>[test@compute-107 ~]$ module list \n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0\n</code></pre></li> </ul>","tags":["in-progress"]},{"location":"ourtools/wishlist/","title":"Features we might/should implement","text":""},{"location":"ourtools/wishlist/#places-with-resources-to-investigate","title":"Places with resources to investigate","text":"<p>This best-of-mkdocs is updated regularly and has categorized MkDocs plugins and other solutions generated from github star rankings.</p> <p>This page has a number of recommendations that I find interesting. As always, one has to consider whether something is the best-in-class. The PDF export link below for example hasn't been updated in years. Is it still the right choice? Maybe. Mermaid graph generation plugin, PDF export, page redirects, exclude and exclude-from-search, Jupyter notebook</p>"},{"location":"ourtools/wishlist/#tables-multi-line-column-headers","title":"Tables: Multi-line column headers","text":"<p>Maybe this is possible. Would be nice to be able to use more words-per-column without forcing the table to be pushed out and require the use of a scrollbar.</p>"},{"location":"ourtools/wishlist/#absolute-paths","title":"Absolute paths","text":"<p>So references to other documents could be made to a single absolute path. Right now you have to use relative paths, which is just prone to more errors. </p> <p>20240317 - JRT noticed that Adi's use of absolute paths for web portal images were rendered on the web site but not when viewed through a \"mkdocs serve\" session. He changed those links to be relative.</p>"},{"location":"ourtools/wishlist/#tables-sortable-tables","title":"Tables: Sortable tables","text":"<p>(Adi has enabled) (but there are other options we might want to enable)</p> <p>They are possible. Might require an extension or plugin.</p> <p>This might have been enabled by adding this to mkdocs.yml</p> <p>Note that tablesort provides alternative comparison implementations like numbers, filesizes, dates and month names. See the tablesort documentation for more information.</p>"},{"location":"ourtools/wishlist/#announcement-bar-in-header","title":"Announcement bar in header","text":"<p>For things like planned outages. Update <code>/overrides/main.html</code> file and replace the message with any custom one. Site needs to be rebuild afterwards for the announcement to propagate to live.</p> <p>To hide the announcement (or any overriden block) use <code>-%</code> at the top block definition and <code>%-</code> at definitioan closing. <pre><code>{% block announce -%}\n{%- endblock %}\n</code></pre> To adjust colors modify <code>docs/stylesheets/extra.css</code> and adjust the <code>.md-banner</code> style definition to match your color preference. <pre><code>.md-banner {\n  background-color: #ffcc00;\n  color: #cc3300;\n}\n</code></pre></p>"},{"location":"ourtools/wishlist/#open-urls-in-new-tabs","title":"Open URLs in new tabs","text":"<p>(Adi has enabled) I think there might be plugins which make this easier than what you have to do othwerwise, which is to use HTML instead of the Markdown notation. In normal HTML you add a space and a string to the end of the URL: <code>target=\"_blank\"</code></p> <pre><code>&lt;a href=\"https://squidfunk.github.io/mkdocs-material/reference/admonitions/\" target=\"_blank\"&gt;About admonitions&lt;/a&gt;\n</code></pre>"},{"location":"ourtools/wishlist/#maybe-a-user-contribution-capability","title":"Maybe a user-contribution capability","text":"<p>For topic headings like SOFTWARE where users have recipes and tuning advice. Implemented how? git pull requests against any page on the site? Is there a way to limit to a subsection?</p>"},{"location":"ourtools/wishlist/#search-enhancements","title":"Search enhancements","text":"<pre><code>plugins:\n  - search\n</code></pre> <p>Search is a plugin. There are many plugins out there for MkDocs. A different set, I suppose, for Material for MkDocs. Some overlap, some exclusive.</p> <p>Note that search is enabled by default but if you add options it HAS TO BE LISTED in the plugins stanza.</p> <p>There are A LOT of extras you can do. Someone explore later if desired.</p> <p>https://squidfunk.github.io/mkdocs-material/setup/setting-up-site-search/</p>"},{"location":"ourtools/wishlist/#blog","title":"BLOG","text":"<p>A blog writable only by site administrators. As a tool for making announcements. <pre><code>plugins:\n  - blog\n</code></pre> Blog is a plugin. https://squidfunk.github.io/mkdocs-material/setup/setting-up-a-blog/ Be aware that you cannot use features preceeded by \"Insiders\" This is enabled by adding a line to the plugins stanza</p> <p><code>- blog</code></p>"},{"location":"ourtools/wishlist/#last-modified-dates-on-documents","title":"last-modified dates on documents","text":"<p>(Adi added this. Jeffrey added the info to the \"mkdocs serve\" recipe in features.md) This might be provided by multiple plugins out in the world. The one that might be the most common is \"git-revision-date-localized\"</p>"},{"location":"ourtools/wishlist/#glossary","title":"Glossary","text":"<p>There's a way to create a document which is automatically updated when people define abbreviations. Jeffrey created a glossary by hand.</p>"},{"location":"portal/datacatalog/","title":"Database Catalog","text":"<p>The JHPCE Data Catalog provides a way to browse and search an online collection of data sets generated at JHU as well as publicly available ones.</p> <p>The goals of the JHPCE Data Catalog are:</p> <ul> <li>Provide a central place for JHU generated data</li> <li>Facilitate the collaboration between different JHU Departments</li> <li>Help JHU researchers idenfity data sets that relevant to their work</li> </ul> <p>The catalog does not store the data sets but rather provides an index of available ones to help researchers identify data sets pertinent to their work.</p> <p>Warning</p> <p>This is being developed. Please send suggestions about functional matters to bitsupport. After further refinement we will welcome contributions to the database itself.</p> <p>Access requires a JHED credential and a computer that is on campus or using the VPN.</p> <p>https://jhpce-app02.jhsph.edu/</p>"},{"location":"portal/web-apps/","title":"JHPCE Web Enabled User Apps","text":"<p>Our web portal has several sections. You will need to log with your JHED ID and password. This web site is only available on campus, so if you are outside of the school network, you will need login to the JHU VPN first.</p> <p></p> <ul> <li> <p> JupyterLab</p> <p></p> <p>JupyterLab is the latest web-based interactive development environment for notebooks, code, and data. Its flexible interface allows users to configure and arrange workflows in data science, scientific computing, computational journalism, and machine learning. A modular design invites extensions to expand and enrich functionality. The JHPCE JupyterLab session will timeout after 60 minutes of inactivity, a new session will need to be requested if that happens.</p> </li> <li> <p> RStudio</p> <p></p> <p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. The JHPCE RStudio session will timeout after 1 day of inactivity, a new session will need to be requested if that happens.</p> </li> <li> <p> Visual Studio Code</p> <p></p> <p>Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages and runtimes (such as C++, C#, Java, Python, PHP, Go, .NET). You can visit the link below for additional setup and usage details (this web portal is only available on campus, so if you are outside of the school network, you will need login to the JHU VPN first.) The Visual Studio Code session will timeout after 12 hours of inactivity, a new session will need to be requested if that happens.</p> <p> Access Instructions</p> </li> </ul>"},{"location":"portal/web-reset/","title":"JHPCE User Account Tools","text":"<p>Our web portal has several sections.</p> <p>The JHPCE User Account Tools section provide the means to:</p> <ul> <li>Reset your password</li> <li>Request Authenticator Code (\"One Time Password\")</li> <li>Update Contact Information</li> </ul> <p>You can visit web portal to get a one-time use verification code and/or reset your password.  You log into that web site with your JHED ID and password, and then you should find a link for requesting a verification code or resetting your password.  This web site is only available on campus, so if you are outside of the school network, you will need login to the JHU VPN first.</p> <p>After you ssh into the cluster:</p> <ul> <li> <p>If you need to change your password, use the <code>kpasswd</code> command. You will need to use a password with three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</p> </li> <li> <p>If you need to set up your Google Authenticator, you can use the following steps:</p> </li> <li>On your smartphone, bring up the \"Google Authenticator\" app</li> <li>On the JHPCE cluster, run <code>auth_util</code></li> <li>In <code>auth_util</code>, use option \"5\" to display the QR code (you may need to resize your ssh window - \u201dview-&gt;terminal unzoom\u201d in MobaXterm)</li> <li>Scan the QR code with the Google Authenticator app</li> <li>Next in <code>auth_util</code> use option 2 to display your scratch codes</li> <li>In <code>auth_util</code>, use option \"6\" to exit from <code>auth_util</code></li> </ul> <p>Going forward, you would then use the 6 digit code from Google Authenticator when prompted for \u201cVerification Code\u201d when logging into the JHPCE cluster.</p> <p>Warning</p> <p>As of 20240306, C-SUB users cannot use these services, as they are not yet included in an underlying database.</p> <p>Note</p> <p>Your JHPCE cluster username and password are NOT the same as your JHED ID and password. These are maintained by different groups and do not change in one place when changed in the other.</p> <p>Password complexity requirements</p> <p>After you get logged in, use the \"kpasswd\" command to choose a new password. You will need to use a password with three of the following four sets of characters: upper-case, lower-case, numerical digits, and special characters.</p>"},{"location":"slurm/about-jobs/","title":"All About SLURM Jobs","text":"<p>Well, maybe not ALL. </p> <p>Here we try to describe some basic facts about jobs. We will point to this document in the other places in the web site where we mention some of these core facts. There are links to some of these other web pages in this document, but see the navigation bar for a complete list of SLURM topics/pages.</p> <p>SLURM is popular because it is flexible and supports many different computational needs. Users do complex things on various clusters, and the way you normally use SLURM may not at all match the way someone else does.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#what-is-a-slurm-job","title":"WHAT IS A SLURM JOB?","text":"<p>A job can span multiple compute nodes and is the sum of all task resources.</p> <p>A job consists of</p> <ol> <li>an optional set of directives</li> <li>a command to run, either bash (for an interactive job) or a shell script<sup>1</sup> (for a batch job)</li> <li>one or more steps, numbered starting from 0.<sup>2</sup></li> </ol> <p>Interactive jobs provide a shell interface on a single compute node. There you can run programs interactively, including ones with GUI interfaces that display via the X11 protocal. For more information, see our page here.</p> <p>Batch jobs are handled by a job scheduler within SLURM, and start when they can get access to one or more compute nodes providing sufficient resources. These jobs do their work unattended. For more information, see our page here.</p> <p>Array jobs are one type of batch job. They consist of 2 or more jobs spawned by a single batch script. For more information, see this section of our batch job page here.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#directives","title":"DIRECTIVES","text":"<p>Directives can come from a variety of sources, including, in order of preference: on the command line, from environment variables, embedded in a batch file script on <code>#SBATCH</code> lines, or from a personal <code>~/.slurm/defaults</code> file. If you don't specify a directive, them some default value will be used. Often that default is one, as in one task, one CPU core, one compute node, ...  The default partition is <code>shared</code>. 5 gigabytes is the default amount of RAM per job.</p> <p>Directives consist of:</p> <ol> <li>requests for resources (time, partition, memory, CPU)</li> <li>instructions to SLURM</li> </ol> <p>You will find directives described in the <code>sbatch</code> manual page.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#tasks","title":"TASKS","text":"<p>A job aims to complete one or more tasks.</p> <p>Tasks are requested at the job level with <code>--ntasks</code> or <code>--ntasks-per-node</code>, or at the step level with  <code>--ntasks</code>. CPUs are requested per task with <code>--cpus-per-task</code>.</p> <p>A task is executed on a single compute node in a job step, using one or more CPU cores and a non-zero amount of memory (5 gigabytes if you don't specify -- you can request less than 5G!! -- the less memory your job needs, the more likely it will be able to run).</p> <p>If there are multiple tasks, then each uses a subset of the resources of the overall job.  Multiple tasks can run on a single compute node or on multiple nodes. The resources used by any one task cannot exceed what is found on a single node.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#nodes-and-partitons","title":"NODES and PARTITONS","text":"<p>Jobs execute on a set of one or more compute nodes called a nodelist. The first node in that set is called the 0th node.<sup>3</sup></p> <p>Compute nodes are grouped together into partitions. For more information about partitions, see our page here.</p> <p>Warning</p> <p>If a job exceeds its time limit or memory, it will be killed before it finishes doing its work.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#job-notation","title":"JOB NOTATION","text":"<p>Once jobs are submitted and accepted, they are given a job id number. When specifying a job to a SLURM program like <code>scontrol</code> or reading output from a SLURM program like <code>sacct</code>, you will see a variety of forms of identification numbers.</p> <p>In our documentation and email, we often say simply \"job\" or \"jobid\" with the expectation that you will determine the right value to use in the situation.</p> <p>Underscores _ divide job arrays from job ids</p> <p>Periods . divide job ids from step ids.</p> <p>Sometimes you can use a job array number where the documentation says job id, as when cancelling the whole job array with <code>scancel</code> as opposed to cancelling just one of its job elements.</p> <pre><code>&lt;job id&gt;\n&lt;job id&gt;.&lt;step id&gt;\n&lt;job array&gt;_&lt;job id&gt;\n&lt;job array&gt;_&lt;job id&gt;.&lt;step id&gt;\n</code></pre>","tags":["slurm"]},{"location":"slurm/about-jobs/#jobs-have-multiple-steps","title":"JOBS HAVE MULTIPLE STEPS","text":"<p>The average job will have two or more steps of these types: </p> <ul> <li>external step - the connection from the node on which you submitted the job and the leading node of your nodelist. This step normally succeeds whether or not your overall job does.</li> <li>batch step - created for jobs submitted with <code>sbatch</code>  The exit code of this script impacts the final STATE of this step.</li> <li>interactive step - created for jobs submitted with <code>srun</code> (outside of a batch job)</li> <li>normal step - a batch job can have multiple normal steps, which will appear in accounting output as <code>&lt;job_id&gt;.&lt;step_id&gt;</code> Each step will be created by an <code>srun</code> command. Step numbers start at 0. Interactive jobs do not have any normal steps.</li> </ul> <p>You will notice in the output of <code>sacct</code> commands that each job has multiple entries, including one for the overall job. Each entry has a STATE code (e.g. COMPLETED, FAILED, CANCELLED) and an EXITCODE (e.g. 0:0).  Keep in mind when viewing state information whether you are looking at the overall job state or that of a component step.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#job-names","title":"JOB NAMES","text":"<p>You can give a job a job name. This explicit name can be used with some commands instead of jobids.</p> <ul> <li>The name of an external step will always be \"extern\"</li> <li>The default overall name of a batch job will be \"bash\"</li> <li>The name of a batch job's normal step(s) will always be \"bash\"</li> <li>The default overall name of an interactive job will be \"bash\"</li> </ul> <p>The <code>sacct</code> command is an important one to know how to use. We have a document explaining how to use it here. Please read it, because you need to know how to look up information about your jobs. By default it prints fields only 20 characters wide. It will display a + when there is more information beyond 20 char, as seen below for the external steps. (See our document for instructions on changing the output format.)</p> <p>Example of a batch job<pre><code>  JobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n  ------------ ---------- ---------- ---------- ---------- ---------- -------- \n\n  1922948            bash        cee      jhpce         24    RUNNING      0:0 \n  1922948.ext+     extern                 jhpce         24    RUNNING      0:0 \n  1922948.0          bash                 jhpce         24    RUNNING      0:0 \n</code></pre> Example of an interactive job (lacks normal step)<pre><code>  JobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n  ------------ ---------- ---------- ---------- ---------- ---------- -------- \n  1542874            bash     shared      jhpce          2    RUNNING      0:0 \n  1542874.ext+     extern                 jhpce          2    RUNNING      0:0 \n</code></pre></p>","tags":["slurm"]},{"location":"slurm/about-jobs/#job-states","title":"JOB STATES","text":"<p>The overall job and each step has its own job state code. They often differ!! </p> <p>Job states have short names consisting of one or two letters, and a full name.  You can use either form when working with SLURM commands. They are shown here capitalized for emphasis but can be specified as lower-case.</p> <p>The main job states you will see:</p> Short Name Long Name Explanation PD PENDING Job is waiting to start R RUNNING Job is currently running CG COMPLETING Job has ended, clean-up has begun CD COMPLETED Job finished normally, with exit code 0 F FAILED Job finished abnormally, with a non-zero exit code CA CANCELLED Job was cancelled by the user or a sysadmin OOM OUT_OF_MEMORY Job was killed for exceeding its memory allocation TO TIMEOUT Job was killed for exceeding its time limit Click for a complete list of job states <p>FROM https://slurm.schedmd.com/squeue.html#lbAG</p> BF BOOT_FAIL Job terminated due to launch failure, typically due to a hardware failure (e.g. unable to boot the node or block and the job can not be requeued). CA CANCELLED Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. CD COMPLETED Job has terminated all processes on all nodes with an exit code of zero. CF CONFIGURING Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG COMPLETING Job is in the process of completing. Some processes on some nodes may still be active. DL DEADLINE Job terminated on deadline. F FAILED Job terminated with non-zero exit code or other failure condition. NF NODE_FAIL Job terminated due to failure of one or more allocated nodes. OOM OUT_OF_MEMORY Job experienced out of memory error. PD PENDING Job is awaiting resource allocation. PR PREEMPTED Job terminated due to preemption. R RUNNING Job currently has an allocation. RD RESV_DEL_HOLD Job is being held after requested reservation was deleted. RF REQUEUE_FED Job is being requeued by a federation. RH REQUEUE_HOLD Held job is being requeued. RQ REQUEUED Completing job is being requeued. RS RESIZING Job is about to change size. RV REVOKED Sibling was removed from cluster due to other cluster starting the job. SI SIGNALING Job is being signaled. SE SPECIAL_EXIT The job was requeued in a special state. This state can be set by users, typically in EpilogSlurmctld, if the job has terminated with a particular exit value. SO STAGE_OUT Job is staging out files. ST STOPPED Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. S SUSPENDED Job has an allocation, but execution has been suspended and CPUs have been released for other jobs. TO TIMEOUT Job terminated upon reaching its time limit. <p>That complete list comes from this section of the <code>sacct</code> manual page, which has also been saved to a text file you can copy for your own reference: <code>/jhpce/shared/jhpce/slurm/docs/job-states.txt</code></p> <p>Warning</p> <p>The overall job and each step has its own job state code. They often differ!!  The <code>-X</code> flag to <code>sacct</code> will show you only the overall job state, such as FAILED, which is useful for most cases. However, sometimes you need to check the state of all of a jobs steps in order to see that a \"batch\" step ran OUT_OF_MEMORY.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#life-of-a-job-and-associated-states","title":"LIFE OF A JOB AND ASSOCIATED STATES","text":"<p>Until we draw a nice diagram, a written description will have to suffice. Short job state names are specified in parentheses below.</p> <ol> <li>User submits job</li> <li>SLURM evaluates syntax and resource requests.</li> <li>If problems found, then job is rejected immediately.</li> <li>Otherwise it is accepted and becomes PENDING (PD).</li> <li>SLURM's scheduler algorithms begin testing where your job will \"fit\" in its planning process for the future. There will be more slots where smaller jobs (in RAM, CPU and duration) will fit than larger ones.</li> <li>PENDING jobs can remain pending (see reasons below), CANCELLED (CA) or be dispatched to compute node(s) to start RUNNING (R).</li> <li>RUNNING jobs can immediately run into a problem due to coding errors and become FAILED (F).</li> <li>RUNNING jobs can run correctly but then exceed their memory allocation and become OUT_OF_MEMORY (OOM).</li> <li>RUNNING jobs can run correctly but run into their wall-clock time limit and become DEADLINE (DL) or FAILED.</li> <li>RUNNING jobs can run correctly, switch to COMPLETING (CG) as processes are quitting, and then COMPLETED (CD).</li> </ol> <p>If you are very curious, the SLURM vendor describes how jobs are started and terminated in this document.</p>","tags":["slurm"]},{"location":"slurm/about-jobs/#pending-job-reasons","title":"PENDING JOB REASONS","text":"<p>These codes identify the reason that a job is waiting for execution. A job may be waiting for more than one reason, in which case only one of those reasons is displayed. These reasons are seen in the output of <code>squeue</code> and <code>scontrol show job &lt;jobid&gt;</code></p> <p>The main pending reasons you will see:</p> Name Explanation Notes BeginTime The job's earliest start time has not yet been reached Dependency Job waiting on a user-defined dependency See why with <code>showjob &lt;jobid&gt;|grep -i depend</code> DependencyNeverSatisfied Something went wrong You should investigate why. Incorrect dependency specified? JobArrayTaskLimit Array job configured to run only so many tasks at a time Good way to control resource use JobHeldAdmin The job is held by a system administrator JobHeldUser The job is held by the user Priority One or more higher priority jobs exist for this partition or advanced reservation QOSJobLimit The job's QOS has reached its maximum job count QOSMaxCpuPerUserLimit Your other running jobs have consumed your CPU quota <code>slurmuser --me</code> will show your used/pending resources QOSMaxMemoryPerUser Your other running jobs have consumed your RAM quota <code>slurmuser --me</code> will show your used/pending resources Reservation Job waiting for its advanced reservation to become available Resources Needed resources not currently available in partition <code>slurmpic -p &lt;partition&gt;</code> can show you what's used &amp; consumed in that partition ReqNodeNotAvail Some node specifically required by the job is not currently available Usu seen when all nodes in a partition are unavailable Click for a mostly-complete list of pending reasons <p>AssocGrp*Limit The job's association has reached an aggregate limit on some resource.</p> <p>AssociationJobLimit The job's association has reached its maximum job count.</p> <p>AssocMax*Limit The job requests a resource that violates a per-job limit on the requested association.</p> <p>AssociationResourceLimit The job's association has reached some resource limit.</p> <p>AssociationTimeLimit The job's association has reached its time limit.</p> <p>BadConstraints The job's constraints can not be satisfied.</p> <p>BeginTime The job's earliest start time has not yet been reached.</p> <p>Cleaning The job is being requeued and still cleaning up from its previous execution.</p> <p>Dependency This job has a dependency on another job that has not been satisfied.</p> <p>DependencyNeverSatisfied This job has a dependency on another job that will never be satisfied.</p> <p>FrontEndDown No front end node is available to execute this job.</p> <p>InactiveLimit The job reached the system InactiveLimit.</p> <p>InvalidAccount The job's account is invalid.</p> <p>InvalidQOS The job's QOS is invalid.</p> <p>JobHeldAdmin The job is held by a system administrator.</p> <p>JobHeldUser The job is held by the user.</p> <p>JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc.</p> <p>Licenses The job is waiting for a license.</p> <p>NodeDown A node required by the job is down.</p> <p>NonZeroExitCode The job terminated with a non-zero exit code.</p> <p>PartitionDown The partition required by this job is in a DOWN state.</p> <p>PartitionInactive The partition required by this job is in an Inactive state and not able to start jobs.</p> <p>PartitionNodeLimit The number of nodes required by this job is outside of its partition's current limits. Can also indicate that required nodes are DOWN or DRAINED.</p> <p>PartitionTimeLimit The job's time limit exceeds its partition's current time limit.</p> <p>Priority One or more higher priority jobs exist for this partition or advanced reservation.</p> <p>Prolog Its PrologSlurmctld program is still running.</p> <p>QOSGrp*Limit The job's QOS has reached an aggregate limit on some resource.</p> <p>QOSJobLimit The job's QOS has reached its maximum job count.</p> <p>QOSMax*Limit The job requests a resource that violates a per-job limit on the requested QOS.</p> <p>QOSResourceLimit The job's QOS has reached some resource limit.</p> <p>QOSTimeLimit The job's QOS has reached its time limit.</p> <p>QOSUsageThreshold Required QOS threshold has been breached.</p> <p>ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available.</p> <p>Reservation The job is waiting its advanced reservation to become available.</p> <p>Resources The job is waiting for resources to become available.</p> <p>SystemFailure Failure of the Slurm system, a file system, the network, etc.</p> <p>TimeLimit The job exhausted its time limit.</p> <p>WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</p> <p>That mostly-complete list comes from this section which refers to yet another page for THE complete list here</p> <ol> <li> <p>One does not have to use the bash shell for interactive sessions or to execute your batch script. You can specify other shells or interpreters.\u00a0\u21a9</p> </li> <li> <p>(It is unclear to the author whether an interactive job officially has a step. No steps are listed for them in the sacct output, but he suspects that, technically, in the vendor's documentation, it does have a step. Because tasks are implemented by job steps.)\u00a0\u21a9</p> </li> <li> <p>If there are more than one nodes, one should look on the 0th node for  information about the job in /var/log/slurm/slurmd.log file when troubleshooting.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/crafting-jobs/","title":"Crafting SLURM Batch Jobs","text":"<p>Authoring Note</p> <p>This document is accumulating information which might best be split into several more-focused documents. There are many ways of creating jobs!</p> <p>There are a variety of techniques one can use to initiate SLURM jobs to accomplish various tasks. Here we will describe important topics and accumulate pointers to documentation written by others for their clusters. Later we hope to write concrete examples of our own.</p> <p>In the directories under <code>/jhpce/shared/jhpce/slurm/</code> you will find files used during orientation, some accumulated documents and batch examples. This web site probably contains a better set of information, though.</p> <p>Note</p> <p>If you want to submit example batch jobs, or documents describing good workflows for specific tasks, or anything of the kind, we will happily consider adding them to that directory and to this document!!!</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#sbatch-srun-and-salloc","title":"Sbatch, srun and salloc","text":"<p>There are three commands used to request resources from SLURM. You will find all three discussed in the linked documentation.</p> <p>Here at JHPCE we have been using <code>srun</code> primarily as way to start interactive sessions. However it can also be used after allocating resources with <code>salloc</code> and inside of <code>sbatch</code> scripts. See web site below for examples.</p> <p>Be careful using <code>salloc</code> that you don't leave allocated resources unused. One way to use salloc is to request resources and launch a new bash shell. Those resources are not released until that bash shell is ended.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#basic-sbatch-rules","title":"Basic Sbatch Rules","text":"<ol> <li>First characters in the batch file need to be: <code>#!/bin/bash</code> (although you can use an interpreter other than bash)</li> <li><code>#SBATCH</code> directives need to appear as the first characters on their lines.</li> <li><code>#SBATCH</code> directives need to appear before any shell commands.</li> <li>You can put comments after # symbols.</li> <li>You may need to add a <code>wait</code> command at the bottom to ensure that processes spawned earlier complete before the script does.</li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#slurm-directive-order-of-precendence","title":"SLURM Directive Order of Precendence","text":"<p>Authoring Note</p> <p>This needs to be written. Should it be its own document, for better readability?</p> <p>For now see these two PDF pages from an orientation document.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#job-environment","title":"Job Environment","text":"","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#troubleshooting-mysterious-failures","title":"Troubleshooting mysterious failures","text":"<p>The shell that runs your command can have different environments depending on how it was created by the operating system. The \"dot files\" like <code>.bashrc</code> and <code>.bash_profile</code> (and equivalents in /etc/) are processed in different combinations depending on whether the shell is a (login or non-login) and (interactive vs non-interactive) one.</p> <p>This can be extremely confusing! Usually things Just Work, but if you run into problems running batch jobs that you cannot reproduce in interactive debugging sessions, see this important document: other aspects of job environments</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#keep-in-mind-your-use-of-environments-via-conda-or-python","title":"Keep in mind your use of environments via conda or python","text":"<p>You may have dot files which include commands that load named conda environments. If those commands are in the wrong kind of dot file, then they won't be processed during batch jobs.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#where-am-i","title":"Where am I?","text":"<p>By default jobs start in the same directory that was the current working directory where you ran <code>sbatch</code> or <code>srun</code>. The directive <code>--chdir=somepath</code> or <code>#SBATCH --chdir=somepath</code> can be used to set the working directory of the job. Whether you choose to use this directive depends on your workflow and typical awareness of where you \"are\". Would you rather change batch files or set command line arguments to ensure that jobs run in the desired directories or would you rather double-check your location? </p> <p>Another file-location issue is where your SLURM output and error files are created. By default they are created by SLURM in the job's working directory. You may want to make them easier to find and/or keep project directories clear of transient clutter by creating directories in your home directory or file allocation (e.g. <code>/dcs07/my-research-group/data/slurm-output</code>) and use directives (e.g. <code>--output=~/slurmout/%j.out</code>).</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#inputoutput-considerations","title":"Input/output Considerations","text":"<p>Authoring Note</p> <p>Use the same terms as used in the storage overview page. We want to be consistent. It helps users, and helps us make links between related articles.</p> <ul> <li>home directory</li> <li>project storage</li> <li>fastscratch</li> <li>local compute node /tmp</li> </ul> <p>Some programs have specific variables you can set to indicate where files should be created.</p> <p>SAS has \"WORK\" -- is this set to something in the module load process?</p> <p>R has \"TMPDIR\" which defaults to /tmp. We may be changing the module load code to define this to /fastscratch where that is present (which should be everywhere except on C-SUB nodes).</p> <pre><code>You could use your 1TB of fastscratch space for this. So your SLURM script could use commands like:\n\nmodule load conda_R\nexport TMPDIR=$MYSCRATCH\nR CMD BATCH myprog.R\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#job-arrays","title":"Job Arrays","text":"<p>Tip</p> <p>Excellent set of explanations and examples from University of Arizona!!! (See their other examples)</p> <p>Array jobs allow you to use one job script to run many jobs across a set of samples. Each array job spawns subordinate \"array task\" jobs. The overall job has a jobid, and the individual array task jobs are given their own jobid number by appending an underscore and their index number to the master array jobid.</p> <p>That SLURM script (and the program(s) it calls) need(s) to contain syntax that uses shell and SLURM environment variables to assign different work to different task element jobs. And to store the results in files with unique names.  One has to name files the right way to be able to script the input and output processing.</p> <p>There are a variety of ways to go about creating such scripts and managing the output.  Example scripts are very useful, so we have provided links to other web sites. There are also tools people have written to help manage the overall workflow. These can, for example, make it easier to identify jobs that failed out an array job and to submit them to run again.</p> <p>The underscore character is used in commands and is seen in the output of programs like <code>sacct</code> as a separator between the job ID number and that of the array task ID number. Output and error files also use underscores in their names.</p> <p>Controlling your usage with \"step size\"</p> <p>When you submit an array job you can indicate that you only want a certain number of tasks to be executed at a time. This will allow other users of your partition to be able to access it, or control the disk and network input/output rate to avoid choking a file server.</p> <p>You can use the <code>scontrol</code> command to add or adjust that aspect of the job for pending or running jobs. See our scontrol tips  page.) To reference a specific task of a job array, combine SLURM_ARRAY_JOB_ID with SLURM_ARRAY_TASK_ID (e.g. \"scontrol update ${SLURM_ARRAY_JOB_ID}_{$SLURM_ARRAY_TASK_ID} ...\").</p> <p>Vendor docs about job arrays are here.</p> <p>Excellent set of explanations and examples from University of Arizona!!!</p> <p>New Mexico State University job array section here. Good, compact example document.</p> <p>Another nice straightforward description is in this Ronin blog post. </p> <p>USC explanation of job arrays.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#job-array-email-notifications","title":"Job Array Email Notifications","text":"<p>These are handled at the job level, not the individual task element level, unless you provide an extra arguement. Please don't do that, unless you KNOW that your jobs are going to take a long time, so you don't create a mail \"storm\" of messages.<sup>1</sup></p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#job-array-related-environment-variables","title":"Job array-related environment variables","text":"<p>The _ARRAY_ variables are only set if this job is part of a job array.</p> Variable name Meaning Script variable Usage Result JOB_ID Jobid %J #SBATCH %J.err 123.err SLURM_ARRAY_JOB_ID The overall job ID %A #SBATCH -o %A.out 123.out SLURM_ARRAY_TASK_ID The individual task ID %a #SBATCH -o %A_%a.out 123_6.out SLURM_ARRAY_TASK_STEP The optional step size of task IDs not available #SBATCH --array 2-15%3 3 SLURM_ARRAY_INX The array index range not available #SBATCH --array 2-15 2 15 SLURM_ARRAY_TASK_MIN The minimum task ID not available #SBATCH --array 2-15 2 SLURM_ARRAY_TASK_MAX The maximum task ID not available #SBATCH --array 2-15 15","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#dependent-jobs","title":"Dependent jobs","text":"<p>You can configure jobs to run in order with some conditional control. Conditions include jobs starting, jobs finishing, jobs finishing without errors, all of a set of jobs finishing, etc.</p> <p>This can be used to achieve different goals, such as</p> <ul> <li>controlling how many resources you consume at one time when running jobs on PI partitions where there are no enforced QOS limits, and</li> <li>breaking up jobs into smaller pieces yet still have them run in the right order. For example, copying some data to /fastscratch and, if that was successful, launching an analysis job.</li> </ul> <p>Smaller jobs start more quickly, and it can be better to make progress where possible while, for example, waiting for a large amount of RAM to become available on a single node. This is also more efficient for the cluster, since work can be done with, say, less RAM for this portion of the overall task, then a large amount for that portion, then a smaller amount again for some cleanup or merging portion. This is much better than having to wait for a large amount of RAM to become available and then, when it is allocated, tying it down for the entire duration of the single job.</p> <p>This approach also allows you to use different resources to complete a larger task. For example, use the more common CPU nodes to prepare some data for another job to process on a GPU node, of which there are fewer, and then possibly doing some further data reduction on any available CPU node.</p> <p>As you can see, people can get more work done more quickly by crafting a set of jobs to use only the necessary resources for each stage of a larger task.</p> <p>You can use different terms (afterany, aftercorr, afterok, afternotok, singleton) and syntaxes to build dependencies. Some people use programs to create dependency graphs to manage their jobs.</p> <p>Tip</p> <p>You may think that the dependency term \"after\" is okay to use. Note that it does not mean after a job ends. It means after a job is started or cancelled.</p> <p>Note that SLURM's idea of whether a job completed successfully may not match your definition. You may need to look at the logic of your batch script and add \"exit N\" statements and other logic to provide non-zero exit codes if something goes wrong before the very last command in the script. You can also add code that specifically checks for success (as opposed to only counting on a final exit code).</p> <p>See this part of the sbatch manual page. Many clusters have sections describing this technique. Yale. CECI has a bunch of info on workflows including using additional software to manage job arrays and dependencies.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#heterogeneous-job-support","title":"Heterogeneous Job Support","text":"<p>These kinds of jobs aim to achieve some of the same goals as the simpler technique of specifying dependencies between jobs. Such as specifying different resource requirements for different components.</p> <p>Each component of such jobs has virtually all job options available including partition and QOS (Quality Of Service). See this vendor document.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#checkpointing-jobs","title":"Checkpointing Jobs","text":"<p>Checkpointing is saving the state of your computation at intervals so that you can pick up where you left off if your job ends earlier than expected. And the corresponding logic at the outset of your job that checks for the existence of state data that indicates where it should start and with which files.</p> <p>Losing work is a sad occasion. If you add some complexity to your job scripts you can have them save enough data of the right kind to be able to resume work if the job ends prematurely.  Jobs end through no fault of your own, such as file systems filling up or nodes crashing or needing to be rebooted before they can accept new jobs (this is something that occurs regularly!!!).  If you set a time limit for your job too low, at least you can submit a new job that picks up where the old one left off.</p> <p>Depending on the workflow you use, adding data saving steps might occur in different locations. Possibilities include:</p> <ol> <li>Inside your sbatch script as you loop through some steps -- add a step to save aside checkpoint data, such as which step number you are on and the data needed to resume from that point</li> <li>Inside your sbatch script you might add signal handling code (see section below) if you have a single long-running process. Your code could save aside a copy of generated data with the time and date in its file name.</li> <li>If you have a series of jobs which build upon previous work, you could intersperse checkpointing jobs which save aside state information. You might want to make them dependent upon successful completion of previous ones.</li> </ol> <p>One example checkpointing document about implementing this is from the NMSU cluster. It features example code workflows for Python, R and other languages!!!</p> <p>Here is a Youtube video about checkpointing from CECI, a European organization.</p> <p>Until we add more examples, you can search for them yourself.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#using-signals-to-clean-upcheckpoint","title":"Using signals to clean up/checkpoint","text":"<p>If a job is cancelled or killed because it exceeds its time limit (maybe also memory too?), SLURM sends two signals some time apart. Normally the first is a TERM signal, later a KILL signal. You can dispatch jobs with instructions to send them specific signals a specified number of seconds before the KILL signal is sent.</p> <p>You can modify your batch jobs so they do Good Things when they receive the first signal. This extra code is called a signal handler.</p> <p>You can also ask SLURM to send specific signals to your program.</p> <p>This is an advanced topic and will require some care and perhaps experimentation to verify your solution.</p> <p>This cluster's checkpointing page includes examples of handling signals with a focus on checkpointing. Signal handler examples are shown for bash, C/C++ and Python.</p> <p>See the sbatch manual page's explanation for the <code>--signal</code> argument.</p> <p>Pay attention to which process(es) are sent signals. The batch job, all of the job steps, ...</p> <p>Here is a blog post which discusses this in some detail.</p> <p>That post refers to this one which was updated after the post was written, so there might be newer info than was incorporated in the post.</p> <p>This stackoverflow answer seems to take a different approach. </p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#example-batch-jobs","title":"Example Batch Jobs","text":"","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#copying-data-within-cluster","title":"Copying data within cluster","text":"<p>Here is a sample batch job. More information about this topic is accumulating here.</p> Click to expand <pre><code>#!/bin/bash\n\n#SBATCH -p shared\n#SBATCH --mem=10G\n#SBATCH --job-name=cp-files\n#SBATCH --time=15-0\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --output=cp-files-%j.out    # file to collect standard output\n#SBATCH --error=cp-files-%j.err # file to collect standard output\n#SBATCH --mail-user=my-email-address@jhu.edu\n#SBATCH --mail-type=BEGIN,FAIL,END\n\ndate\ncd /dcs04/sample/path/\n\necho \"I am running on compute node:\"\nhostname\n\necho \"In directory:\"\npwd\n\necho \"The files found in this directory are:\"\n/bin/ls\n\n# args are meant to try to prevent files from being deleted in destination\nrsyncargs=\"-h --progress --sparse --numeric-ids --one-file-system --stats --ignore-existing --max-delete=0\"\n\necho \"about to try to rsync\"\n\nrsync -a $rsyncargs directory-to-be-copied /dcs05/destination/path/\n\ndate\necho \"done\"\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#copying-data-intoout-of-cluster","title":"Copying data into/out of cluster","text":"<p>We have a transfer node which is a SLURM client. It has its own \"transfer\" partition. You can craft a batch job to move data using that partition by modifying the script in the data copying section above.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#running-a-job-on-every-node","title":"Running A Job On Every Node","text":"<p>This is put here as a tool for system administrators needing to do maintenance where a SLURM job is appropriate. Maybe the technique will be useful for someone for a more limited case.</p> Click to expand <pre><code>#!/bin/bash\n#\n# JPHCE - dispatch-to-everynode - Dispatch a job to each node which is responding\n#\n#       FAILS TO WORK IF YOU DON'T SPECIFY A BUNCH OF PARTITIONS\n#       BECAUSE SHARED IS USED. Following worked at this time\n#       #SBATCH --partition=shared,cee,transfer,sysadmin,sas,gpu,bstgpu,neuron\n#\n# You need to specify a batch file at the minimum\n# You can specify additional arguments\n# TODO: Nice to be able to specify a partition to sinfo if desired\n#--------------------------------------------------------------------------\n#--------------------------------------------------------------------------\n# Date          Modification                                       Initials\n#--------------------------------------------------------------------------\n# 20231222      Created, added standard comment section.                JRT\n#--------------------------------------------------------------------------\n\nusage()\n{\necho \"Usage: $0 [directives..] batchfile \"\necho \"Usage:   Specify at least a job file\"\necho \"Usage:   Good idea to include in your batch file --output=/dev/null\"\nexit 1\n}\n\nif [ $# -lt 1 ]; then\n        usage\nelse\n        for i in `sinfo -N -r | awk '{print $1}' | sort -u | grep -v NODELIST`\n        do\n                echo $i\n                sbatch --nodelist=${i} \"$@\"\n        done\nfi\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#examples-from-elsewhere","title":"Examples from Elsewhere","text":"","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#running-multiple-jobs-from-one-script","title":"Running Multiple Jobs From One Script","text":"<p>Using srun inside of sbatch scripts, in serial and parallel. Remember to include the <code>wait</code> bash command at the end of your batch file so the job doesn't end before all of the tasks inside of it. You may need to specify the amount of memory each <code>srun</code> command needs (out of the total allocated to the overall job) so the first <code>srun</code> isn't given all of the RAM.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#workflow","title":"Workflow","text":"<p>Researchers have their own workflows, whether they call them that or not. The information below may help you understand workflow concepts and give you a vernacular with which to document what you are doing and/or encourage your research group or collaborators to improve their efficiency. </p> <p>Groups have created tools to help manage the processing of jobs. Some go quite far in terms of creating job scripts automatically from some core file which defines the work to be done. All of them have a learning curve. Whether or not you want to use a tool set, these links may help you conceive of ways to use the ideas and accumulated wisdom/insight for your own workflow.</p> <p>This cluster  has some good material about workflows. It includes references to a number of tools which make managing job arrays easier. Also links to papers about the topic of creating workflows for scientific computing.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#nersc-examples","title":"NERSC Examples","text":"<p>Good examples.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#usc-examples","title":"USC Examples","text":"<p>Good examples of basic different types of batch jobs</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#using-snakemake","title":"Using Snakemake","text":"<p>A workflow-creation tool for Python users called snakemake can be used to generate SLURM jobs. Here are some links about using snakemake with SLURM:</p> <ol> <li>vendor doc on snakemake and SLURM clusters</li> <li>a lesson on implementing snakemake with SLURM looks very nice for beginners!!! That web page is a part of an overall short course on doing HPC in Python</li> <li>the BIH cluster's instructions</li> <li>the University of Zurich's instructions</li> <li>a discussion of snakemake pros and other tools like Nextflow One group talking there developed their own suite called SciPipe</li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#nextflow","title":"Nextflow","text":"<p>Use with SLURM is mentioned here</p> <p>Nextflow is a workflow system for creating scalable, portable, and reproducible workflows. It is based on the dataflow programming model, which greatly simplifies the writing of parallel and distributed pipelines, allowing you to focus on the flow of data and computation. Nextflow can deploy workflows on a variety of execution platforms, including your local machine, HPC schedulers, AWS Batch, Azure Batch, Google Cloud Batch, and Kubernetes. Additionally, it supports many ways to manage your software dependencies, including Conda, Spack, Docker, Podman, Singularity, and more.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#atools","title":"Atools","text":"<p>atools seems to revolve around the use of CSV files to help manage job arrays which perform the same computation on many input files, or run an algorithm with many different parameter sets.</p>","tags":["in-progress","slurm"]},{"location":"slurm/crafting-jobs/#dead-simple-queue","title":"Dead Simple Queue","text":"<p>dSQ from Yale is a set of tools which aim to make running array jobs easier.</p> <p><code>dSQ</code> itself is a tool that creates batch job scripts for you, and can submit the resulting job. It can create simple jobs but its focus seems to very much be job arrays.</p> <p>It is written to be used as a module. (Users can create their own modules on JHPCE.)</p> <ul> <li>You get a simple report of which job ran where and for how long</li> <li>dSQAutopsy can create a new job file that has only the jobs that didn't complete from your last run. See this spot in the README where it describes how to do that.</li> <li>All you need is Python 2.7+, or Python 3.</li> </ul> <ol> <li> <p>Such storms can get our servers banned by mail administrators, leading to no one getting any job-related email.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/environment/","title":"SLURM Job Environments","text":"<p>Warning</p> <p>Most things Just Work. But your environment in an interactive session may be different than what your batch jobs \"see\". By environment, I mean the contents of collectively all of the shell and environment variables, such as which directories are in your PATH. Loading (or not) modules, entering conda or python \"environments\" ... such things lead to very different set ups.</p>","tags":["slurm"]},{"location":"slurm/environment/#existing-environment","title":"Existing environment","text":"<p>Are any parts of the environment in the shell you submit a job from copied into the job's environment? This can be (somewhat) controlled with the <code>--export</code> argument.</p> <p>Some clusters strongly advise their users to create batch scripts in which the <code>#!/bin/bash</code> first line has a <code>-l</code> flag. Because shells like bash process \"dot files\" (e.g. <code>.profile</code> and .<code>bashrc</code>) differently for login versus non-interactive shells (see Internet for explanations of how they differ, like this page). Perhaps this can explain confusing behaviors you notice.</p> <p>We have not here at JHPCE taught the use of that flag. Perhaps this is more important in some clusters than others because of the way accounts are provisioned with files like <code>.bashrc</code> when created.</p> <p>You can test the difference in environment by submitting trivial test jobs that run a command to save the environment of the job (<code>printenv &gt; my-jobs-env.txt</code>) and compare them with what you see in an interactive <code>srun</code> session.</p> <p>Tip</p> <p>To see all variables, simply issue the <code>set</code> command. If you want to exclude shell function definitions, use <code>set -o posix</code>. To see only shell function definitions, use <code>declare -f</code> For just environment variables, use <code>env</code> or <code>printenv</code>.</p>","tags":["slurm"]},{"location":"slurm/environment/#set-by-slurm","title":"Set by SLURM","text":"<p>SLURM sets a large number of shell environment variables for jobs to consult if desired.  A good list can be found in the sbatch manual page's INPUT ENVIRONMENT VARIABLES and OUTPUT ENVIRONMENT VARIABLES sections.</p> <p>For example, you can write your scripts to pull the number of CPUs assigned to that job by having it read <code>${SLURM_CPUS_PER_TAsK}</code></p> <p>You can see some of them by starting an interactive session and running <code>printenv | grep -i slurm</code> (there may be others that are set for jobs depending on their type and arguments -- array jobs, dependent jobs, ???).</p>","tags":["slurm"]},{"location":"slurm/environment/#managing-output-of-sacct-and-squeue","title":"Managing output of sacct and squeue","text":"<p>The way SLURM commands operate can be influenced by setting some certain environment variables, such as SLURM_TIME_FORMAT, SACCT_FORMAT, SQUEUE_FORMAT, SQUEUE_FORMAT2, SQUEUE_SORT. It can be useful to define these in aliases or shell scripts to format output in ways you need. Simply changing the value of these variables can produce vastly different output for commands like sacct and squeue.</p>","tags":["slurm"]},{"location":"slurm/environment/#passing-info-into-jobs","title":"Passing info into jobs","text":"<p>It is possible to pass variables into a SLURM job when you submit the job using the <code>-\u2013export</code> flag. For example to pass the value of the variables A and b into the job script script.sh you can use:</p> <p><code>sbatch --export=A=25,b='test' script.sh</code></p> Inside of script.sh<pre><code>#!/bin/bash\necho ${A}\necho ${b}\n</code></pre> <p>or use the special keyword <code>ALL</code> (although one can imagine that possibly going awry)</p> <p><code>sbatch --export=ALL,A=7,b='test3' script.sh</code></p>","tags":["slurm"]},{"location":"slurm/environment/#using-variables-to-set-slurm-job-name-and-output-files","title":"Using variables to set SLURM job name and output files","text":"<p>SLURM does not support using variables in the #SBATCH lines within a job script. However, values passed from the command line have precedence over values defined in the job script. So the job name and output/error files can be passed on the sbatch command line:</p> <pre><code>A=70\nb='trial-24'\n\nsbatch --job-name=${A}.${b}.run --output=${A}.${b}.out --export=D=${A},f=${b} script.sh\n</code></pre> <p>Are there any tricks to propagating information between components of a job? I mean setting your own variables in batch job scripts and having them be visible by all of the processes you mean to use them? This UPenn page in item number 2 suggests creating a shell script that you run within an sbatch file:</p> <p>\"In certain situations (e.g. multiple nodes), the scriptfile itself will not see your environment variable. To get around this, separate your submission into a control file that you submit to sbatch, and a script file that is actually run within each task slot of your job. For example, if you want to run sbatch \u2013export=MYVARIABLE controlfile, OR you have an environment variable MYVARIABLE already set and you just run sbatch controlfile, then your controlfile would have your regular #SBATCH headers and one command: srun scriptfile. This makes sure that your entire environment is transferred to the scriptfile on every job step, inside every task.\"\"</p> <p>Warning</p> <p>When modifying environment variables in shell scripts or your current shell, remember that you may be dealing with existing information that you don't want to discard (by redefining the variable without referring to the existing value). When dealing with variables that you don't make up out of whole cloth and which have simple names which might match existing ones, check for their existence first. If they exist, you may want to prepend your changes, say to a PATH-type variable which usually has multiple entries. Or append, if you want system defaults to come before your values.</p> <p>Remember that modules change environment variables when loaded and unloaded. User's default environments are controlled by some JHPCE-created modules. </p>","tags":["slurm"]},{"location":"slurm/environment/#until-this-is-fleshed-out","title":"Until this is fleshed-out...","text":"<p>Consult our list of good documentation at other clusters.</p> <p>Send suggestions of items to include here to bitsupport.</p>","tags":["slurm"]},{"location":"slurm/getting-started/","title":"Introduction to SLURM","text":"<p>See the latest This Link for examples of how to use SLURM to run jobs.</p> <p>Please check out other SLURM-related pages on this web site. There is already some great information here. </p>","tags":["slurm"]},{"location":"slurm/getting-started/#defaults","title":"Defaults","text":"<p>By default, if you do not request any specific resources, your job will receive:</p> <ul> <li>1 CPU core (with support for two hyperthreads)</li> <li>5G of memory</li> <li>1 day of run time (on public partitions)</li> <li>assignment to the <code>shared</code> partition.</li> </ul>","tags":["slurm"]},{"location":"slurm/getting-started/#the-basics","title":"The Basics","text":"<p>The Slurm Workload Manager, or simply Slurm, formerly known as Simple Linux Utility for Resource Management (SLURM), is a free and open-source job scheduler for UNIX operating systems (Linux is a branch of UNIX). We've continued to capitalize the name in our documentation at JHPCE to help keep the term more visible.</p> <p>Job schedulers like SLURM perform a number of functions. At heart, they allow people to manage computing work across many compute nodes.</p> <ul> <li>Tasks are called jobs.</li> <li>Jobs can be submitted on any SLURM node, although usually this is done on one of the login nodes.  If the job is accepted, it is assigned a unique number, the jobid.</li> <li>Jobs can be interactive, (run in real time), or batch, which are scheduled for future unattended execution</li> <li>Jobs specify resources that they will need, such as the number of CPU cores, amount of RAM (in megabytes) and job duration</li> <li>Jobs are submitted to queues or partitions (collections of compute nodes governed by specific rules (such as priorities))</li> <li>Jobs are assigned to compute nodes as the CPU and RAM resources required become available in sufficient quantities</li> <li>The dispatch of jobs from a pending status to running on compute node(s) can be governed by a number of policy tools, such as priorities and Quality of Service</li> </ul> <p>Please visit the About SLURM Jobs document to read about basic SLURM facts like job notation, steps, names and states.</p>","tags":["slurm"]},{"location":"slurm/getting-started/#slurm-commands-and-tips-for-using-them","title":"SLURM Commands and Tips For Using Them","text":"<p>The commands tip and reference document describes programs and their uses, whether they were written by the vendor or JHPCE staff. That page contains links to both manual pages and pages we've written with guidance and examples.</p> <p>If you can't find what you are looking for here, we have a collection of good documentation from other clusters here. SLURM is a popular tool, so Google searches will also be productive. Just keep in mind that the documentation for other clusters will include information specific to their hardware, software and policies.</p>","tags":["slurm"]},{"location":"slurm/getting-started/#transition-from-sun-grid-engine-to-slurm","title":"Transition from Sun Grid Engine to SLURM","text":"<p>We transitioned from the Sun Grid Engine to SLURM in 2024. We wrote this document for users familiar with SGE who needed to learn some SLURM basics.</p>","tags":["slurm"]},{"location":"slurm/interactive-jobs/","title":"Interactive Jobs","text":"","tags":["slurm","in-progress"]},{"location":"slurm/interactive-jobs/#the-essentials","title":"The essentials","text":"<p>The core command to create an interactive session is:</p> <p><code>srun --pty --x11 bash</code></p> <p>The last element of an srun command has to be a program.</p> <p>The <code>--x11</code> is optional, and only needed if you are going to be running X11 programs during your session. Omitting it can avoid a number of issues.</p>","tags":["slurm","in-progress"]},{"location":"slurm/interactive-jobs/#shortcuts","title":"Shortcuts","text":"<p>Warning</p> <p>If you want to type 5 characters instead of 22 every time you want to create an interactive job, read on and indicate that this page was helpful.  If you like to type every last character of every command, skip this section.</p> Here are two equivalent commands. Which would you rather use? <code>srun --pty --x11 --mem=20G -p interactive bash</code> <code>jsrun --mem=20G -p interactive</code> <p>You can define aliases and bash routines in your <code>.bashrc</code> file for commands you use frequently.  Bash functions can be useful for commands that are difficult to specify in simple shell aliases. You can view the definition of aliases with <code>alias</code> and bash routines with the command <code>declare -f function_name</code>.   Changes to your <code>.bashrc</code> file aren't available to your currently running shell unless you log out and back in again, launch a new shell on top of the old one, or simply have your current shell re-read the file (\"source it\") with <code>. .bashrc</code> The following routines can be copied to your buffer by hovering your mouse over the right-hand side and clicking on the copy icon which appears (faintly).</p> <p>jsrun - JHPCE srun with X11 support: <pre><code>jsrun() { if [ -z ${DISPLAY} ]; then /usr/bin/srun --pty \"$@\" bash; else /usr/bin/srun --pty --x11 \"$@\" bash; fi }\n</code></pre></p> <p>jxrun - JHPCE srun when you want to skip X11 entirely:</p> <pre><code>jxrun() { /usr/bin/srun --pty \"$@\" bash; }\n</code></pre> <p>Caution: If your DISPLAY var is set on the login node and you use <code>jxrun</code>, it will be set on the compute node but X11 apps won't work.</p> <p>These bash routine examples include <code>$@</code> symbols, which are replaced by any additional arguments you provide. The srun command requires bash or another program to come last, which is one of the reasons why a simple shell alias can't be used. You have to create either a shell script or a routine.</p> <p>So you can use these functions like a normal srun command, e.g.</p> <p><code>jsrun --mem=3G -p transfer</code></p>","tags":["slurm","in-progress"]},{"location":"slurm/monitoring/","title":"Monitoring SLURM Jobs","text":"","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#overview","title":"Overview","text":"<p>Has my job ended? Why did my job fail? How much memory did my sample job use so I can use memory efficiently in future similar jobs?<sup>1</sup> There are a number of ways in which you can learn the answers to these questions. The state of the job will determine which tools and techniques you use.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#job-states","title":"JOB STATES","text":"<p>Until we draw a nice diagram, a written description will have to suffice. Here we focus on batch jobs and common states. Job states have short names consisting of one or two capitalized letters, and a full name. Short names are introduced in parentheses below. We have placed a copy of the list in <code>/jhpce/shared/jhpce/docs/job-states.txt</code>.</p> <p>Batch jobs consist of several job steps, two at minimum (\"batch\"<sup>2</sup> and \"extern\"<sup>3</sup>). The overall job and each step has its own job state code. They often differ. The <code>-X</code> flag to sacct will show you only the overall job state, such as FAILED, but some times you need to check the state of all of a jobs steps in order to see that a \"batch\" step ran OUT_OF_MEMORY.</p> <ol> <li>User submits job</li> <li>SLURM evaluates syntax and resource requests.</li> <li>If problems found, then job is rejected immediately.</li> <li>Otherwise it is accepted and becomes PENDING (PD).</li> <li>SLURM's scheduler algorithms begin testing where your job will \"fit\" in its planning process for the future. There will be more slots where smaller jobs (in RAM, CPU and duration) will fit than larger ones.</li> <li>PENDING jobs can be held, CANCELLED (CA) or be dispatched to compute node(s) to start RUNNING (R).</li> <li>RUNNING jobs can immediately run into a problem due to coding errors and become FAILED (F).</li> <li>RUNNING jobs can run correctly but then exceed their memory allocation and become OUT_OF_MEMORY (OOM).</li> <li>RUNNING jobs can run correctly but run into their wall-clock time limit and become DEADLINE (DL) or FAILED.</li> <li>RUNNING jobs can run correctly, switch to COMPLETING (CG) as processes are quitting, and then COMPLETED (CD).</li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#pending-jobs","title":"PENDING JOBS","text":"<p>Authoring Note</p> <p>Need to insert here the most common reasons and their explanations.</p> <p>Jobs waiting to run will sit \"in the queue.\" They will be shown to have a \"Reason\"</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#tools-for-pending-jobs","title":"Tools for pending jobs","text":"<p><code>squeue --me</code></p> <p><code>squeue --me --start</code></p> <p><code>scontrol show job</code> - this only works for pending and running jobs.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#running-jobs","title":"RUNNING JOBS","text":"","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#how-to-get-information-about-running-jobs","title":"How to get information about running jobs?","text":"<p>If you want or need to get more information about your jobs, you should add \"instrumentation\" to them. Instrumentation is any of a number of techniques which gather and save or print information for you to inspect. A trivial example is adding a command in your batch file script that echoes \"I'm running on node\" and then runs the hostname command.</p> <ol> <li><code>scontrol show job</code></li> <li>Look at their output and error files</li> <li>Look at files they are writing to</li> <li>Use <code>sstat</code> to inspect parameters SLURM has collected for the job</li> <li>Use <code>sattach</code> to connect to the input/output of your script on the leading node of a job. (For advanced users. Using the <code>tail -f</code> command on job output files as the job runs is probably much more useful.)</li> <li>Log into a node and inspect system state with commands like <code>ps</code> or <code>htop</code></li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#completed-jobs","title":"COMPLETED JOBS","text":"<p>Here we mean jobs that are no longer running, whether they succeeded or failed for some reason.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#how-to-get-information-about-completed-jobs","title":"How to get information about completed jobs?","text":"<p>If you want or need to get more information about your jobs, you should add \"instrumentation\" to them. Instrumentation is any of a number of techniques which gather and save or print information for you to inspect. A trivial example is adding a command in your batch file script that echoes \"I'm running on node\" and then runs the hostname command.</p> <ol> <li>Look at their output and error files</li> <li>Look at files they wrote to</li> <li>The command <code>seff jobid</code> for a given jobid will report on RAM and CPU efficiency. </li> <li>The <code>reportseff</code> command provides a way to inspect multiple jobs that match some criteria. See our page about it.</li> <li>Use <code>sacct</code> to inspect parameters about the job, including their \"exitcode\", memory usage, nodes used, etc. See our page about it.</li> </ol> <p>Exit codes are poorly documented, unfortunately. See this section of the sacct tips page for what is known.</p> <p><code>scontrol show job</code> doesn't work for completed jobs.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#investigating-failures","title":"INVESTIGATING FAILURES","text":"<p>Authoring Note</p> <p>This might warrant a dedicated page. THERE IS IMPORTANT INFO TO ADD -- regarding techniques, approaches. </p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#exit-codes","title":"Exit codes","text":"<p>In addition to the STATE of jobs and job steps, you can inspect the EXITCODE and a probably rarely useful DERIVEDEXITCODE fields with the <code>sacct</code> command.</p> <p>The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon).</p> <p>Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. ONLY THE STATUS OF THE LAST COMMAND EXECUTED IS USED TO DETERMINE JOB SUCCESS OR FAILURE.</p> <p>The echo command will produce a misleading job status for your-main-command</p> <pre><code>#!/bin/bash\n...\n...\nyour-main-command &lt; inputfile &gt; outputfile\necho \"job finished\"\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#investigating-resource-consumption","title":"INVESTIGATING RESOURCE CONSUMPTION","text":"<p>Authoring Note</p> <p>This might warrant a dedicated page. THERE IS IMPORTANT INFO TO ADD -- regarding techniques, approaches. </p> <p>People are primarily interested in reducing the amount of memory required to run a job, as memory contention is normally high in the cluster.</p> <p>Until this section is further developed, see the sections below on the seff, sstat, and sacct commands.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#tool-set","title":"TOOL SET","text":"<p>Here is a list of tools and techniques. You may use the same command to look at jobs in different states so we've gathered details here rather than repeating them in every section above.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#output-and-error-files","title":"Output and error files","text":"<p>By default your job will create, in the same working directory where your job was submitted, an output file named \"slurm-%j.out\" (where the \"%j\" is replaced by the job ID) containing both job output and error messages.</p> <p>You can inspect this file during job execution. The <code>tail</code> command has a useful option <code>-f</code> which will print new lines as they appear.</p> <p>You can direct SLURM to seperate normal output from error messages by specifying   the directives <code>-o</code> or <code>--output</code> and <code>-e</code> or <code>--error</code>. These directives also allow you to specify the desired location of these file(s) elsewhere and change their names.</p> <p>For job arrays, the default file name is \"slurm-%A_%a.out\", \"%A\" is replaced by the job ID and \"%a\" with the array index. </p> <p>If you add <code>echo</code> statements in your job batch file at different points, then you can inspect the job output/error files for clues as to its status/progress.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#sview-a-gui-interface","title":"sview, a GUI Interface","text":"<p>Sview is an X11 program which will display information about the cluster. You can see information about jobs, nodes, partitions, and reservations.  Double-clicking on an object usually launches a pop-up window with more details.</p> <p>It is not the slickest interface, and has quirks. It does not remember all of your choices for the basic items to display, for example (click on the tab labeled \"visible tabs\" to check the ones you want to see). Hopefully some of these issues will be addressed when we upgrade to a newer version of SLURM.</p> <p>Explore the options available in various menus. Double-click on various objects to see what happens.</p> <p>The <code>sview</code> manual page is available [here](https://slurm.schedmd.com/archive/slurm-22.05.9/sview.html</p> <p>Warning</p> <p>Do NOT set its refresh interval to 1 second. It defaults to 5 seconds, which is already quite bad enough. Sview contacts the master SLURM daemon running the entire cluster every interval seconds and requests a ton of information. If enough people run it, it will slow down processing of everyone's jobs and accounting records and ... Do you really need to know about changes every five seconds? How will that change what you do?</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#email-notification","title":"Email Notification","text":"<p>You can direct SLURM to send you email when various things happen with your job. These directives can be given on the command line, in batch job scripts, and set in your SLURM defaults file. You can even modify running jobs to set or change their notification settings (see the scontrol tips page).</p> <p>Warning</p> <p>Take care not to cause a storm of outgoing email from our cluster!!! This will lead to our server being blacklisted by Hopkins and/or other mail administrators. Then NO ONE will get email until we can convince them that it won't happen again.</p> <p>By default email notifications are sent for entire job arrays, not individual tasks. Be VERY careful if you change that behavior.</p> <p>The mail arguments are shown in the sbatch manual page.</p> Specify your email address<pre><code>--mail-user=&lt;email-address&gt;\n</code></pre> Specify notification events<pre><code>--mail-type=&lt;list-of-types&gt;  # comma-separated\n</code></pre> Here are the main types: <ul> <li>NONE, BEGIN, END, FAIL, INVALID_DEPEND</li> <li>ALL (equivalent to BEGIN, END, FAIL, INVALID_DEPEND, REQUEUE, and STAGE_OUT)</li> <li>TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit)</li> </ul>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#squeue","title":"squeue","text":"<p>squeue - Display information about pending and running jobs.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#sstat","title":"sstat","text":"<p>Display process statistics of a running job/step. Some of the advice given for <code>sacct</code> (below) applies to <code>sstat</code>.</p> <p>The manual page is at https://slurm.schedmd.com/archive/slurm-22.05.9/sstat.html</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#scontrol-show-job","title":"scontrol show job","text":"<p>The <code>scontrol</code> command has many uses. Before a job ends you can get detailed information with <code>scontrol</code>.</p> <p>Yay</p> <p>We have written a document describing frequent uses of scontrol. See this page.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#sacct","title":"sacct","text":"<p><code>sacct</code> - Display accounting data for running and completed jobs in the Slurm database.</p> <p>Yay</p> <p>We have written a document describing key elements of using sacct. See this page.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#seff-slurm-efficiency","title":"<code>seff</code> (Slurm EFFiciency)","text":"<p>Warning</p> <p>seff output might not be correct for some jobs. If you need to double-check its output, gather the raw info with sacct.</p> <p>CPU efficiency indicates how frequently allocated CPUs were busy. The more CPU-bound a job is, the closer to 100% this will be. The more input/output-bound a job is, the closer this will be to 0.</p> <p>Memory efficiency indicates the proportion of requested memory used at the high-water mark. Values over 100% are not uncommon. It seems that such cases represent cases where the way in which shared memory (such as dynamically-linked code libraries) are counted, perhaps against multiple threads running on one CPU.</p> <pre><code>Job ID: 3946128\nArray Job ID: 3560537_102\nCluster: jhpce3\nUser/Group: jmuschel/biostats\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 2-12:47:05\nCPU Efficiency: 99.94% of 2-12:49:12 core-walltime\nJob Wall-clock time: 2-12:49:12\nMemory Utilized: 15.34 GB\nMemory Efficiency: 76.71% of 20.00 GB\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#reportseff","title":"<code>reportseff</code>","text":"<p>The <code>reportseff</code> command provides a way to inspect multiple jobs that match some criteria. See our page about it.</p>","tags":["in-progress","slurm"]},{"location":"slurm/monitoring/#suggestions-to-a-user","title":"**Suggestions to a user","text":"<p>**</p> <p>Note</p> <p>The following was sent to a user who was having SAS jobs fail with errors indicating a problem with space or quota.</p> <p>Instrumenting your job means adding commands to it to gather information. Running df commands to check the size and capacity of all or specific file systems. (df -h -x tmpfs might be a helpful variant). As mentioned in the orientation material, the sstat command can gather information for running jobs (as opposed to sacct which is oriented towards completed jobs). Some of the output fields available via sstat and sacct relate to memory usage while others can reveal disk input/output volumes.</p> <p>Because your batch job script might not be able to run sstat\u2019s during SAS\u2019s execution, you may need to run the information-gathering commands interactively or via a second batch job. (I mean your batch job could run commands before or after invoking SAS, unless they put the SAS program into the background they won\u2019t be able to run the commands while SAS is running. I don\u2019t know what happens if one tries to run a program in the background (by putting an &amp; after it, like one can run thunar for example) inside of a SLURM batch job.</p> <p>Your gathering might involve the sleep command in between invocations. I would suggest also running the date command so you have timestamps.</p> <p>You could put the sleep, date, sstat commands inside of a while (true) loop. Then cancel the loop with a control c or scancel depending on whether they are running interactive or batch. Redirecting the output to text files would be useful.</p> <p>So your information-gathering efforts should probably start by running some sacct commands to inspect the parameters of previous failed jobs, and perhaps even the ones that succeeded to see if any differences appear.</p> <p>The -e flag is available to both sstat and sacct to show the fields available. The sets overlap but also differ.</p> <ol> <li> <p>Making sure your jobs request the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more efficiently, and in turn get more work done more quickly.\u00a0\u21a9</p> </li> <li> <p>The batch script that you have created to run your commands.\u00a0\u21a9</p> </li> <li> <p>The external step is the connection SLURM makes to the compute node to begin executing your job.\u00a0\u21a9</p> </li> </ol>","tags":["in-progress","slurm"]},{"location":"slurm/node-features/","title":"Node Features","text":"<p>Warning</p> <p>For advanced users who want the highest performance at the expense of possibly waiting longer for jobs to start and maintaining multiple versions of code. We cannot advise you on building optimized code.</p> <p>JHPCE has a wide variety of compute nodes. Some have GPU processors, some have large amounts of memory, and there are many models of CPUs present. We tend to keep equipment running as long as possible.</p> <p>Therefore there is a wide variety of machine configurations, including the kinds of CPUs and the speed of data buses between the CPU and other components such as memory. As an example, our oldest machines use DDR3 RAM while our latest use DDR5 RAM (our oldest node has RAM configured to run at 1333 MT/s versus 4400 MT/s on our very newest node). Generally speaking, the later the machine the faster a job (especially those which are CPU-bound (don't do a lot of network I/O)) job can run.  (Of course the load on a node from other people's jobs will impact the performance of your job.)</p> <p>SLURM allows for the assignment of keywords to nodes so that users can guide their jobs to appropriate machines. This is, of course, in addition to the other methods of indicating resource needs, such as desired partition(s), amount of memory, and the need for a GPU.</p> <p>If you believe that your code will gain a lot from being run on specific kinds of CPUs, you can compile it with optimization flags to perform well on them. Of course, you have then created binaries which might fail to run altogether on older nodes. Please name your binaries in a way which makes their special status apparent to you down the road, in case you forget. That might be a challenging puzzle to figure out.</p> <p>One could, at run-time, write batch job scripts which check the features of the node they were assigned, and choose to execute code highly-optimized for that node's CPU.</p> <p>Tip</p> <p>See most available tag info for the cluster's node with <code>sinfo -o \"%25N %50f %20G\" | less</code></p> Click to see feature list as of 20240926: <pre><code>NODELIST                  AVAIL_FEATURES                                     GRES             \ncompute-[063,070]                   amd,opteron,bdver2,mem500                                        (null)\ncompute-[113-115]                   intel,skylake-avx512,mem750                                      (null)\ncompute-116                         intel,skylake-avx512,mem500                                      (null)\ncompute-[124,127]                   amd,znver3,mem1000                                               (null)\ncompute-152                         amd,znver3,mem250                                                (null)\ncompute-[154-169]                   intel,sapphirerapids,mem1000                                     (null)\ncompute-[119-122]                   amd,znver2,mem1000                                               (null)\ncompute-129                         intel,icelake-server,mem2000                                     (null)\ncompute-[140-151,153]               intel,icelake-server,mem500                                      (null)\ncompute-[053-054]                   amd,opteron,bdver1,mem500                                        (null)\ncompute-[057-058,062,069]           amd,opteron,bdver2,mem250                                        (null)\ncompute-[065,067-068]               amd,opteron,bdver2                                               (null)\ncompute-[089-092,094-096,098-100]   intel,haswell                                                    (null)\ncompute-[093,097]                   intel,haswell,mem250                                             (null)\ncompute-[101-103,105-112]           intel,broadwell                                                  (null)\ncompute-123                         intel,cascadelake,gpu,tesv100,gpu-32gb,mem750                    gpu:tesv100s:4\ncompute-170                         intel,sapphirerapids,gpu,tesh100,gpu-96gb,mem1000                gpu:tesh100:2\ncompute-[126,128]                   intel,icelake-server,gpu,tesa100,gpu-80gb,mem500                 gpu:tesa100:4\ncompute-117                         intel,skylake-avx512,gpu,tesv100,titanv,gpu-11gb,gpu-32gb,mem250 gpu:tesv100:2,gpu:ti\ncompute-118                         intel,cascadelake,gpu,tesv100,gpu-32gb,mem750                    gpu:tesv100:4\ncompute-125                         intel,icelake-server,gpu,tesa100,mem1000,gpu-80gb                gpu:tesa100:4\ntransfer-01                         intel,ivybridge,transfer                                         (null)\n</code></pre>","tags":["slurm"]},{"location":"slurm/node-features/#current-tag-categories","title":"Current tag categories","text":"<p>Currently we have defined features for:</p> <ul> <li>CPU manufacturer (intel,amd)</li> <li>CPU line (e.g. Opteron)<sup>1</sup></li> <li>CPU cpu-type architecture as used by the gcc compiler (see below)</li> <li>presence of a GPU</li> <li>GPU model</li> <li>GPU with memory amount</li> <li>RAM amount in 250Gb increments at 500 and above<sup>1</sup></li> </ul>","tags":["slurm"]},{"location":"slurm/node-features/#using-features-with-slurm-jobs","title":"Using Features with SLURM Jobs","text":"<p>You can tell the scheduler that you prefer or require features using the <code>--prefer=string</code> or <code>--constraint=string</code> when using <code>srun</code> or <code>sbatch</code> or <code>salloc</code></p> <p>Of course, saying that you prefer a feature means that your job might be sent to nodes which lack that feature. If your code requires a feature, then this will mean that your job has fewer nodes on which it might run and might stay pending for longer.</p>","tags":["slurm"]},{"location":"slurm/node-features/#logical-operators","title":"Logical operators","text":"<p>See the <code>--prefer</code> and <code>--constraint</code> sections of the sbatch manual page for descriptions of using AND and OR operators to combine features.</p>","tags":["slurm"]},{"location":"slurm/node-features/#excluding-nodes","title":"Excluding Nodes","text":"<p>Sadly it does not seem possible to simply exclude features (so you could say I want any node except ones with these features (because they are too old for my code)). However, you can tell <code>srun</code> and <code>sbatch</code> to exclude one or more hosts with the <code>--exclude=</code> directive. You can provide a comma-seperated list of hostnames or a text file name (although this file name needs to include a / in it).</p> <p>Tip</p> <p>When specifying multiple features, you may need to enclose them in double quotes. The symbols used for AND, OR and number of matches are all ones which the bash shell will mis-interpret unless told to not process them.</p>","tags":["slurm"]},{"location":"slurm/node-features/#viewing-features","title":"Viewing Features","text":"<p>These features keywords are up to systems administrators to choose and define by adding \"Features=string1,string2\" to \"NodeName=\" entries in <code>/etc/slurm/nodes.conf</code></p> <p>For any particular node, you can see features information with the command <code>scontrol show node nodename</code></p> <p>If you specified a feature for a job, it will appear in the output of <code>scontrol show job jobid</code></p>","tags":["slurm"]},{"location":"slurm/node-features/#cpu-related-feature-info","title":"CPU-Related Feature Info","text":"","tags":["slurm"]},{"location":"slurm/node-features/#manufacturer","title":"Manufacturer","text":"<p>JHPCE has CPUs made by both Intel and AMD.</p> <p>So one feature that has been defined for our nodes is the CPU manufacturer: <code>amd</code> or <code>intel</code></p>","tags":["slurm"]},{"location":"slurm/node-features/#instruction-set-extensions","title":"Instruction Set Extensions","text":"<p>As time has passed, computer science has progressed and users have needed new kinds of computation to be as fast as possible, CPU manufacturers have decided that it is worth extending the assembly language used in central processing units because there were significant gains for notable use cases. Nothing is faster than silicon inside of a CPU core, so moving frequent calculations into CPU designs provides notable performance gains for some code sets.</p> <p>The x86 instruction set is used on all JHPCE nodes. It has a number of extensions. The x86 Wikipedia page includes pointers to them.</p> <p>Extensions are listed as \"flags\" when you look at the output of <code>less /proc/cpuinfo</code> on a particular node.</p>","tags":["slurm"]},{"location":"slurm/node-features/#cpu-types","title":"CPU \"Types\"","text":"<p>Instead of listing all of the extensions as features, we instead use cpu-type as defined by the authors of gcc.</p> <p>The GNU C/C++ compiler suite is the most commonly used one for those languages. See this page for cpu-type details, including which extensions each type represents.</p> <p>This command was run on each node to determine its cpu-type:</p> <p><code>/usr/bin/gcc -march=native -Q --help=target | grep -- '^[ ]*-march' | cut -f3</code></p>","tags":["slurm"]},{"location":"slurm/node-features/#building-optimized-code","title":"Building Optimized Code","text":"<p>That is beyond the scope of this document.</p> <p>\u201cmtune\u201d and \u201cmarch\u201d are important gcc flags to investigate.</p> <p>The various levels of optimization requested via <code>-O#</code> might also impact the flexibility of the binaries you build.</p> <p>Note</p> <p>If you find good documents that would help other users optimize their binaries, please let us know at the bitsupport address.</p> <p>If you want to create batch scripts which can switch between binaries at run-time, there are several techniques you can try: 1)  If you use <code>-\u2014constraint</code> and your job starts, then (we believe) it will be running on the requested feature. (We haven\u2019t tested to ensure that that is 100% respected. It\u2019s supposed to be (as opposed to using <code>-\u2014prefer</code>). Therefore you can also pass a variable into your job with the <code>-\u2014export</code> flag as explained in this page 2)  If you  do some output munging with regular expression matching, you can get the info out of your environment at runtime. You DO see the specified feature in the output of <code>scontrol show job $SLURM_JOBID</code> as <code>Features=amd</code> But you have to search for it in the output AND it only shows you the feature you specified, not all of those applicable to that node. 3) You could also examine the available features of the node on which your shell is running in the <code>ActiveFeatures</code> attribute in the output of <code>scontrol show node $SLURMD_NODENAME</code></p>","tags":["slurm"]},{"location":"slurm/node-features/#cluster-composition-by-cpu-type","title":"Cluster Composition By CPU-type","text":"<p>In August 2024, the cluster compute nodes consisted of approximately, in family name alphabetical order</p> <pre><code>      3 bdver1\n      7 bdver2\n     10 broadwell\n      2 cascadelake\n     11 haswell\n     17 icelake-server\n     16 sapphirerapids\n      5 skylake-avx512\n      4 znver2\n      2 znver3\n</code></pre> <p>As of August 2024 the oldest family is \"bdver1\" and the newest is \"sapphirerapids\".</p> <p>The bdver and znver types represent AMD CPUs.</p> <p>Perhaps the largest delta in compiler flags is between bdver1 and the rest. bdver1 does not support these gcc flags:   -mbmi   -mf16c   -mfma   -mdirect-extern-access</p> <ol> <li> <p>It's not clear that these tags add much value. The RAM tags were thought to perhaps aid someone trying to increase memory locality. SLURM's scheduler, will, of course, allocate nodes with enough free memory to meet the jobs requirements.\u00a0\u21a9\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/partitions/","title":"Partitions","text":"<p>A partition is a logical collections of nodes that comprise different hardware resources and limits to help meet the wide variety of jobs that get scheduled on the cluster. Nodes can belong to more than one partition.</p> <p>There are several types of partitions:</p> <ul> <li>General access (e.g. shared, interactive, gpu, transfer)</li> <li>Application only (e.g. sas)</li> <li>GPU (equipped with GPU cards)</li> <li>PI-owned (for use only by members of the PI's group)</li> </ul>","tags":["slurm"]},{"location":"slurm/partitions/#choosing-partitions-for-your-jobs","title":"Choosing Partitions For Your Jobs","text":"<p>You should only submit jobs to partitions that you are entitled to use.</p> <p>Jobs can be submitted to multiple partitions to increase the odds that they will start more quickly. They will generally start in the first partition that has the required resources. This is mainly of use to members of PI partitions where their PI partition member nodes are busy at the moment, or they need maximum resources to meet a deadline.</p> <p>Simply separate partition names with commas (and no spaces!!). For example, in a batch job file:</p> <pre><code>`#SBATCH --partition=cancergen,shared`\n</code></pre> <p>There are a number of ways in which partitions are configured which may nudge a pending job to one partition over others. Pending jobs in some partitions are evaluated ahead of others. Priorities can be set on some partitions (such as <code>interactive</code>) as described here.</p> <p>Array jobs seem to be able to dispatch child jobs to any of the partitions specified. We are not 100% sure of this -- they might all be run into the first partition chosen. (If the latter is true, you may not want to tie your array completion to a partition that has many fewer resources than your other option(s).)</p>","tags":["slurm"]},{"location":"slurm/partitions/#pi-partitions","title":"PI Partitions","text":"<p>JHPCE exists because Primary Investigators worked together to create a cluster. They share their resources via public partitions (see cost recovery descriptions here and here).</p> <p>Only submit jobs to these partitions if you are a member of the Primary Investigator's research groups or have been given explicit permission to do so. If you are in doubt, ask before submitting. Jobs from non-group members will be killed and repeated abuse will lead to repercussions.  </p>","tags":["slurm"]},{"location":"slurm/partitions/#public-partitions","title":"Public Partitions","text":"<p>Partitions shared, interactive, interactive-larger, gpu, sas, scavenge and transfer are considered public and available to all.</p> <p>Specific use</p> <p>Only jobs which require the use of GPU cards should be submitted to the gpu partition.</p> <p>Only jobs which require the use of the SAS application should be submitted to the sas partition.</p> <p>Only jobs related to transferring data into or out of the cluster should be submitted to the transfer partition.</p> <p>The public partitions provide low-priority access to unused capacity throughout the cluster. Capacity on the shared queue is provided on a strictly \u201cas-available\u201d basis and serves two purposes:</p> <p>First it provides surge capacity to stakeholders who temporarily need more compute capacity than they own, and second, it gives provides non-stakeholders access to computing capacity.</p> <p>Scheduling polices attempt to harvest unused capacity as efficiently as possible while mitigating the impact on the rights of stakeholders to use their resources. The JHPCE service center does not guarantee that stakeholders will provide sufficient excess capacity to meet non-stakeholders needs, however in practice the cluster is rarely operating at full capacity so there is usually ample computing capacity on the shared queue.</p>","tags":["slurm"]},{"location":"slurm/partitions/#getting-info-about-partitions","title":"Getting Info About Partitions","text":"<p>Our command <code>slurmpic</code> shows information about partitions, including the member nodes, their current utilization, and some summary statistics.<sup>1</sup> Text is color-coded to try to indicate how fully consumed nodes are. By default it displays the shared partition. Specific partitions can be displayed using <code>slurmpic -p partitionname</code>. All of the nodes in all of the GPU partitions can be displayed with <code>slurmpic -g</code>. Individual GPU-containing partitions can be shown with <code>slurmpic -g -p partitionname</code> Important: Run <code>slurmpic -h</code> to see important usage notes!</p> <p>The best way to see the configuration of a partition is with the scontrol command.  <pre><code>scontrol show partition partitionname\n</code></pre> (For tips about using <code>scontrol</code>, see our local scontrol tips page.)</p> <p><code>slurmpic</code> is based on the SLURM command <code>sinfo</code>, which shows information about nodes and partitions. It has many options, so you can also use it to see information about nodes. (Note: Partitions which require group membership to submit to are only visible via <code>sinfo</code> to members of those groups. Because the local command <code>slurmpic</code> uses <code>sinfo</code> to retrieve information, the output of <code>slurmpic -a</code> (show all nodes) will omit those private PI partitions' nodes.)</p>","tags":["slurm"]},{"location":"slurm/partitions/#cpu-partitions","title":"CPU Partitions","text":"","tags":["slurm"]},{"location":"slurm/partitions/#public-cpu-partitions","title":"Public CPU Partitions","text":"<p>The CPU and RAM limits for <code>shared</code> change depending on node availability. We change the QOS <code>shared-default</code> up and down based on usage over the past month or two. We try to keep limits as high as possible to increase the utilization rate of the cluster while not allowing one person to dominate the <code>shared</code> partition. You can learn more about QOS here and see the currently defined QOS values with the command <code>showres</code></p> <p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Core RAM Time Notes/Use shared public 400 2.5TB (1d/90d) DEFAULT interactive public 2 20gb (1d/90d) Small but accessible interactive-larger public 8 80gb (1d/5d) Limit 2 jobs per user gpu public (none) (none) (1d/90d) Only for GPU jobs sas application (none) (none) (none/90d) Licensed for SAS scavenge public 11 per job 250G per job (1d/5d) See below transfer public (none) (none) (none/90d) Data in or out of cluster via SLURM jobs <p>To reduce table width, column names are terse.</p> <p>Experimental <code>interactive-larger</code> partition: Our interactive partitions give jobs a higher priority and are evaluated before those in other partitions. This \"larger\" one was created to try to allow people to run interactive jobs a bit larger in size than our smaller <code>interactive</code> partition. This is meant to help you explore data visually, debug a batch script that has failed by running one command at a time (on a small-enough data set), etc. WE MAY NEED TO REMOVE THE PARTITION OR MODIFY THE ALLOWED PARAMETERS. 20250220</p> <p>Experimental <code>scavenge</code> partition: This was created to try to \"harvest\" some usually-unused resources from some specific nodes. You can specify it along with your normal partition, e.g. <code>--partion=shared,scavenge</code> and it will be used if available. If your job's CPU, RAM, or duration parameters exceed those set for <code>scavenge</code> then your job will run on the other partition(s) you specified. WE MAY NEED TO REMOVE THE PARTITION OR MODIFY THE ALLOWED PARAMETERS. 20250220</p>","tags":["slurm"]},{"location":"slurm/partitions/#pi-cpu-partitions","title":"PI CPU Partitions","text":"<p>To see the member nodes and resources of the any partitions, use <code>slurmpic -p partitionname</code></p> <p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Core RAM Time Notes/Use bader PI (none) (none) (none/90d) cancergen PI (none) (none) (none/90d) caracol PI (none) (none) (none/90d) UNIX group cee PI (none) (none) (none/90d) cegs2 PI (none) (none) (none/90d) chatterjee PI (none) (none) (none/90d) echodac PI (none) (none) (none/90d) gwas PI (none) (none) (none/90d) not yet defined hl PI (none) (none) (none/90d) hl = hearing loss hongkai PI (none) (none) (none/90d) katun PI (none) (none) (none/90d) UNIX group mommee PI (none) (none) (none/90d) stanley PI (none) (none) (none/90d) UNIX group sysadmin admin (none) (none) (none/90d) For system testing","tags":["slurm"]},{"location":"slurm/partitions/#gpu-partitions","title":"GPU Partitions","text":"<p>To see the member nodes and resources of the any partitions, use <code>slurmpic -g</code></p> <p>To learn more about the GPU card types and how to use them, see https://jhpce.jhu.edu/gpu/gpu-info/</p> <p>The Biostatistics partions are for anyone who is sponsored by a PI in that department.</p> <p>Limits for CPU cores, RAM and Time (default/maximum)</p> Name Type Requires Approval Core RAM GPU Time Notes/Use gpu public no (none) (none) (none) (1d/90d) ONLY public GPU resource caracol PI yes (none) (none) (none) (none/90d) Lieber neuron PI yes (none) (none) (none) (none/90d) bstgpu PI yes (none) (none) (none) (none/90d) bst=biostatistics bstgpu2 PI yes (none) (none) (none) (none/90d) bst=biostatistics bstgpu3 PI yes (none) (none) (none) (none/90d) bst=biostatistics <ol> <li> <p>Note that the statistics displayed are for that partition, not the whole cluster. Also, memory and CPU use of nodes that are DOWN or in DRAIN are not included in the stats.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/qos/","title":"Quality of Service","text":"<p>Systems administrators can define resource limits called QOS and assign them to a variety of objects, mainly users and job partitions. We use them to share resources equitably and to allow exceptions.</p> <p>For example, we have a QOS named <code>shared-default</code> which is normally set to allow a user to use 100 CPU cores and 1TB of RAM at any one time in the <code>shared</code> partition.  These values were chose to represent roughly 20% of those available in that partition. When Primary Investigators who own nodes need to remove them from the shared partition for their own use, that QOS' definition can be changed to a lower value to maintain the 20% goal.</p> <p>Vendor QOS documentation about them can be found here. There are also entries in the manual pages for various SLURM commands about QOS<sup>1</sup>.</p> <p>QOS definitions for users and partitions are stored in a database.</p> We have a document containing useful QOS-related commands. Those commands include ones like this one which allow you to see the value of all defined QOS in a readable format: <code>sacctmgr show qos format=Name%20,Priority,Flags%30,MaxWall,MaxTRESPU%20,MaxJobsPU,MaxSubmitPU,MaxTRESPA%25</code>","tags":["done","slurm"]},{"location":"slurm/qos/#using-qos","title":"Using QOS","text":"<p>If you need to submit a job with a particular QOS, AND that QOS is in both your and the specified partition's AllowedQOS list (see below sections), then you need to add an extra SLURM directive in your batch job or on the command line.</p> <p><code>sbatch --qos=shared-200-2 myjobscript</code></p> <p>You can modify a pending job with jobid# with this command:</p> <p><code>scontrol update job jobid# qos=shared-200-2</code></p> <p>Specifying a QOS that you do not have access to or which your specified partition does not allow will cause the job to be rejected immediately. Pending jobs which specify a QOS that you lose access to will probably fail the next time the scheduler inspects them. If not then, then when they are otherwise ready to be started.</p>","tags":["done","slurm"]},{"location":"slurm/qos/#partition-qos","title":"Partition QOS","text":"<p>Job partitions like <code>shared</code> have two QOS-related attributes:</p> <ol> <li> <p>Qos - If present, this specifies the QOS which by default applies to all jobs submitted. The <code>interactive</code> partition has <code>QoS=interactive-default</code></p> </li> <li> <p>AllowQoS - By default, this value is set to ALL. If set to a comma-separated list, then only those can be used or requested by users. The <code>interactive</code> partition has <code>AllowQos=normal,interactive-default</code></p> </li> </ol> <p>You can see the configuration of a partition with the scontrol command. (Vendor's scontrol manual page. Our local scontrol tips page.) Here is a command which will show you the QOS attributes on an exmple partition named partitionname</p> <pre><code>scontrol show partition partitionname | grep -i qos\n</code></pre>","tags":["done","slurm"]},{"location":"slurm/qos/#user-qos","title":"User QOS","text":"<p>User accounts have two QOS-related attributes:</p> <ol> <li> <p>Qos - Our users (currently) all have a QOS value of \"normal\", (which is inherited from their parent account, named \"jhpce\").  The \"normal\" QOS  (currently) has no limits defined for it.</p> </li> <li> <p>Allowed Qos - By default, this value is empty. If set to a comma-separated list, then the user can choose to submit jobs using one of them.  Jobs request a QOS using the \"--qos=\" option to the sbatch, salloc, and srun commands. (If the partition does not allow a QOS to be used, then your job will be rejected.)</p> </li> </ol> <p>See your own user database values: <code>sacctmgr show user withassoc where name=your-cluster-username</code></p> <p>The SLURM FAQ includes an entry about the error you receive if you ask for more RAM in a single job than is allowed. </p> <p>If you submit jobs which together ask for more memory than you are allowed to use at one time, then ones that \"fit\" inside the limit will run and the remaining jobs will be waiting in PENDING state. </p> <p>The \"Reason\" code shown in the output of <code>squeue --me -t pd</code> (\"show me my pending jobs\") will be <code>QOSMaxMemoryPerUser</code>for jobs waiting for your running jobs to be using less than the RAM limit defined in the QOS that is impacting you.  The Reason will be <code>QOSMaxCpuPerUserLimit</code> for jobs waiting for your core usage to drop below that which is allowed.</p>","tags":["done","slurm"]},{"location":"slurm/qos/#how-we-are-using-qos-to-date","title":"HOW WE ARE USING QOS TO DATE...","text":"<p>This is a summary of how QOS is configured at JHPCE.</p> <p>Our users all have a QOS value of \"normal\", which is inherited from their parent account, named \"jhpce\"</p> <p>(The account jhpce, of the organization jhpce, in the cluster jhpce3 is the parent of all of the users.)</p> <p>The \"normal\" QOS  (currently) has no limits defined for it. (It is the default QOS defined in SLURM. We didn't create it.)</p> <p>A fundamental element of our current practice is that the user\u2019s default QOS entries in the database are \"normal\".</p> <p>We\u2019ve defined additional ones and, for some users, changed the Allowed QoS field in their user entries to list ones that they can optionally use in addition to \"normal\". </p> <p>Those additional ones are being applied</p> <ol> <li>via per-partition QOS= definitions in /etc/slurm/partitions.conf     (e.g. QOS \"interactive-default\", \"shared-default\"), and </li> <li>via users specifying additional ones via per-job     directives (b/c we granted them access to them and     told them that they needed to use those directives).     (Users do not have to specify optional QOS on every     job thenceforward. Users can pick and choose what QOS to use for each job.</li> </ol> <ol> <li> <p>Capitalization doesn't matter when specifying QOS in commands.\u00a0\u21a9</p> </li> </ol>","tags":["done","slurm"]},{"location":"slurm/ram/","title":"Memory Usage","text":""},{"location":"slurm/ram/#requesting-additional-memory","title":"Requesting Additional Memory","text":"<p>All jobs need to use a certain amount of Memory or RAM (We use the terms Memory and RAM interchangeably) in order to run. By default on the JHPCE cluster when you submit a job with sbatch, or run srun, you are allotted 5GB of RAM  for your job. </p> <p>If you are running a larger job and need more than the 5GB of RAM, you can request more (or less) RAM  via 2 options: <pre><code>    --mem : memory per node (for all cores used)\n    --mem-per-cpu : memory per core (harder to accurately estimate)\n</code></pre> Some examples of using these options: <pre><code>sbatch --mem=10G job1.sh\n</code></pre> - This would give your batch job a total of 10GB of RAM <pre><code>srun --mem-per-cpu=5G --cpus-per-task=4 --pty --x11 bash\n</code></pre> - This would give your interactive session a total of 20GB of RAM and 4 cores</p>"},{"location":"slurm/ram/#estimating-ram-usage","title":"Estimating RAM usage","text":"<p>There is, sadly, no easy formula to know ahead of time how much RAM a job will need when working with large data. Every program has its own way of dealing with RAM. It may sometimes be an iterative process to determine the best value to use for your jobs.  Here are some tips to help find good values for your job:</p> <ul> <li>You can run a test job on a small subset of data.  From here you should be able to extrapolate the amount of RAM your full job will need.</li> <li>One good place to start is to look at the size of the files you will be reading in. Add a bit extra, as a starting point.  So if your job is reading in a 20GB image file, you may want to ask for 25GB or RAM.</li> <li>You can run sacct to gather info on a completed job: sacct -o JobID,JobName,ReqTRES%40,MaxVMSize,MAXRSS,State%20 -j JOBID</li> <li>If the \"STATE\" from the above command is \"OUT_OF_MEMORY\" it means that you job has run out of RAM, and you will need to resubmit your job with a larger RAM request.</li> <li>Use <code>slurmpic</code> to see the current core and RAM availability, and plan accordingly</li> </ul> <p>Try to make your RAM request slightly higher than your expected usage.</p> <ul> <li>Too low and your job will get killed for exceeding your request</li> <li>Too high and your job may take longer to get scheduled, plus you\u2019ll be squatting on RAM that others can use.</li> </ul>"},{"location":"slurm/slurm-commands-ref/","title":"SLURM COMMANDS","text":"<p>Here is information about SLURM-related commands, whether from the vendor or created by JHPCE staff members. Links are provided to online copies of the manual pages for commands. If we've written a page with advice about using the command, use the (LOCAL TIPS) link.</p> <p>Warning</p> <p>Do not frequently<sup>1</sup> run slurmpic, squeue, sacct or other Slurm client commands using loops in shell scripts or other programs. </p> <p>These commands all send remote procedure calls to slurmctld, the main SLURM control and scheduling daemon, They may also perform look-ups in the accounting database. That process and the database need to be highly responsive to the input/output caused by running jobs.</p> <p>Ensure that programs limit calls to slurmctld to the minimum necessary for the information you are trying to gather. Add arguments to limit to needed partitions or users or job data fields, etcetera.</p>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#locally-written-tools","title":"Locally Written Tools","text":"<p>Many of these bash scripts. You can inspect their contents and if desired, make copies of them for yourself that are modified to suit you. There will also soon be bash routines defined in the default environment via either module files or the /etc/profile.d/ directory.  Most scripts have <code>-h</code> help options which will reveal more usage information.</p> <p>The command <code>which programname</code> is how you find out where it is located, if it is a program. If it is a bash routine, the same command will display the code of the routine. </p>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#information-about-cluster-and-jobs","title":"Information about cluster and jobs","text":"<ul> <li>slurmpic: An essential program for getting cluster status info. Use -h option to see key usage details. By default, with no arguments, it provides info for the <code>shared</code> partition.</li> <li>slurmuser: Provides a per-user list of CPU/RAM in use for running jobs and requested for pending jobs (if any). By default this is for all jobs in all partitions.</li> <li>qoverview: Quick view into number of running, pending jobs on the whole cluster. Also counts the pending jobs in some primary reasons.</li> <li>showjob: Displays job information when given a jobid. Only works for pending or running jobs. Currently simply a shortcut for <code>scontrol show job jobid --details</code> but hopefully in the future will produce more readable output.</li> <li>showqos: Displays list of our QOS definitions in a readable format.</li> <li>showreason: Show the Reason line from <code>showjob</code> for nodes that are in DRAIN or DOWN etc.</li> <li>slurm-hist-all-cores: Histogram of core consumption for whole cluster</li> <li>slurm-hist-all-mem: Histogram of RAM consumption for whole cluster</li> <li>slurm-hist-shared-cores: Histogram of core consumption for only the shared partition</li> <li>slurm-hist-shared-mem: Histogram of RAM consumption for only the shared partition</li> <li>slurmuser: Displays per-user summary usage of RAM &amp; CPU across the cluster. Can display by partition or for a specific user.</li> <li>smem: Displays memory used by your currently running jobs. If given a jobid number, it will display info about the memory usage of that job. (no man page yet)</li> <li>memory reporting script - puts per-user output daily into directories under <code>/jhpce/shared/jhpce/jhpce-log/</code></li> <li>useron: List nodes where a user has running jobs.</li> <li>jobson: Displays running jobs running on a node when given a three digit node number.</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#special-purpose","title":"Special purpose","text":"<ul> <li>timeleft: Produces a SLURM \"time_spec\" in the format DAYS-HH:MM:SS indicatingt time left before an upcoming announced outage. Meaningless if there is not an imminent outage.</li> <li>jobtimeleft: Given a jobid, it uses <code>scontrol</code> to update the job's time limit using <code>timeleft</code>. Useful for jobs that went into pending status instead of running because the user did not specify a job time limit.</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#contributed-slurm-programs-weve-installed","title":"Contributed SLURM Programs We've Installed","text":"<ul> <li>reportseff: (LOCAL TIPS) Very handy tool! Displays efficiency of CPU and RAM usage for jobs, job array elements. Can be given many options to control output.</li> <li>seff: Display efficiency of CPU and RAM usage of a single completed job. (no man page yet)</li> <li>slurm-mail: Tool used to add details to mail sent to you. Not something you can modify. Listed for completeness.</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#provided-with-slurm","title":"Provided with Slurm","text":"<p>All of the manual pages are here, including those for the configuration files found in /etc/slurm/</p>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li>salloc: request an interactive job allocation (doesn't start any processes anywhere)</li> <li>sbatch: submit a batch script to Slurm to create an allocation and run processes</li> <li>srun: launch one or more tasks of an application using allocated resources</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#information-about-cluster-and-jobs_1","title":"Information about cluster and jobs","text":"<p>Warning</p> <p>Do not frequently<sup>1</sup> run slurmpic, squeue, sacct or other Slurm client commands using loops in shell scripts or other programs. </p> <p>These commands all send remote procedure calls to slurmctld, the main SLURM control and scheduling daemon, They may also perform look-ups in the accounting database. That process and the database need to be highly responsive to the input/output caused by running jobs.</p> <p>Ensure that programs limit calls to slurmctld to the minimum necessary for the information you are trying to gather. Add arguments to limit to needed partitions or users or job data fields, etcetera.</p> <p>Some SLURM commands such as sacct and squeue can display a wide variety of information. It can be complex to specify what you want to see and to format it so it is readable. We've tried to document some common choices in the LOCAL TIPS documents. A tip: you  set certain environment variables to specify output arguments instead of providing the arguments on the command line. It can be useful to define these different ways in aliases or shell scripts to format output in ways you need, because simply changing the value of these variables can produce vastly different output for commands like sacct and squeue. Example variables are: SLURM_TIME_FORMAT, SACCT_FORMAT, SQUEUE_FORMAT, SQUEUE_FORMAT2, SQUEUE_SORT. </p> <ul> <li>sacct: (LOCAL TIPS): display accounting data for jobs in the Slurm database</li> <li>sattach: attach to a running job step</li> <li>scontrol: (LOCAL TIPS): display (or modify when permitted) the status of Slurm entities (jobs, nodes, partitions, reservations)</li> <li>sinfo: display node and partition information</li> <li>sprio: (LOCAL TIPS): display the factors that comprise a job's scheduling priority</li> <li>squeue: display the jobs in the scheduling queues, one job per line</li> <li>sshare: display the shares and usage for each charge account and user</li> <li>sstat: display process statistics of a running job/step</li> <li>sview: X11 graphical tool for displaying jobs, partitions, reservations</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#controlling-jobs","title":"Controlling Jobs","text":"<ul> <li>scancel: cancel or pause a job or job step or signal a running job or job step to pause</li> <li>scontrol: (LOCAL TIPS): display (and modify when permitted) the status of Slurm entities (jobs, nodes, partitions, reservations)</li> </ul>","tags":["slurm"]},{"location":"slurm/slurm-commands-ref/#for-systems-administrators","title":"For Systems Administrators","text":"<ul> <li>sacctmgr: (LOCAL TIPS): display and modify Slurm account information</li> <li>scontrol: (LOCAL TIPS): display and modify Slurm jobs and partitions</li> <li>sdiag: display scheduling statistics and timing parameters</li> <li>slurmctld: central management daemon</li> <li>slurmd: client-side daemon</li> <li>sreport: generate canned reports from job accounting data and machine utilization statistics</li> </ul> <ol> <li> <p>Frequently meaning more than once every five minutes. Do you REALLY need to know something sooner than that? If you want to know when a job starts, fails, or finishes, use email notification settings. You can add them to pending and running jobs using scontrol. (See sbatch manual page for possible mail types.)\u00a0\u21a9\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/slurm-faq/","title":"JHPCE SLURM FAQ","text":"<p>The search field on this web site produces good results. Please use it to find answers across all of the documents on the site.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#faqs-from-the-vendor","title":"FAQs from the vendor","text":"<p>https://slurm.schedmd.com/faq.html</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#faqs-from-other-clusters","title":"FAQs from other clusters","text":"<p>Note that the information in these pages will include details which do not apply here in JHPCE. Caveat emptor.</p> <p>CECI</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#where-can-i-learn-more-about-slurm-related-commands","title":"Where can I learn more about SLURM-related commands?","text":"<p>Start with this document which describes commands and provides links to more information. Commands documented include both those written by SLURM's vendor, but also ones we have created.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#what-are-the-default-job-resources","title":"What are the default job resources?","text":"Click to open <p>Jobs that do not specify CPUs, RAM, or duration, or partition receive 1 CPU, 5G of RAM, assignment to the <code>shared</code> partition, and, because shared is a public partition, a time limit of 1 day. </p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#when-will-my-job-start","title":"When will my job start?","text":"Click to open <p>See this document for a description of factors affecting when jobs start.  Of course the load on the cluster impacts job start times. Please consult the output of <code>slurmpic</code> for information about the state of the cluster and its available resources. Note that your job cannot start until a match is found for the resources you specified. There may be a lot of unused CPUs on a node, for example, but if someone has allocated all of the RAM on that node your job won't fit there.</p> <p>If the Reason given for a pending job is <code>QOSMaxMemoryPerUser</code> or <code>QOSMaxCpuPerUserLimit</code> then you can read about Quality of Service in our document here.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-will-i-know-if-my-job-ended","title":"How will I know if my job ended?","text":"<p>See this document for information about monitoring pending, running and completed jobs. You can specify directives requesting email notfication when jobs start, fail, etc.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#help-my-job-is-going-to-run-out-of-time","title":"Help! My job is going to run out of time!","text":"Click to open <p>You can modify the time limit of a pending job, but not one that has begun running. You can email bitsupport and ask system administrators to extend the time limit of running jobs. Please provide the job id number(s) and the new, revised total duration. You can add \"mailtype\" and \"mailuser\" directives when submitting a job, while it is pending, and while it is running. See this document for instructions.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#what-partitions-exist","title":"What partitions exist?","text":"Click to open <p>The default partition is \"shared\". By default <code>slurmpic</code> describes the state of this partition. (Run <code>slurmpic -h</code> to see a list of options, including the flag to show other partitions.)</p> <p>See this document for a description of partitions and their purposes and limits.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-cancel-a-job","title":"How do I cancel a job?","text":"Click to open <p>Use <code>scancel &lt;jobid&gt;</code> where jobid is the number for your job. You can specify multiple jobs in a space-separated list.</p> <p>If your job is a member of a job array, it will have an underscore in it, e.g. 2095_15. You can cancel an array task (2095_15) or the whole job array (2095).</p> <p>If you want to cancel all of your pending jobs, use <code>scancel --me -t PENDING</code>. If you want to cancel all of your jobs, use <code>scancel -u your-username</code></p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-hold-or-modify-a-job","title":"How do I hold or modify a job?","text":"Click to open <p>Note that if you want to modify something about the job, instead of cancelling it and losing any accumulated age priority it has accrued, you can hold the job, modify it and release it with <code>scontrol</code> commands. Only some of the parameters of a job can be modified when it is pending.</p> <p><code>scontrol hold &lt;jobid&gt;</code> where  can be a comma-separated list. <p>If your jobs have names, you can hold using the name. (This will hold all matching jobs.) <code>scontrol hold jobname=&lt;jobname&gt;</code> <code>scontrol update jobid=&lt;jobid&gt; ...</code> See this section of the manual page for a list of attributes which can be modified.</p> <p>We have a number of scontrol examples spelled out for you.</p> <p>You can submit a job directly into a held status if you want to delay its start, using the <code>-H</code> or <code>--hold</code> directive. However, you probably want to instead use the <code>-b</code> or <code>--begin</code> directive to specify a start time. Or, if you want a job to wait until something else finishes, you can make it dependent on the first job. See this document for information about dependent jobs.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-do-i-control-the-order-my-jobs-start","title":"How do I control the order my jobs start?","text":"Click to open <p>You can rank your jobs with the \"nice\" and \"top\" subcommands to <code>scontrol</code>.  See the <code>scontrol</code> tips document. You can also submit jobs with dependency directives and also create heterogenous jobs which spawn other jobs. See our document for information about dependent jobs.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#unable-to-allocate-resources","title":"\"Unable to allocate resources\"","text":"Click to open <p>If the scheduler determines that your job is invalid in some fashion, it will generally reject it immediately instead of putting it into the queue with a pending status. There are a few causes of this. The wording of the error may or may not be clear. Inspect your job directives. Are you asking for more resources than any node can ever provide?</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-to-run-your-job-on-the-fastest-node","title":"How to run your job on the fastest node","text":"Click to open <p>JHPCE keeps nodes running until they die. Therefore there is a wide variety of machine configurations, including the kinds of CPUs and the speed of data buses between the CPU and other components such as memory. As an example, our oldest machines use DDR3 RAM while our latest use DDR5 RAM (our oldest node has RAM configured to run at 1333 MT/s versus 4400 MT/s on our very newest node). Generally speaking, the later the machine the faster a job (especially those which are CPU-bound (don't do a lot of network I/O)) job can run.  (Of course the load on a node from other people's jobs will impact the performance of your job.) We have defined a SLURM parameter for each node which lists its \"features\"  You can specify that your job prefers or requires certain features. (Of course doing that means that it may take longer for your job to start.)Our page describing how to do this includes a list of CPU model family names as used/defined by the GNU compiler, in alphabetical order.</p> <p>Similarly, you can specify that your job only be allowed to run on specific node or nodes by issuing the <code>--nodelist=compute-blah1,compute-blah2]</code> directive to your <code>srun</code> or <code>sbatch</code> commands.  You can also provide a file name to specify a file which contains a list of nodes. See the nodelist info in the sbatch manual page.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#users-group-not-permitted-to-use-this-partition","title":"User's group not permitted to use this partition","text":"Click to open <p>Some of our PI partitions have UNIX groups defined to control who can submit jobs to them. If you are not a member of that group and try to submit a job, you'll see an error like this:</p> <p><code>srun: error: Unable to allocate resources: User's group not permitted to use this partition</code></p> <p>You can see which groups you belong to with the <code>groups</code> command.</p> <p>You can see which group you need to belong to by looking for the partition in question in the file <code>/etc/slurm/partitions.conf</code></p> <p>If you feel like you were not included in a group in error, ask the PI to contact bitsupport@lists.jh.edu asking us to add you. </p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#job-violates-accountingqos-policy","title":"Job violates accounting/QOS policy","text":"Click to open <p>If you ask for more resources than will ever be able to be allocated to you, you might receive one of several error messages.</p> <p>This one appeared when a job asked for more RAM than was allowed by a QOS limit:</p> <p><code>Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)</code></p> <p>But when more CPUs were requested (instead of too much RAM), the error was different:</p> <p><code>Unable to allocate resources: More processors requested than permitted</code></p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#my-job-wont-run-on-a-particular-node","title":"My job won't run on a particular node","text":"Click to open <p>If a node is misconfigured and your job runs on other nodes, then please report the problem to bitsupport@lists.jh.edu (see this document). In the meantime, you can exclude that node from being considered to run your job by supplying the <code>--exclude=nodename1,nodename2</code> directive to your <code>srun</code> or <code>sbatch</code> commands.  You can also provide a file name to specify a file which contains a list of nodes. See the exclude info in the <code>sbatch</code> manual page</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#how-many-jobs-can-i-run-at-a-time","title":"How many jobs can I run at a time?","text":"Click to open <p>As of 20240405 we are limiting the number of simultaneous jobs submitted or running to 10,000.</p> <p>Array jobs are also limited to 10,000 tasks by variable <code>max_array_tasks</code> in /etc/slurm/slurm.conf</p> <p>Total jobs at a single time in the whole cluster is 90,000, which is determined by the variable <code>MaxJobCount</code> in /etc/slurm/slurm.conf</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#the-cluster-is-slower-than-my-laptopdesktop","title":"The cluster is slower than my laptop/desktop!!","text":"Click to open <p>From a purely hardware perspective, this is unlikely. However, it might be true because you ARE using shared and distributed resources. For example, data files are being stored on file servers and used by compute nodes over a network. We use RAID on our file servers to distribute the input/output load over many hard drives, which is usually  faster than what a single hard drive can provide.  We use 40 gigabit-per-second (or faster) network interfaces on our file servers, and 10 gigabit-per-second network interfaces on our compute nodes. But everything your desktop or laptop is doing is local, and dedicated only to you.</p> <p>If your job is running more slowly than you expect, the most common reasons are:</p> <ul> <li>Incorrect/inadequate resource request (you only requested 1 CPU core, or the default amount of RAM)</li> <li>Your code is doing something that you don't know about</li> </ul> <p>A combination of these two factors are almost always the reason for jobs running more slowly on the cluster than on a different resource.</p> <p>One example is Matlab code running on the cluster versus a modern laptop. Matlab implicitly performs some operations in parallel (e.g. matrix operations), spawning multiple threads to accomplish this parallelization. If your resource request on the cluster only includes 1 or 2 CPU cores, no matter how many threads Matlab spawns, they will be pinned to the cores you were assigned.</p> <p>This can lead to a situation where a user's Matlab code is running using all 4 CPU cores on their laptop, but only 1 CPU core on the cluster. This can give the impression that the cluster is slower than a laptop. By requesting 4 or 8 CPU cores in your resource request, Matlab can now run these operations in parallel.</p> <p>It is important to understand what the code or application you are using is actually doing when you run your job</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#x11-related-errors","title":"X11-related errors","text":"<p>See also our general X11 document for other information.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#cant-open-display","title":"Can't open display","text":"Click to open <p>If you are on a compute node in an interactive session and get a message like <code>Error: Can't open display: localhost:12.0</code> when launching an X11 program, something went wrong while other things went okay. (Normally if there is an X11 problem you don't wind up having a DISPLAY variable set.) So far this error has appeared after an <code>srun: error: _half_duplex: read error -1 Connection reset by peer</code> error. See that entry in this FAQ.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#srun-error-_half_duplex","title":"srun: error: _half_duplex","text":"Click to open <p><code>srun: error: _half_duplex: read error -1 Connection reset by peer</code></p> <p>We consider this a SLURM bug in version 22.05.09. It appears when <code>srun</code> is used with the <code>--x11</code> argument. Sometimes immediately, but typically when an X11 program is launched.</p> <p>If you do not intend to run any X11 programs during your interactive session, then you can log out of that session and start a new one without the <code>--x11</code> flag.</p> <p>If you do intend to use X11 programs, when that error appears the only solution we have found is to abandon that whole interactive session to the compute node by typing \u201cexit\u201d to quit the shell. Back on the login node, verify that basic X11 functionality works by starting a simple X11 command, such as the <code>xterm</code> or <code>xclock</code> programs. If that works, start a new srun command to get back onto a compute node. Once on a compute node, verify that basic X11 functionality works by starting either the xterm or xclock programs. If they did, then try your X11 program again.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#srun-error-ignoring-x11-option","title":"srun: error: Ignoring --x11 option","text":"Click to open <p><code>srun: error: Ignoring --x11 option for a job step within an existing job. Set x11 options at job allocation time.</code></p> <p>This error is issued because the srun command was run after already logging into a compute node via an interactive job. In other words, it is an srun inside of an srun.  <code>Srun</code> can be issued inside of resource allocations created by the <code>salloc</code> or <code>sbatch commands</code>, but not inside of other srun\u2019s.</p>","tags":["slurm"]},{"location":"slurm/slurm-faq/#srun-related-error-could-not-retrieve-magic-cookie","title":"srun-related error: Could not retrieve magic cookie","text":"Click to open <p><code>Could not retrieve magic cookie. Cannot use X11 forwarding</code></p> <p>This error is issued because the srun command was run after already logging into a compute node via an interactive job. In other words, it is an srun inside of an srun.  <code>Srun</code> can be issued inside of resource allocations created by the <code>salloc</code> or <code>sbatch commands</code>, but not inside of other srun\u2019s.</p>","tags":["slurm"]},{"location":"slurm/time-limits/","title":"Time Limits","text":"<pre><code>Note: The following was largely written using ChatGPT.\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#setting-a-time-limit-for-your-slurm-job-on-jhpce","title":"Setting a time limit for your SLURM job on JHPCE","text":"<p>One crucial aspect of job management in SLURM is setting time limits for job execution. Time limits help ensure fair resource allocation, prevent jobs from monopolizing system resources, and facilitate efficient utilization of computing resources.</p>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#importance-of-time-limits","title":"Importance of Time Limits:","text":"<p>Setting time limits is essential for maintaining a balanced workload on an HPC system. By specifying a maximum runtime for each job, users prevent inefficient use of resources and minimize delays for others waiting to run their tasks. Time limits also aid in preventing runaway or stuck jobs that may consume resources indefinitely. This proactive approach to job management promotes fairness and improves the overall system efficiency, making it more accessible to a larger number of users.</p>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#syntax-for-setting-time-limits","title":"Syntax for Setting Time Limits:","text":"<p>In SLURM, users can set time limits using the --time or -t options when submitting a job. </p> <p>The time limit is specified in the format days-hours:minutes:seconds. For example, to request a job with a maximum runtime of 2 days, the following command can be used:</p> <p><code>sbatch --time=2-00:00:00 my_script.sh</code></p> <p>Or simply</p> <p><code>sbatch --time=2-00 myscript.sh</code></p> <p>To request an srun session with at 4 hour time limit you would run:</p> <p><code>srun --pty --x11 -t 4:00:00 bash</code></p> <p>Note that the \u201c-t\u201d option should be followed by a space and the <code>\u2013-time</code> option should be followed by an \u201c=\u201d and a time, with no spaces.</p> <p>You can specify a default time in your ~/.slurm/defaults file. As documented in this page you can override such defaults with directives in your batch file or via the command line options.</p> Examples of 5 and 10 days for interactive &amp; batch jobs<pre><code>srun:*:time=5-00\nsbatch:*:time=10-00\n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#jhpce-cluster-time-limit-policies","title":"JHPCE Cluster Time Limit Policies:","text":"<p>On the JHPCE cluster, specific time limit policies are in place to balance the needs of diverse users. The default time limit for job execution on the public partitions (shared, interactive and gpu partitions) is set to 1 day, ensuring that shorter tasks do not face unnecessary delays. </p> <p>Additionally, users have the flexibility to request a maximum time limit of up to 90 days for longer-running jobs. This range accommodates a variety of computational workloads, offering users the freedom to tailor time limits according to the requirements of their specific tasks. Please note that by default these limits are only set on the public partitions (shared, interactive and gpu partitions).</p>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#best-practices","title":"Best Practices:","text":"","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#receive-email-notifications","title":"Receive Email Notifications","text":"<p>The directives <code>--mail-user</code> and <code>--mail-type</code> are used to tell SLURM that you want to receive email updates for certain events. One set of events is related to an approaching time limit: TIME_LIMIT_50, TIME_LIMIT_80, TIME_LIMIT_90 and TIME_LIMIT.</p> <p>If you did not use these directives when submitting your job, you can define or modify them on both pending and running jobs using <code>scontrol</code></p> <p>Be notified at 80% of job duration<pre><code>scontrol update jobid=&lt;jobid&gt; mailtype=time_limit_80\n</code></pre> But only if you tell it where to send email<pre><code>scontrol update jobid=&lt;jobid&gt; mailuser=&lt;your-address@jh.edu&gt;\n</code></pre></p>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#what-chatgpt-wrote","title":"What ChatGPT Wrote:","text":"<p>When setting time limits in SLURM, it is crucial to consider the nature of the computational task. Jobs with well-defined execution times benefit from precise time limits, while more unpredictable tasks may require a degree of flexibility. Regularly reviewing and adjusting time limits based on job characteristics and system performance is a best practice to ensure optimal utilization of resources. If you don\u2019t know precisely how long your job will take to run, it is generally better to request a longer time limit than expected, to avoid jobs being terminated prematurely mid-run. The time limit on an array job will be applied to each task, rather than the job as a whole. You can tell that a job has been terminated due to exceeding its time limit by noting that the \u201cState\u201d in the output of \u201csacct\u201d is \u201cTIMEOUT\u201d, or noting the a line like \u201cslurmstepd: error: *** JOB 123456 ON compute-123 CANCELLED AT 2024-02-13T12:03:27 DUE TO TIME LIMIT ***\u201d in the SLURM output file.</p> <pre><code>[mmill116@login31 class-scripts]$ cat script1\n #!/bin/bash\n # Test script for JHPCE cluster\n date\n echo \"I am now running on compute node:\"\n hostname\n SLEEPTIME=$((20 + $RANDOM % 60))\n echo \"Sleeping \" $SLEEPTIME \" seconds...\"\n sleep $SLEEPTIME\n date\n echo \"Done...\"\n exit 0 \n[mmill116@login31 class-scripts]$ sbatch -a 1-10%4 -t 00:00:30 script1\nSubmitted batch job 1980474 \n\n[mmill116@login31 class-scripts]$ sacct -j 1980474\n JobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n ------------ ---------- ---------- ---------- ---------- ---------- -------- \n 1980474_1       script1     shared      jhpce          2  COMPLETED      0:0 \n 1980474_1.b+      batch                 jhpce          2  COMPLETED      0:0 \n 1980474_1.e+     extern                 jhpce          2  COMPLETED      0:0 \n 1980474_2       script1     shared      jhpce          2    TIMEOUT      0:0 \n 1980474_2.b+      batch                 jhpce          2  CANCELLED     0:15\n\n. . .\n\n[mmill116@login31 class-scripts]$ cat slurm-1980474_2.out\nTue Feb 13 12:02:19 PM EST 2024\nI am now running on compute node:\ncompute-093.cm.cluster\nSleeping  71  seconds...\nslurmstepd: error: *** JOB 1980476 ON compute-093 \nCANCELLED AT 2024-02-13T12:03:27 DUE TO TIME LIMIT *** \n</code></pre>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#backfill-scheduler-strategies","title":"Backfill Scheduler Strategies:","text":"<p>SLURM\u2019s backfill scheduler introduces an additional layer of sophistication to job scheduling. Backfilling allows shorter jobs to be scheduled into available resources ahead of longer jobs, provided that the overall system efficiency is not compromised. Time limits play a crucial role in backfill strategies, ensuring that shorter jobs do not overstay their welcome while maximizing resource utilization. The backfill scheduler contributes to improved job throughput, reduced queue times, and enhanced overall system responsiveness.</p>","tags":["in-progress","slurm"]},{"location":"slurm/time-limits/#conclusion","title":"Conclusion:","text":"<p>In conclusion, setting time limits in SLURM is a fundamental practice for efficient job management in HPC environments. It promotes fair resource allocation, prevents runaway jobs, and contributes to the overall effectiveness of the computing cluster. Users should be mindful of their job requirements, system dynamics, and best practices to harness the full potential of SLURM\u2019s capabilities while maintaining a balanced and responsive computing environment.</p>","tags":["in-progress","slurm"]},{"location":"slurm/tips-reportseff/","title":"reportseff useful command examples","text":"<p>Caution</p> <p>If you have not already read about basic job facts like job notation, steps, names and states, please first visit the \"About SLURM Jobs\" document here.</p>","tags":["slurm"]},{"location":"slurm/tips-reportseff/#reportseff-overview","title":"Reportseff Overview","text":"<p><code>seff</code> is a program which looks up accounting data using the `sacct command to show some stats for a single completed job.</p> Example seff output <pre><code>Job ID: 9709\nCluster: cms\nUser/Group: ttotoj/users\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:47:34\nCPU Efficiency: 97.74% of 00:48:40 core-walltime\nJob Wall-clock time: 00:48:40\nMemory Utilized: 67.40 GB\nMemory Efficiency: 22.47% of 300.00 GB\n</code></pre> <p><code>Reportseff</code> is a python script which makes displaying job efficiency easier for other cases such as \"all of a user's jobs for a time period\" or \"all of the elements of an array job\". It also uses <code>sacct</code> to retrieve information.  It was written so that you could use the same kinds of syntax as you would with <code>sacct</code>. For example when indicating time ranges or asking for changes to the output format. You can also pass other arguments straight to <code>sacct</code> if desired.</p> <p>Home page for more information: https://github.com/troycomi/reportseff</p> <p>Example reportseff output</p> Show my COMPLETED jobs between noon and now, adding 2 fields<pre><code>module load reportseff\nexport LESS=\"-R\"\n\nreportseff  --since now-3days --until now -s CD --format +reqcpus,reqmem\n  JobID    State       Elapsed  TimeEff   CPUEff   MemEff   ReqCPUS   ReqMem \n  12394  COMPLETED    00:44:58   0.2%     92.3%    113.0%     50      1600G\n  12403  COMPLETED    00:45:12   0.2%     91.2%    115.2%     50      1600G\n  12413  COMPLETED    00:00:04   0.0%      ---      0.0%      70      1800G\n  12415  COMPLETED    00:00:03   0.0%      ---      0.0%      70      1800G\n  12416  COMPLETED    00:00:03   0.0%      ---      0.0%      75      1800G\n  12418  COMPLETED    00:03:04   0.0%      1.8%     0.9%      50      1500G\n  12419  COMPLETED    00:03:24   0.0%      ---      0.0%      50      1500G\n  12435  COMPLETED    00:07:03   0.0%     49.1%    46.5%      20       500G\n  12455  COMPLETED    06:22:15   26.5%     9.6%    38.1%       1       50G\n  12464  COMPLETED    00:01:41   0.0%     16.6%     3.2%      15       350G\n  12468  COMPLETED    00:02:56   0.0%     48.1%    54.0%      10       250G\n  12469  COMPLETED    08:39:19   1.8%     96.3%    97.7%      10       400G\n  12503  COMPLETED    06:10:40   1.3%     91.7%    152.1%     10       380G\n  12531  COMPLETED    03:24:26   0.7%     86.8%    159.9%     10       350G\n  12539  COMPLETED    00:37:51   0.1%      5.1%    46.8%      20       100G\n</code></pre> Second example reportseff output Show my COMPLETED jobs between noon and now, adding a field<pre><code>module load reportseff\nexport LESS=\"-R\"\n\nreportseff --since noon --until now -s CD --format +user\n\n    JobID    State         Elapsed  TimeEff   CPUEff   MemEff     User   \n   5011394  COMPLETED      19:16:18   40.1%    99.5%    130.4%    mnagle\n   5013943  COMPLETED      14:27:08   30.1%    99.9%    127.9%    mnagle\n   5016834  COMPLETED      07:48:51   16.3%    98.4%    124.0%    mnagle\n   5024400  COMPLETED      05:54:33   24.6%     8.0%    60.3%    enorton\n   5025014  COMPLETED      04:28:52   18.7%     9.2%     3.4%    jzhang5\n   5025583  COMPLETED      03:57:29   16.5%    89.4%    26.1%      sli1\n   5025584  COMPLETED      04:11:32   17.5%    89.1%    26.9%      sli1\n   5025586  COMPLETED      03:55:38   16.4%    82.8%    26.4%      sli1\n</code></pre> <p>There are two ways to use reportseff:</p> As a module: <pre><code>module load reportseff\nexport LESS=\"-R\"  # to provide colorized output\n</code></pre> As a bash function: <pre><code>Define an alias on the command line or in your .bashrc\n\nrs() { export LESS=\"-R\";export PYTHONPATH=/jhpce/shared/jhpce/core/reportseff/2.3.2/;export PATH=/jhpce/shared/jhpce/core/reportseff/2.3.2/bin:$PATH; reportseff \"$@\"; }\n</code></pre> <p>Examples</p> Show my COMPLETED jobs between noon and now<pre><code>module load reportseff\nexport LESS=\"-R\"\nreportseff --since noon --until now -s CD -u $USER\nreportseff --since noon --until now -s CD -u $USER --format \"user,jobid,state,nodelist,start,end,cpueff,memeff\"\n\nreportseff --since noon --until now -s CD -u $USER --format \"+user,nodelist,start,end,cpueff,memeff\"\n</code></pre> <p>you can define environment variables as you do with sacct to change your formatting. See here</p> <p>Caution</p> <p>you may not be able to see other user's job info unless you add <code>--extra-args -a</code></p> Reportseff arguments, click for complete list <pre><code>login31:~% reportseff --help\nUsage: reportseff [OPTIONS] [JOBS]...\n\nMain entry point for reportseff.\n\nOptions:\n  --modified-sort                 If set, will sort outputs by modified time\n                              of files\n  --color / --no-color            Force color output. No color will use click\n                              defaults\n  --format TEXT                   Comma-separated list of columns to include.\n                              Options are any valid sacct input along with\n                              CPUEff, MemEff, Energy, and TimeEff.  In\n                              systems with jobstat caching, GPU usage can\n                              be added with GPUEff, GPUMem or GPU (for\n                              both). A width and alignment may optionally\n                              be provided after \"%\", e.g. JobID%&gt;15 aligns\n                              job id right with max width of 15\n                              characters. Generally\n                              NAME[[%:][ALIGNMENT][WIDTH[e$]?]]. When an\n                              `e` or `$` is present after a width\n                              argument, the output will be truncated to\n                              the right.Prefix with a + to add to the\n                              defaults. A single format token will\n                              suppress the header line. Wrap in quotes to\n                              pass a string literal, otherwise alignment\n                              may be misinterpreted.\n  --slurm-format TEXT             Filename pattern passed to sbatch.  By\n                              default, will handle patterns like\n                              slurm_%j.out, %x_%j, or slurm_%A_%a.  In\n                              particular, the jobid is expected to start\n                              with '_'.  Setting this to the same entry as\n                              used in sbatch will allow parsing slurm\n                              outputs like `1234.out`.  Array jobs must\n                              have %A_%a to properly interface with sacct.\n  --debug                         Print raw db query to stderr\n  -u, --user TEXT                 Ignore jobs, return all jobs in last week\n                              from user\n  --partition TEXT                Only include jobs with the specified\n                              partition\n  --extra-args TEXT               Extra arguments to forward to sacct\n  -s, --state TEXT                Only include jobs with the specified states\n  -S, --not-state TEXT            Include jobs without the specified states\n  --since TEXT                    Only include jobs after this time. Can be\n                              valid sacct or as a comma separated list of\n                              time deltas, e.g. d=2,h=1 means 2 days, 1\n                              hour before current time. Weeks, days,\n                              hours, and minutes can use case-insensitive\n                              abbreviations. Minutes is the minimum\n                              resolution, while weeks is the coarsest.\n  --until TEXT                    Only include jobs before this time. Can be\n                              valid sacct or as a comma separated list of\n                              time deltas, e.g. d=2,h=1 means 2 days, 1\n                              hour before current time. Weeks, days,\n                              hours, and minutes can use case-insensitive\n                              abbreviations. Minutes is the minimum\n                              resolution, while weeks is the coarsest.\n  -n, --node / -N, --no-node      Report node-level statistics. Adds `jobid`\n                              to format for proper display.\n  -g, --node-and-gpu / -G, --no-node-gpu\n                              Report each GPU for each node. Sets `node`\n                              and adds `GPU` to format automatically.\n  -p, --parsable                  Output will be '|' delmited without a '|' at\n                              the end.\n  --version                       Show the version and exit.\n  --help                          Show this message and exit.\n</code></pre> <p>sacct is a command used to display information about jobs. It has a number of subtleties, such as the time window reported on and the formatting of output. We hope that this page will help you get the information you need.</p> <p><code>sacct</code> can be used to investigate jobs' resource usage, nodes used, and exit codes. It can point to important information, such as jobs dying on a particular node but working on other nodes<sup>1</sup>.</p> <p>sacct will show all submitted jobs but cannot, of course, provide data for a number of fields until the job has finished. Use the sstat command to get information about running programs. \"Instrumenting\" your jobs to gather information about them can include adding one or more <code>sstat</code> commands to batch jobs in multiple places.</p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["slurm"]},{"location":"slurm/tips-reportseff/#arguments","title":"ARGUMENTS","text":"<p>One of the appeals of this tool is that it uses sacct of course but allows you to pass extra sacct args. And it uses the same syntax for time that sacct does. </p> <p>So I was able to run <code>reportseff -u someuser --since now-4days</code></p> <p>I suppose that they chose their argument flags to avoid overlapping with those used by sacct, so you see things like <code>--since</code> and <code>--until</code> instead of <code>sacct</code>'s <code>-S</code> and <code>-E</code>  But most <code>sacct</code> arguments will work. </p> <p>Note that you can add <code>sacct</code> fields to <code>reportseff</code>'s default ones with a simple <code>--format +fieldname,otherfield</code></p>","tags":["slurm"]},{"location":"slurm/tips-reportseff/#gpu-support","title":"GPU support?","text":"<p>Another feature of interest is supposedly the ability to calculate memory use efficiency for GPU jobs. If we discovered that many users were typically not using all of the memory on GPUs, it would open the door to partitioning GPUs into slices and therefore increasing the number of available GPUs for zero dollars. However it is the case that we would have to install other software (parts of the Princeton jobstats suite) to enable the acquisition of GPU usage data. We probably won't have time to do that.</p>","tags":["slurm"]},{"location":"slurm/tips-reportseff/#colorized-output","title":"COLORIZED OUTPUT","text":"<p>It seems to use <code>less</code> as its pagination tool by default when output is longer than your terminal's size. Normally it produces colorized output which is very nice, as Borat would say. But the colors go away and you see the ugly escape characters unless you have the LESS environment variable defined to include <code>-R</code>. So you may want to add to your <code>~/.bashrc</code> file a command like <code>export LESS=\"-i -M -R\"</code> (The <code>-i</code> means \"do case-insensitive searches, unless a capital is used\", <code>-M</code> means \"make prompt show lines and current byte\" and <code>-R</code> means \"show color text where ESC sequences are present\".) </p> <p>Adding the flag <code>--color</code> did not help. There is a <code>--no-color</code> which might be handy for generating reports lacking escape characters when redirecting the output to a file.</p>","tags":["slurm"]},{"location":"slurm/tips-reportseff/#memory-use-above-100","title":"MEMORY USE ABOVE 100%","text":"<p>Like other tools I've used which provide memory efficiency stats, this one shows jobs where efficiency is above 100%. Which I perhaps mistakenly interpret as indicating that the user has exceeded their memory limits. (I think I've also seen this in pure sacct output where no one is trying to combine data and calculate anything.)   DO THESE CASES REPRESENT INSTANCES OF THE USER'S PROCESSES GRABBING MORE MEMORY THAN \"ALLOWED\" FOR BRIEF-ENOUGH PERIODS THAT CGROUP LIMITS ARE NOT RESULTING IN KILLED PROCESSES? Or some other explanation that doesn't indicate that our understandings about memory stats or cgroups are flawed?</p> <p>Ultimately I would like to be able to tell users / write in the docs an explanation for why you see values above 100% even if only to aid them in understanding how to utilize the info to make their cluster use better. (At least I've never seen any negative numbers!)</p> <p>One explanation for  memory numbers that are \"off\" that I've seen is that someone is using MaxVM instead of MaxRSS etc. I happen to see in output_renderer.py a line <code>\"MemEff\": [\"REQMEM\", \"NNodes\", \"AllocCPUS\", \"MaxRSS\", \"NTasks\"]</code> which might imply which field is being used.</p> <ol> <li> <p>In which case you can add the directive <code>--exclude=compute-xxx</code> to your job submission, then notify us via bitsupport so we can fix that node.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/tips-sacct/","title":"sacct useful command examples","text":"<p>Caution</p> <p>If you have not already read about basic job facts like job notation, steps, names and states, please first visit the \"About SLURM Jobs\" document here.</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#sacct-overview","title":"Sacct Overview","text":"<p>The sacct command is used to query the SLURM job accounting database, usually for jobs which have ended (one way or the other). It has a number of subtleties, such as the formatting of output. <code>sacct</code> can be used to investigate jobs' resource usage, nodes used, and exit codes. It can point to important information, such as jobs dying on a particular node but working on other nodes<sup>1</sup>.</p> <p>The sstat program is aimed at those who are looking for information about running jobs. Much of the information on this page can be used with <code>sstat</code>, but there are differences, particularly in available output fields (compare the output of <code>sacct -e</code> and <code>sstat -e</code>).</p> You can use <code>sstat</code> to profile the resource use of or instrument your jobs over their lifetimes. <p>You can do that in a variety of ways: (a) interactively, (b) from within your batch job scripts (although not while some other command is running), adding one or more <code>sstat</code> commands to the batch script in multiple places, and (c) from other batch jobs that you craft. (You could specify a very small resource job so it launches immediately, (perhaps using the <code>interactive</code> partition), learn how to create a hetrogenous job that spawns both the sstat commands in a timed loop as well as running your real program(s), or maybe reads files you create in the main batch job in order to figure out what jobid sstat should be monitoring.</p> <p>Example</p> Show my failed jobs between noon and now<pre><code>sacct -s F -o \"user,jobid,state,nodelist,start,end,exitcode\" -S noon -E now\n\nUser JobID             State        NodeList               Start                 End ExitCode \n--------- ------------ ---------- --------------- ------------------- ------------------- -------- \ntunison 14084984         FAILED     compute-112 2025-02-19T14:30:46 2025-02-19T15:35:09      0:9 \n        14084984.ex+  COMPLETED     compute-112 2025-02-19T14:30:46 2025-02-19T15:35:09      0:0 \n        14084984.0   CANCELLED+     compute-112 2025-02-19T14:30:46 2025-02-19T15:35:09      0:9 \n</code></pre> <p>More examples can be found throughout this document, as well as at the end.</p> <p>sacct is a command used to display information about jobs.</p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#sacct-basics","title":"sacct basics","text":"<ol> <li>By default only your own jobs are displayed. Use the <code>--allusers</code> or <code>-a</code> flag if necessary.</li> <li>Only jobs from a certain time window are displayed by default. That window varies in a confusing manner depending the arguments you provide. See this section of the manual page. Therefore it is recommended to always provide start (<code>-S</code>) and end (<code>-E</code>) times to be sure that you are seeing what you expect.</li> <li>You can choose output fields and control their width. </li> <li>Even the simplest of batch jobs contain multiple \"steps\" as far as SLURM is concerned. One of them, named \"extern\" represents the ssh to the compute node on behalf of your job. Job records consist of a primary entry for the job as a whole  as  well as entries for job steps. The Job Launch page has a more detailed description of each type of job step. You may find the <code>-X</code> flag helpful to omit clutter.</li> <li>Regular jobs are in the form: JobID[.JobStep]</li> <li>Array jobs are in the form: ArrayJobID_ArrayTaskID</li> <li>Jobs have multiple steps!! (Explained here.)</li> </ol> <p>Warning</p> <p>Sacct retrieves data from a SQL database. Be careful when creating your sacct commands to limit the queries to the information you need. Narrow the search as much as possible.  That database needs to be modified constantly as jobs start and complete, so we don't want it tied up answering sacct queries. If you want to look at a large amount of data in a variety of ways, consider saving the output to a text file and then working with that file.</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#command-options-of-note","title":"Command Options of Note","text":"<p>Check the man page. There are other useful options.</p> <ul> <li><code>-X</code>  show stats for the job allocation itself, ignoring steps (try it)</li> <li><code>-R</code> reasonlist  show jobs not scheduled for given reason</li> <li><code>-a</code>  allusers</li> <li><code>-N</code> nodelist  only show jobs which ran on this/these nodes</li> <li><code>-u</code> userlist  only show jobs which ran by this/these users</li> <li><code>--name=</code>namelist - only show jobs with this list of names</li> <li><code>-n</code>  noheader</li> <li><code>-p</code>  parsable  puts a | between fields and at end of line</li> <li><code>-P</code>  parsable2  does not put a | at end of line</li> <li><code>--delimeter</code>  - use that char instead of | for <code>-p</code> or <code>-P</code> <li><code>--units=[KMGTP]</code> - display in this unit</li> <li><code>-k</code> minimum time - looking for jobs with time limits in a range</li> <li><code>-K</code> maximum time - looking for jobs with time limits in a range</li> <li><code>-q</code> qoslist - list of qos used</li>","tags":["slurm"]},{"location":"slurm/tips-sacct/#sorting-or-processing-output","title":"Sorting or processing output","text":"<p><code>sacct</code> can output with other delimiters if you specify either <code>-p</code> or <code>-P</code> and <code>--delimiter=&lt;characters&gt;</code></p> <p><code>sacct</code> does not have a sort option. You need to sort its output by other methods. If you want to change the order of the information, specify the field names in the desired order. Remember that if you do not include the <code>-X</code> option to <code>sacct</code>, then the number of output fields might vary between the first of several lines per job and later lines. For example if you request an output format starting with \"user\", that field is only printed for the first line per job. (shake fist!)</p> <p>This is a prime example of where it is kindest to the SLURM master daemon if you run your query such that you store the results in a text file, then work with the text file contents, rather than running many variations of a command pipeline beginning with <code>sacct</code> when you already have all of the desired output option fields and are just trying to figure out the right text-processing logic to deal with the output using sort or awk or what have you. OR do your initial searching in order to produce the right kinds of output using limited date or other ranges.</p> sort failed jobs by exitcode<pre><code>sacct -X -a -s F -o \"user,jobid,state,nodelist,exitcode\" -S noon -E now|sort -k5| less\n</code></pre> sort failed jobs by exitcode, then by nodelist<pre><code>sacct -X -a -s F -o \"user,jobid,state,nodelist,exitcode\" -S noon -E now|sort -k5,5 -k4,4| less\n</code></pre> <p>Useful sort options:</p> <ol> <li><code>-kKEYDEF</code> or <code>--key=KEYDEF</code> Given just a number as the KEYDEF, it sorts by column</li> <li><code>-k5,5</code> Repeating the column number prevents multiple words within a column from being considered extra columns (at least the author thinks that is what is going on) (maybe there is a more sophisticated version of KEYDEF that can also prevent that issue)</li> <li><code>-r</code>or <code>--reverse</code></li> <li><code>-n</code> or <code>--numeric-sort</code> </li> </ol> <p>See the sort manual page for other options, including multiple kinds of numeric sorts, as well as the syntax of KEYDEF.</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#start-and-end-times","title":"Start and End Times","text":"<p>It is best to use always specify a <code>-S</code> start time and a <code>-E</code> end time.</p> <p>Special time words: today, midnight, noon, now</p> <p>Positive and negative deltas from now can be used, which is very handy, but remember to use the full unit word (e.g. \"days\" not \"d\" or \"day\")!!!</p> <p>now[{+|-}count[seconds(default)|minutes|hours|days|weeks]]</p> Examples: <code>now-3days</code> <code>now-2hours</code> <p>Valid time formats are (spare brackets indicate optional elements):</p> <pre><code>               HH:MM[:SS][AM|PM]\n               MMDD[YY][-HH:MM[:SS]]\n               MM.DD[.YY][-HH:MM[:SS]]\n               MM/DD[/YY][-HH:MM[:SS]]\n               YYYY-MM-DD[THH:MM[:SS]]\n</code></pre> Examples: 0408  # April 8th 09:15 # 9:15am (24-hour time is assumed) 09:15pm # 9:15pm <code>-S $(date -d '21 days ago' +%D-%R) -E $(date -d '17 day ago' +%D-%R)</code> # the date command can interpret many human-readable expressions, then express them using the format mentioned afterwards. Cool, eh!!! 04/15  # the bash shell will probably have problems with the forward slash unless you surround the string with double quotes","tags":["slurm"]},{"location":"slurm/tips-sacct/#job-time-limit","title":"Job Time Limit","text":"<p>If you want to filter for jobs with a certain time limit, use one or both of the <code>-k/--timelimit-min</code> and <code>-K/--timelimit-max</code> flags. To show only jobs with time limit between 48 and 72 hours that ran between 4 and 8 weeks ago:</p> <p><code>sacct -S $(date -d '8 weeks ago' +%D-%R) -E $(date -d '4 weeks ago' +%D-%R) -k 48:00 -K 72:00</code></p> <p>If you know the exact time limit of the jobs you are looking for, set both min and max time limit to the same value. For jobs with a time limit of 30 minutes that ran in the last month:</p> <p><code>sacct -S $(date -d 'last month' +%D-%R) --timelimit-min 30 --timelimit-max 30</code></p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#job-state-values","title":"Job State Values","text":"<p>Using the <code>-s &lt;states&gt;</code> option, you can prune your search by looking for only jobs which match the state you need, such as F for failed. (All of these work: f, failed, F, FAILED). You can specify more than one state if you separate them with commas. Note that you use a different flag for state with the <code>squeue</code> command, <code>-t &lt;states&gt;</code>.</p> <p>Job states have short names consisting of one or two letters, and a full name.  You can use either form when working with SLURM commands. They are shown here capitalized for emphasis but can be specified as lower-case.</p> <p>Warning</p> <p>Different steps of a job can have different end states. For example the \"extern\" step is often COMPLETED when the \"batch\" and overall steps are FAILED. The <code>-X</code> flag to <code>sacct</code> will show you only the overall job state, such as FAILED, which is useful for most cases. However, sometimes you need to check the state of all of a jobs steps in order to see that a \"batch\" step ran OUT_OF_MEMORY.</p> <p>Primary job states of interest:</p> Short Name Long Name Explanation PD PENDING Job is waiting to start R RUNNING Job is currently running CG COMPLETING Job has ended, clean-up has begun CD COMPLETED Job finished normally, with exit code 0 F FAILED Job finished abnormally, with a non-zero exit code CA CANCELLED Job was cancelled by the user or a sysadmin OOM OUT_OF_MEMORY Job was killed for exceeding its memory allocation TO TIMEOUT Job was killed for exceeding its time limit Click for a complete list of job states <p>FROM https://slurm.schedmd.com/squeue.html#lbAG</p> BF BOOT_FAIL Job terminated due to launch failure, typically due to a hardware failure (e.g. unable to boot the node or block and the job can not be requeued). CA CANCELLED Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. CD COMPLETED Job has terminated all processes on all nodes with an exit code of zero. CF CONFIGURING Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG COMPLETING Job is in the process of completing. Some processes on some nodes may still be active. DL DEADLINE Job terminated on deadline. F FAILED Job terminated with non-zero exit code or other failure condition. NF NODE_FAIL Job terminated due to failure of one or more allocated nodes. OOM OUT_OF_MEMORY Job experienced out of memory error. PD PENDING Job is awaiting resource allocation. PR PREEMPTED Job terminated due to preemption. R RUNNING Job currently has an allocation. RD RESV_DEL_HOLD Job is being held after requested reservation was deleted. RF REQUEUE_FED Job is being requeued by a federation. RH REQUEUE_HOLD Held job is being requeued. RQ REQUEUED Completing job is being requeued. RS RESIZING Job is about to change size. RV REVOKED Sibling was removed from cluster due to other cluster starting the job. SI SIGNALING Job is being signaled. SE SPECIAL_EXIT The job was requeued in a special state. This state can be set by users, typically in EpilogSlurmctld, if the job has terminated with a particular exit value. SO STAGE_OUT Job is staging out files. ST STOPPED Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. S SUSPENDED Job has an allocation, but execution has been suspended and CPUs have been released for other jobs. TO TIMEOUT Job terminated upon reaching its time limit. <p>That complete list comes from this section of the <code>sacct</code> manual page, which has also been saved to a text file you can copy for your own reference: <code>/jhpce/shared/jhpce/slurm/docs/job-states.txt</code></p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#output-fields-of-interest","title":"Output Fields of Interest","text":"All sacct fields (output of sacct -e) <pre><code>Account             AdminComment        AllocCPUS           AllocNodes         \nAllocTRES           AssocID             AveCPU              AveCPUFreq         \nAveDiskRead         AveDiskWrite        AvePages            AveRSS             \nAveVMSize           BlockID             Cluster             Comment            \nConstraints         ConsumedEnergy      ConsumedEnergyRaw   Container          \nCPUTime             CPUTimeRAW          DBIndex             DerivedExitCode    \nElapsed             ElapsedRaw          Eligible            End                \nExitCode            Flags               GID                 Group              \nJobID               JobIDRaw            JobName             Layout             \nMaxDiskRead         MaxDiskReadNode     MaxDiskReadTask     MaxDiskWrite       \nMaxDiskWriteNode    MaxDiskWriteTask    MaxPages            MaxPagesNode       \nMaxPagesTask        MaxRSS              MaxRSSNode          MaxRSSTask         \nMaxVMSize           MaxVMSizeNode       MaxVMSizeTask       McsLabel           \nMinCPU              MinCPUNode          MinCPUTask          NCPUS              \nNNodes              NodeList            NTasks              Partition          \nPriority            QOS                 QOSRAW              Reason             \nReqCPUFreq          ReqCPUFreqGov       ReqCPUFreqMax       ReqCPUFreqMin      \nReqCPUS             ReqMem              ReqNodes            ReqTRES            \nReservation         ReservationId       Reserved            ResvCPU            \nResvCPURAW          Start               State               Submit             \nSubmitLine          Suspended           SystemComment       SystemCPU          \nTimelimit           TimelimitRaw        TotalCPU            TRESUsageInAve     \nTRESUsageInMax      TRESUsageInMaxNode  TRESUsageInMaxTask  TRESUsageInMin     \nTRESUsageInMinNode  TRESUsageInMinTask  TRESUsageInTot      TRESUsageOutAve    \nTRESUsageOutMax     TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutMin    \nTRESUsageOutMinNode TRESUsageOutMinTask TRESUsageOutTot     UID                \nUser                UserCPU             WCKey               WCKeyID\n</code></pre> What output fields are available?<pre><code>sacct -e\n</code></pre> See all fields for a job<pre><code>sacct -o ALL -j &lt;jobid&gt;\n</code></pre> <p>The following fields are probably the ones you'll want. See this section of the manual page for the list and their meaning. Capitalization does not matter; it is used for readability.</p> <ul> <li>TRES means Trackable RESources, such as RAM and CPUs.</li> <li>A number of fields (not listed) are available to tell you on which node a maximum occurred. Similarly there are fields to tell you minimum, average and maximum values for some items.</li> </ul> BasicsTimesNodesResources RequestedResources Consumed <ul> <li>User</li> <li>JobId</li> <li>JobName</li> <li>Partition</li> <li>State</li> <li>ExitCode</li> </ul> <ul> <li>Submit</li> <li>Start</li> <li>Elapsed</li> <li>End</li> </ul> <ul> <li>AllocNodes</li> <li>NNodes - number of nodes</li> <li>NodeList</li> </ul> <ul> <li>ReqTRES # this is what you will be billed for</li> <li>ReqNodes</li> <li>ReqCPUS</li> </ul> <ul> <li>TRESUsageInTot</li> <li>CPUTime - (elapsed)*(AllocCPU) in HH:MM:SS format</li> <li>MaxRSS - Max resident set of all tasks in job</li> <li>MaxVMSize - Max virtual memory of all tasks in job</li> <li>MaxDiskRead - Number bytes read by all tasks in job</li> <li>MaxDiskWrite - Number bytes written by all tasks in job</li> </ul>","tags":["slurm"]},{"location":"slurm/tips-sacct/#about-memory-fields","title":"About Memory Fields","text":"<p>Virtual Memory Size (VMSize) is the total memory size of a job. It includes both memory actually in RAM (the RSS) and parts of executabilities which were not needed to be read in off of disk into RAM. Because, for example, routines in dynamically linked libraries were never called, so those libraries were not loaded. </p> <p>Resident set size (RSS) is the portion of memory (measured in megabytes) occupied by a job that is held in main memory (RAM). The rest of the memory required by the job exists in the swap space or file system, either because some parts of the occupied memory were paged out, or because some parts of the executable were never loaded.</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#formatting-fields","title":"Formatting fields","text":"<p>By default fields are 20 characters wide. That is often insufficient.</p> <p>You can put a \"%NUMBER\" after a field name to specify how many characters should be printed, e.g.</p> <ul> <li>format=name%30 will print 30 characters of field name right justified.  </li> <li>format=name%-30 will print 30 characters left justified.</li> </ul> <p>You can specify your format on the command line or define an environment variable to hold the desired string (see below). (Or you can do both, and the command line will overrule what is in the environment variable. So you can define SACCT_FORMAT to be what you normally want to see but still see what you need to using the CLI.)</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can define environment variables in your shell to reduce the complexity of issuing sacct commands. You can also set these in shell scripts so that the <code>sacct</code> commands inside the script will use those values Command line options will always override these settings. Put the one you use most often into your <code>~/.bashrc</code> file.</p> <p>SACCT_FORMAT</p> <p>SLURM_TIME_FORMAT</p>","tags":["slurm"]},{"location":"slurm/tips-sacct/#formatting-datestimes","title":"Formatting Dates/Times","text":"<p>You can use most variables defined by the STRFTIME(3) system call. This web page is a starting point, but what SLURM has chosen to implement may not match.</p> <ul> <li>%a - abbrieviated name of day of the week</li> <li>%m - month as decimal, 01 to 12</li> <li>%d - day of month as decimal</li> <li>%H - hour as decimal in 24-hour notation</li> <li>%M - minute as decimal, 00 to 59</li> <li>%T - time in 24-hour notation (%H:%M:%S)</li> </ul> <p>Day of week MM-DD HH:MM<pre><code>export SLURM_TIME_FORMAT=\"%a %m-%d %H:%M\" \n</code></pre> The start and end field widths show below are suitable for the time format shown above.</p> Resources requested, used<pre><code>export SACCT_FORMAT=\"user,jobid,jobname,nodelist%12,start%-20,end%-20,state%20,reqtres%40,TRESUsageInTot%200\"\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacct/#exit-error-codes","title":"Exit Error Codes","text":"<p>In addition to the job's \"state\", SLURM also records error codes. Unfortunately the vendor's Job Exit Codes page doesn't provide a meaning for the numerical values.</p> <p>Error <code>0:53</code> often means that something wasn't readable or writable. For example, job output or error files couldn't be written in the directory in which the job ran (or where you told SLURM to put them with a directive).</p> <pre><code>a guide for exit codes:\n\n0 \u2192 success\nnon-zero \u2192 failure\nExit code 1 indicates a general failure\nExit code 2 indicates incorrect use of shell builtins\nExit codes 3-124 indicate some error in job (check software exit codes)\nExit code 125 indicates out of memory\nExit code 126 indicates command cannot execute\nExit code 127 indicates command not found\nExit code 128 indicates invalid argument to exit\nExit codes 129-192 indicate jobs terminated by Linux signals\nFor these, subtract 128 from the number and match to signal code\nEnter kill -l to list signal codes\nEnter man signal for more information\n</code></pre> <p>We will try to put specific entries here that we see more frequently and can identify.</p> Exitcode Probable Meaning 0:9 cancelled (does it indicate who cancelled - user or slurm?) 0:15 cancelled (looks like this is used when either user cancelled or job ran out of time) 0:53 Some file or directory was not readable or writable, ran out of disk space or reached disk quota 0:125 Job ran out of memory 1:0 ?? 2:0 ??","tags":["slurm"]},{"location":"slurm/tips-sacct/#diagnostic-arguments","title":"Diagnostic Arguments","text":"<p>These can be useful to double-check what someone actually did.</p> See the full command issued to submit the job<pre><code>sacct -o SubmitLine%250 -j &lt;jobid&gt; # may need to increase field width\n</code></pre> See batch file used<pre><code>sacct -B -j &lt;jobid&gt;\n</code></pre> Directory used by the job to execute commands<pre><code>sacct -o WorkDir -j &lt;jobid&gt;\n</code></pre> See jobs given a time limit btwn 1min &amp; 1 day<pre><code>sacct -k 00:01 -K 1-0\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacct/#examples","title":"Examples","text":"<p>Please try to specify what you need so you don't tax the system and also have less output to inspect.</p> <p>All jobs for username bob that (ran with a wall time of at least 2 days) and (were killed for running out of memory) in the past 3 months:</p> <pre><code>sacct --user bob --starttime $(date -d '3 months ago' +%D-%R) --state OUT_OF_MEMORY --timelimit-min 2-00:00:00 --format JobID,JobName,Elapsed,NCPUs,TotalCPU,CPUTime,ReqMem,MaxRSS,MaxDiskRead,MaxDiskWrite,State,ExitCode\n</code></pre> <p>HOW MANY JOBS FAILED WITH OUT-OF-MEMORY ERRORS IN THE LAST TWO WEEKS?\"</p> <pre><code>sacct -n -S now-14days -E now -X -s OOM | wc -l\n</code></pre> <p>HOW MANY JOBS FAILED WITH OTHER KINDS OF ERRORS IN THE LAST TWO WEEKS?\"</p> <pre><code>sacct -n -S now-14days -E now -X -s F | wc -l\n</code></pre> If you have a lot of questions about the last 2 weeks worth of jobs... <p>If you were wanting to look at the data for a lot of jobs, do SLURM users a favor and save the information to a text file. Then use normal UNIX commands like <code>grep</code> to work with the text file contents. Rather than keep asking the SLURM accounting database for variations of the information. Because doing that interrupts the master SLURM demon which manages everything.</p> <p>CPU &amp; RAM expended by jobs dying from out-of-memory errors in last 2 hours</p> <pre><code>sacct -n -S now-2hours -E now -u jzhou1 -X -s OOM -o cputimeraw,reqmem --units=G |  awk '{timesum+=$1;ramsum+=$2} END{printf \"%s (CPU hrs) \\t%s (GB)\\n\",timesum/60, ramsum}'\n</code></pre> <p>Some stats don't show up if you use the -X flag!!!!</p> <p>Here we look at jobs in the SAS partition for the last day for a specific user. The job summary line, which is all that you would see if you used the <code>-X</code> flag, does NOT include the memory information. (You also don't see that for the RUNNING job--you'd need to use the <code>sstat</code> command for that info.) And lastly we note that the state of each job step can differ.</p> <pre><code>sacct -ar sas -o user%18,jobid,state,nodelist,maxrss,maxvm,reqmem -S now-1days -E now -u c-egutie16-55548 --units=G\n</code></pre> Click here to see output <pre><code>                 User JobID             State        NodeList     MaxRSS  MaxVMSize     ReqMem \n------------------ ------------ ---------- --------------- ---------- ---------- ---------- \n  c-egutie16-55548 11546            FAILED     compute-132                               5G \n               11546.extern  COMPLETED     compute-132          0          0            \n               11546.0          FAILED     compute-132      1.88G      5.64G            \n  c-egutie16-55548 11565            FAILED     compute-132                               5G \n               11565.extern  COMPLETED     compute-132          0          0            \n               11565.0          FAILED     compute-132      1.86G      5.63G            \n  c-egutie16-55548 11569         COMPLETED     compute-132                               5G \n               11569.extern  COMPLETED     compute-132          0          0            \n               11569.0       COMPLETED     compute-132      1.22G      4.98G            \n  c-egutie16-55548 11589        OUT_OF_ME+     compute-132                              15G \n               11589.extern  COMPLETED     compute-132          0          0            \n               11589.0      OUT_OF_ME+     compute-132     16.49G     16.94G            \n  c-egutie16-55548 11600           RUNNING     compute-132                             400G \n               11600.extern    RUNNING     compute-132                                  \n               11600.0         RUNNING     compute-132\n</code></pre> <p>sstat</p> <p>Here we see the stats at this moment for a running job. You can embed multiple <code>sstat</code> commands in your batch job files to check on various values over the life of your job. Unfortunately <code>sstat</code> doesn't have a <code>--units</code> argument. So here we see memory in kilobytes and output writing in bytes. <pre><code>sstat -j 11600 -o maxrss,maxvmsize,maxdiskwrite\nMaxRSS  MaxVMSize MaxDiskWrite \n---------- ---------- ------------ \n61259220K 203077796K  55875510365\n</code></pre></p> <p>Using environment variables to control output format</p> Day of week MM-DD HH:MM<pre><code>export SLURM_TIME_FORMAT=\"%a %m-%d %H:%M\" \n</code></pre> <p>The start and end field widths show below are suitable for the time format shown above.</p> Resources requested, used<pre><code>export SACCT_FORMAT=\"user,jobid,jobname,nodelist%12,start%-20,end%-20,state%20,reqtres%40,TRESUsageInTot%200\"\n</code></pre> <ol> <li> <p>In which case you can add the directive <code>--exclude=compute-xxx</code> to your job submission, then notify us via bitsupport@lists.jh.edu so we can fix that node.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/","title":"sacctmgr -- useful command examples","text":"<p>Sacctmgr is mostly used by systems administrators. Only they are allowed to make changes. Users can use it to view settings.</p> <p><code>sacctmgr</code> is used by systems administrators to create and modify user accounts as well as QOS (qualities of service).</p> <p>Also note that by \"account\" SLURM means the parent object of user accounts. We do not currently put our users into different accounts (many other clusters do).</p>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#sacctmgr-for-users","title":"Sacctmgr for Users","text":"<pre><code># See your entry in the sacctmgr database\nsacctmgr show user withassoc where name=$USER\n\n# See our currently defined QOS in default format\nsacctmgr show qos\n\n# See our currently defined QOS in a readable format\n# (*we put this in the script \"showqos\")\nsacctmgr show qos format=Name%20,Priority,Flags%30,MaxWall,MaxTRESPU%20,MaxJobsPU,MaxSubmitPU,MaxTRESPA%25\n\n# See all users database values\nsacctmgr show user withassoc  | less\n\n# See a particular user's database values\nsacctmgr show user withassoc where name=smburke\n\n# WHO HAS EXTRA QOS -- shortens the output width\n#  (but needs improvement to align column entries)\nsacctmgr show user withassoc|grep -v \"normal \"|awk '{printf \"%s\\t\\t%s\\t%s\\t\\t%s%\\n\", $1,$3,$4,$7}'\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#sacctmgr-for-systems-administrators","title":"Sacctmgr for Systems Administrators","text":"<p>We currently have two clusters, named \"jhpce3\" and \"cms\" You don't have to specify which cluster you want to consult/change, as we have a SLURM server for each cluster. And because they aren't connected in any way, you cannot extract data from one when logged into a node in the other cluster.</p> <p>Like many administrative SLURM commands, you can just run the <code>sacctmgr</code> command to enter into its shell version. Useful if you are exploring some situation, although you cannot paginate output. It supports up-arrow to get at previous commands. Issuing the command \"verbose\" when in the interactive shell may display interesting info.</p> <p>You can add the flag \"-i\" to avoid the \"are-you-sure\" 30 second delay and prompt. Useful when scripting.</p>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#seeing-database-transactions","title":"Seeing database transactions","text":"<pre><code>sacctmgr list transactions\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#make-a-backup-of-users-and-their-settings","title":"Make A Backup Of Users and Their Settings","text":"<p>Currently JRT has stored some in <code>/root/slurm/sacctmgr-stuff/sacctmgr-dumps/</code></p> <p><pre><code># Dump the accounting database\nsacctmgr dump &lt;cluster-name&gt; file=&lt;filename&gt;\n</code></pre> That backup only contains users. Other things are stored in sacctmgr database(?s?), such as QOS definitions.</p> <pre><code># Dump the QOS definitions\nsacctmgr show qos &gt; qos_backup-date.txt\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#managing-qos-for-users","title":"Managing QOS for users","text":"<p>See our QOS page for more information.</p> <p>Our users have no limits on them, at the user account level. The typical account has a default QOS named \"normal\". If you run <code>showqos</code> you will see that \"normal\" doesn't have any restrictions. Therefore users entitled to run jobs in their private per-research group partitions are not limited in how many of their nodes' resources they can consume. (Users found running jobs on partitions they are not entitled to use will have their jobs killed and have to acknowledge that they understand that they need to use public partitions.)</p> <p>For public partitions like \"shared\" and \"interactive\", the slurm config file /etc/slurm/partitions.conf specifies a default partition QOS of, for example, \"shared-default\". So  we can control how many CPUs and how much RAM each user can use in public partitions by changing the appropriate single QOS.</p> <p>When changing qos for something only use the '=' operator when wanting to explicitly set the qos to something.  In most cases you will want to use the '+=' or '-=' operator to either add to or remove from the existing qos already in place.</p> <pre><code># Add a QOS to a user's existing allowed QOS:\nsacctmgr -i mod user mmill116 set qos+=high-priority\n# or, you can redefine their whole list\nsacctmgr mod user where name=tunison set qos=normal,shared-200-2\n\n# Remove a QOS from a user's existing allowed QOS:\nsacctmgr -i mod user where name=tunison set qos-=shared-200-2 # to remove\n\n# Limit a user's ability to run and submit jobs\nsacctmgr -i mod user where name=tunison set MaxJobs=100,MaxSubmit=200\n\n# How JHPCE3 users accounts are created in the sacctmgr database\nsacctmgr -i create user name=$userid cluster=jhpce3 account=jhpce \n\n# How C-SUB users accounts are created in the sacctmgr database on jhpcecms01\nsacctmgr -i create user name=$userid account=generic cluster=cms \n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#managing-qos","title":"Managing QOS","text":"<p>To delete a parameter from a QOS, set it to the value -1</p> <p>You MUST define flags=DenyOnLimit,OverPartQOS for a QOS to work as expected</p> <p>We now limit all partitions to 10,000 jobs per user.  Therefore every such QOS needs <code>MaxSubmitJobsPU=10000</code></p> <p>Note that the output field names displayed are not necessarily the same as the keywords used when modifying, e.g. MaxTRESPerUser is the keyword but the more concise MaxTRESPU is displayed.</p> <p>If you provide the <code>-i</code> flag, the operation is immediately implemented. Without it, you have 30 seconds to respond (which doesn't work in scripts).</p> <p>There are a number of variants of some categories of parameters -- by user, by group, by account, etc.</p> <p><code>MaxJobs*</code> means jobs that can be running at one time <code>MaxJobsSubmit*</code> means jobs that can be both pending and running, altogether, in total, e.g. <code>MaxJobsSubmitPerUser</code></p> <p>Note</p> <p>There are a number of parameters which feature Group or Account. We cannot use these easily, because for example as far as \"Groups\" are concerned, by default our users are all in the same group (\"users\" in jhpce3 and \"c-users\" in cms). SLURM doesn't look at secondary group membership, only the primary group. AND we don't maintain groups for all of our PI groups. We maintain groups not to give access to computers but to control storage permissions. (In jhpce3 some users DO have primary groups different from \"users\".)</p> <p>Accounts are similar to groups in that our current model has all user accounts belonging to a single higher-level account object (\"jhpce\" in jhpce3 and \"generic\" in cms (for some reason -- there's also a csub account) (and the small number of c-*-10101 users happen to be in the \"sysadmin\" account)).</p> <p>Note that one can use the fact that all users are in the same account to limit the total number of users in the cluster who can do some things, like submit jobs, or whatever the parameter category is. But take care to consider whether you are limiting what everyone can do, in aggregate, not individually.</p> <pre><code># Define a QOS which will limit a user to 25 running jobs with a max of 25 more pending\n# This QOS won't change any behavior until it is added to a partition or association etc\nsacctmgr -i add qos job-25run50sub\n\n# You MUST define these flags for the QOS to work as expected\n# We limit all partitions to 10,000 jobs per user\nsacctmgr -i modify qos job-25run50sub set flags=DenyOnLimit,OverPartQOS\nsacctmgr -i modify qos job-25run50sub set MaxSubmitJobsPU=10000\n\n# Add the job-limiting arguments \nsacctmgr -i modify qos job-25run50sub set MaxJobsPerUser=25 MaxSubmitJobsPerUser=50\n\n# Remove one of those parameters\nsacctmgr modify qos job-25run50sub set MaxJobsPerUser=-1\n\n# Delete the QOS (they can't be renamed!!)\nsacctmgr -i delete qos job-25run50sub\n\n# Limit each user to 100 CPU and 512GB (you must specify RAM in megabytes):\nsacctmgr modify qos shared-default set MaxTRESPerUser=mem=524288 MaxTRESPerUser=cpu=100\n\n# Limit the amount of a Trackable RESource\nsacctmgr modify qos public-gpu-limit set MaxTRESPerUser=gres/gpu=4\n\n# Limit maximum job run time to 1 day\nsacctmgr modify qos cms-larger set MaxWall=1-0\n\n# Limit each user to one of: a pending or running job.\n# Limit whole cluster to running 1 job with up to one more pending\n# This means only one job using this QOS can run in the whole cluster. To put a lid on things. If you include MaxJobsPA=1 then no second user can submit\n#\nsacctmgr modify qos cms-larger set MaxJobsPerUser=1 MaxSubmitJobsPerUser=1  MaxSubmitJobsPA=2\n\n# You cannot rename a QOS, so you have to delete the old name -- be aware that jobs might be using it!!!\nsacctmgr delete qos qosname\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#searching-for-transactions","title":"Searching for transactions","text":"<p>You can use variants of <code>sacctmgr list transactions</code> to inspect the database going back to its founding. The kinds of things you can see: * who ran which command that changed the database * what commands involved a specific user * what commands changed a QOS</p>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#output-formatting","title":"Output formatting","text":"<p>Output columns are: Time, Action, Actor, Where, Info</p> <p>\"Where\" contains things like the names of users and QOS</p> <p>You can control field width by adding the % character then a field width number value.</p> <p>You can specify what you want to see with e.g. \"format=user,time%10\"</p>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#actions","title":"Actions","text":"<p>Some of the actions you can search for are:</p> <ul> <li>Add Users</li> <li>Remove Users</li> <li>Add Associations</li> <li>Add QOS</li> <li>Modify QOS</li> <li>Modify Clusters - I think that this might be <code>slurmctld</code> restarts.</li> </ul> <p>The right-most column in the default output is called \"Info\". It is truncated such that you can't see full QOS definition commands etc. You can see that info by adding format arguments or simply with the <code>-P</code> (\"parsable 2\") flag.</p> <pre><code>sacctmgr show transaction | less\n\nsacctmgr -P show transaction &gt; /tmp/lookatme\n\n# transactions involving cluster user simmons\nsacctmgr list transactions Users=simmons\n\n# Which users were created in January 2025?\n# Here is the version that will produce one user per line\nsacctmgr list transactions Action=\"Add Users\" Start=2025-01-01 End=2025-01-31 format=Where,Time%10\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#tres","title":"TRES","text":"<p>Display TRES (Trackable RESources) (will be different on each cluster)</p> <p>TRES can be specified in QOS (fewer in version 22.05 than in 24.xx)</p> <pre><code>sacctmgr show tres\n    Type            Name     ID \n-------- --------------- ------ \n     cpu                      1 \n     mem                      2 \n  energy                      3 \n    node                      4 \n billing                      5 \n      fs            disk      6 \n    vmem                      7 \n   pages                      8 \n    gres             gpu   1001 \n    gres     gpu:tesv100   1002 \n    gres      gpu:titanv   1003 \n    gres     gpu:tesa100   1004 \n    gres    gpu:tesv100s   1005 \n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-sacctmgr/#miscellaneous","title":"Miscellaneous","text":"<pre><code># View the slurmdbd configuration in force\nsacctmgr show Configuration | less\n\n# Perform a self-check of database\nsacctmgr show problems\n\n# Check RPC statistics (watch out for users with extremely high numbers of RPC)\n# (these stats might be since the last time the slurmctld was restarted??)\nsacctmgr show stats | less\n\n# You can clear the stats and then watch how quickly a suspicious user racks up RPC calls\nsacctmgr clear stats\n</code></pre> <p>```</p>","tags":["slurm"]},{"location":"slurm/tips-scontrol/","title":"scontrol -- useful command examples","text":"<p>Scontrol is a command useful to both users and systems administrators.  It is used to display and modify SLURM configuration. It has a number of sub-commands, which take different arguments. See the index section of the manual page for a pointer to the area of interest (what kind of thing you want to see or modify).</p> <p>All  commands and options are case-insensitive, although node names, partition names, and reservation names are case-sensitive. All  commands  and options can be abbreviated to the extent that the specification is unique. </p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#scontrol-for-users","title":"Scontrol for Users","text":"","tags":["slurm"]},{"location":"slurm/tips-scontrol/#show-things","title":"Show things","text":"<pre><code>scontrol show --details job &lt;jobid&gt;\n</code></pre> <pre><code>scontrol show node &lt;nodename&gt;\n</code></pre> <pre><code>scontrol show partition &lt;partitionname&gt;\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#update-jobs","title":"Update Jobs","text":"<p>You can update many aspects of pending jobs, (more items than for running jobs). What follows is only a sample!!! Click here for the complete list.</p> <p>It can be to your benefit to update existing pending jobs rather than <code>scancel</code> them and re-submit them. For example, jobs accrue priority points as they sit waiting in the queue.  Those points are lost if you cancel the job.</p> <p>You can find the commands necessary to add a Quality of Service attribute to a pending job in our QOS document.</p>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#pending-jobs","title":"Pending Jobs","text":"<p>Add a reservation to one of your jobs<pre><code>scontrol update jobid=&lt;jobid&gt; reservation=&lt;reservation-name&gt;\n</code></pre> Remoive a reservation from one of your jobs<pre><code>scontrol update jobid=&lt;jobid&gt; reservation=\n</code></pre></p> Place one of your jobs ahead of other of your jobs<pre><code>scontrol top &lt;jobid&gt;\n</code></pre> Place one of your jobs ahead or behind other of your jobs<pre><code>scontrol update jobid=&lt;jobid&gt; nice=&lt;adjustment&gt; # larger #s decrease the priority\n</code></pre> <p>Set or modify max # of tasks in an array that execute at same time<pre><code>scontrol update jobid=&lt;jobid&gt; ArrayTaskThrottle=&lt;count&gt;\n</code></pre> Users can change the time limit on their pending jobs. After a job starts to run, only a system administrator can adjust the time.</p> Set max job duration<pre><code>scontrol update jobid=&lt;jobid&gt; TimeLimit=&lt;time-specification&gt;\n</code></pre> Hold one of your jobs (to prefer other of your jobs)<pre><code>scontrol hold &lt;job-list&gt;  # Can be comma-separated list of jobids\n</code></pre> Release a held job<pre><code>scontrol release &lt;job-list&gt;  # Can be comma-separated list of jobids\n</code></pre> Lower the priority of one of your jobs (to prefer other of your jobs)<pre><code>scontrol update jobid=&lt;jobid&gt; nice=10\n</code></pre> This is per-node, not per-job. In megabytes<pre><code>scontrol update jobid=&lt;jobid&gt; MinMemoryNode=1024\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#running-jobs","title":"Running Jobs","text":"<p>These can also be used on pending jobs. They're just examples of something you might want to set afterwards.</p> <p>Be notified at 80% of job duration<pre><code>scontrol update jobid=&lt;jobid&gt; mailtype=time_limit_80\n</code></pre> But only if you tell it where to send email<pre><code>scontrol update jobid=&lt;jobid&gt; mailuser=&lt;your-address@jh.edu&gt;\n</code></pre></p>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#scontrol-for-systems-administrators","title":"Scontrol for Systems Administrators","text":"","tags":["slurm"]},{"location":"slurm/tips-scontrol/#resuming-a-node","title":"Resuming a node","text":"<p>Perhaps the most common use is to put a node back into operation that was marked DRAIN by <code>slurmctld</code> because it noticed a problem with a job. There is a <code>resume</code> shell script that accepts a three digit node number.</p> Put a DOWN/DRAIN node back into service<pre><code>scontrol update nodename=compute-112 state=resume reason=\"JHPCE: Fixed sssd problem\"\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#modifying-jobs-or-partitions","title":"Modifying jobs or partitions","text":"<p>It can be useful to modify a user's pending job. You can give a pending job a different priority or add a QOS as well as more normal things like changing the endtime of a running job. The manual page section on updating jobs</p>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#on-the-fly-modifications-to-slurm","title":"On-the-fly modifications to SLURM","text":"<p>One can modify a number of overall configuration parameters normally defined in slurm.conf. Node and partition definitions can be changed, but not the amount of RAM or CPUs on a node!!!</p> <p>However, this change is temporary, and will be overwritten whenever the daemon (<code>slurmctld</code> on the server or <code>slurmd</code> on a client) reads the slurm.conf configuration file. Since multiple system administrators are involved, someone might push out with Ansible a revised configuration file which will restart all of the server and client demons. </p> Display running configuration<pre><code>scontrol show config\nscontrol show config | grep -i debug # show debug, debugflags\n</code></pre> <p>Modify debug level<pre><code>scontrol setdebug info [nodes=&lt;nodelist&gt;]\n# (info is our normal level, so please return to that after debugging)\n# values in order: quiet,fatal,error,info,verbose,debug,debug2 thru debug5\n</code></pre> If you specify any nodes, then that node's slurmd will be modified rather than the master slurmctld. It is unclear which flags will produce meaningful information on compute nodes since most often we are interested in the decisions made by the scheduler, which is in slurmctld. But the debug level is probably useful sometimes.</p> <p>Modify debug flags<pre><code>scontrol setdebugflags + | - FLAG [nodes=&lt;nodelist&gt;]\n# The plus or minus is required\n# Possibly most interesting flags:\n# Backfill, BackfillMap, CPU_Bind, Gres, NodeFeatures, Priority, Reservation, Steps, TraceJobs\n</code></pre> There is a warning that some debugflags require restarting slurmctld, which means using them requires editing slurm.conf (because scontrol's changes are to the run-time environment...). It would be nice if they told us which ones.</p> Modify a partition<pre><code>scontrol update partitionname=interactive allowqos=normal,interactive-default\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#reservations","title":"Reservations","text":"<p>Reservations can be made for a number of purposes and in a number of ways. See the <code>scontrol</code> manual page for more details - this section is about reservations. There is also the Advanced Resource Reservation Guide</p> <p>Can be used to prevent a node from accepting jobs after reboot. (Our nodes have root cronjobs that try to set their state to RESUME at reboot.)</p> <p>Can be used to create repeating reservations, which start and expire at certain times. This can be used to ensure that resources are avilable for class sessions. Note though that the scheduler will prevent jobs from starting if their job duration means they will run over into these windows. (The start time will be reset by the scheduler to be right after the reservation will end (unless the job's duration is such that it will end in another instance of this repeating reservation!!!  This might create situations that are hard for both users and sysadmins to understand.)</p> <p>Another use: A node has file-system mounting issues that cause new jobs which try to use a missing file system to die. But the node has running jobs that you want to allow to finish.  You can set the node to DRAIN. You can also create a reservation, which prevents the node from accepting new jobs, and after attempts to fix it, allows you to verify that things are back to normal.  The reservation name can be self-documenting as to why the node is in the condition it is. SLURM will set nodes into DRAIN on its own for some types of problems it can detect, but it doesn't create reservations automatically.</p> <p>When creating a reservation, you MUST select:</p> <ol> <li>a starttime -- Reservations have start times. You can create reservations that, for example, take nodes out of service a month from now for a planned maintenance window. (The scheduler will prevent jobs from starting if they would run over into the reserved time.)</li> <li>a duration -- reservation need to expire at some point. (Note that \"UNLIMITED\" turns out to be one year.)</li> <li>a list of users OR a group name -- You cannot mix users and groups. If a research group name exists it can be easier than listing many users.</li> <li>the nodes or partitions being reserved</li> </ol> <p>When creating a reservation, you CAN select:</p> <ol> <li>a meaningful name -- <code>scontrol</code> will generate a name mechanically if you don't specify one. That's not very informative. Users will have to specify the name when submitting jobs. Sysadmins have to specify the name to modify or end it. Names cannot contain spaces. Use hyphens to make it more readable.</li> <li>Resources -- Versions of SLURM newer than 22.05.09 may allow the reservation of CPU, RAM and/or GPU resources (via TRES arguments). If so, you could carve out \"space\" for a user to run certain jobs on any of the nodes in a partition without locking everyone out of the partition. That would be handy.</li> </ol> <p>Show existing reservations<pre><code>scontrol show reservation\n</code></pre> Here is a reservation that prevents normal users from using a node for a year, starting now. System administrators are able to use the reservation to test the node.</p> Create a reservation<pre><code>scontrol create reservation starttime=now duration=UNLIMITED user=root,tunison,mmill116,jyang flags=maint,flex,ignore_jobs,NO_HOLD_JOBS_AFTER reservation=resv-name nodes=compute-number\n</code></pre> Add a user to an existing reservation<pre><code>scontrol update reservation=&lt;resv-name&gt; user+=&lt;username&gt;\n</code></pre> Create downtime reservations on entire partitions<pre><code>grep -v \"^#\" /etc/slurm/partitions.conf|grep PartitionName|grep -v DEFAULT|awk '{print $1}'|cut -c 15- &gt; /tmp/mylist\n\nfor part in `cat /tmp/mylist`;\ndo\nscontrol create reservation=power-outage-$part flags=maint,ignore_jobs,part_nodes starttime=2024-07-15T17:00 endtime=2024-07-22T17:00 nodes=all users=root partitionname=${part}\ndone\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#ending-reservations","title":"Ending Reservations","text":"Delete a reservation<pre><code>scontrol delete reservation=&lt;resv-name&gt;\n</code></pre> <p>If you are prevented from deleting a reservation because jobs are running, you can modify the endtime. Note that it will take a few seconds or a minute to see the reservation go away.</p> Another way to delete a reservation<pre><code>scontrol update reservation=&lt;resv-name&gt; endtime=now\n</code></pre>","tags":["slurm"]},{"location":"slurm/tips-scontrol/#flags","title":"Flags","text":"<p>Some flags are important to include. Capitalized here to make them stand out.</p> <ul> <li>MAINT - this is optional. The node's state will include MAINT in addition to other possible states (IDLE, DRAIN, DOWN). The downtime won't be counted by SLURM in stats about cluster uptime. But perhaps more importantly for us: \"This reservation is permitted to use resources that are already in another reservation.\" Although there is also OVERLAP</li> <li>IGNORE_JOBS - Ignore currently running jobs when creating the reservation. Otherwise you will be prevented from creating the reservation.</li> <li>NO_HOLD_JOBS_AFTER - without it pending jobs requesting a reservation will be changed to \"held\" if a reservation ends. Held jobs require manual intervention. There are user and system administration holds -- don't know which is used in the case of not using this flag.</li> <li>MAGNETIC - Allows jobs to be considered for this reservation even if they didn't request it. (Would this allow jobs sent to a partition which has a reservation on it to run if the user forgot or didn't know to include a reservation directive?) </li> <li>FLEX - Permit jobs requesting the reservation to begin prior to the reservation's start time, end after the reservation's end time, and use any resources inside and/or outside of the reservation regardless of any constraints possibly set in the reservation. A typical use case is to prevent jobs not explicitly requesting the reservation from using those reserved resources rather than forcing jobs requesting the reservation to use those resources in the time frame reserved. Another use case could be to always have a particular number of nodes with a specific feature reserved for a specific account so users in this account may use this nodes plus possibly other nodes without this feature.</li> </ul>","tags":["slurm"]},{"location":"slurm/tips-squeue/","title":"squeue -- useful command examples","text":"<p>Warning</p> <p>AS OF 04/30/2024 THIS DOCUMENT CONTAINS NO UNIQUE/USEFUL INFO. This document started as a copy of that for the sacct command. Because formatting output is similar, etc. It will be pruned and modified.</p> <p>Example</p> Show my pending and running jobs<pre><code>squeue --me\n</code></pre> <p>sacct is a command used to display information about pending and running jobs. It has a number of subtleties, such as the formatting of output. We hope that this page will help you get the information you need.</p> <p><code>squeue</code> can be used to investigate jobs' resource usage, nodes used, and exit codes. It can point to important information, such as jobs dying on a particular node but working on other nodes</p> <p>Example</p> <p><code>squeue --sort=-p,i --states=PD # sort PENDING by partition and job priority</code></p> <p>Adds columns for: amount of CPU, RAM requested as well as time job limit.</p> <p>jobid,partition,job name,user,state,time run,time limit,node count,cpu,memory,nodelist or reason</p> <p>Example</p> <p>```squeue -S u,-t,p -o \"%.6P %.8j %.20u %.2t %.10M %.12l %.4D %.4C %.5m %R\" <pre><code>A bash shell routine you can add to your .bashrc:\n```marksq() { (squeue -O JobArrayID:10\" \",username:8\" \",partition:7\" \",name:10\" \",statecompact:3,reason:12\" \",reservation,,PendingTime:7\" \",tres:20|sed 's/,no.*//' | sed 's/TRES_ALLOC[ ]*$/TRES_ALLOC/') }\n</code></pre></p> <p>sacct will show all submitted jobs but cannot, of course, provide data for a number of fields until the job has finished. Use the sstat command to get information about running programs. \"Instrumenting\" your jobs to gather information about them can include adding one or more sstat commands to batch jobs in multiple places.</p> <p>Examples below use angle brackets &lt; &gt;  to indicate where you are supposed to replace argumements with your values.</p>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#sacct-basics","title":"sacct basics","text":"<ol> <li>By default only your own jobs are displayed. Use the <code>--allusers</code> or <code>-a</code> flag if necessary.</li> <li>Only jobs from a certain time window are displayed by default. That window varies in a confusing manner depending the arguments you provide. See this section of the manual page. Therefore it is recommended to always provide start (<code>-S</code>) and end (<code>-E</code>) times to be sure that you are seeing what you expect.</li> <li>You can choose output fields and control their width. </li> <li>Even the simplest of batch jobs contain multiple \"steps\" as far as SLURM is concerned. One of them, named \"extern\" represents the ssh to the compute node on behalf of your job. Job records consist of a primary entry for the job as a whole  as  well as entries for job steps. The Job Launch page has a more detailed description of each type of job step. You may find the <code>-X</code> flag helpful to omit clutter.</li> <li>Regular jobs are in the form: JobID[.JobStep]</li> <li>Array jobs are in the form: ArrayJobID_ArrayTaskID</li> </ol> <p>Warning</p> <p>Sacct retrieves data from a SQL database. Be careful when creating your sacct commands to limit the queries to the information you need. Narrow the search as much as possible.  That database needs to be modified constantly as jobs start and complete, so we don't want it tied up answering sacct queries. If you want to look at a large amount of data in a variety of ways, consider saving the output to a text file and then working with that file.</p>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#command-options-of-note","title":"Command Options of Note","text":"<p>Check the man page. There are other useful options.</p> <ul> <li><code>-X</code>  show stats for the job allocation itself, ignoring steps (try it)</li> <li><code>-R</code> reasonlist  show jobs not scheduled for given reason</li> <li><code>-a</code>  allusers</li> <li><code>-N</code> nodelist  only show jobs which ran on this/these nodes</li> <li><code>-u</code> userlist  only show jobs which ran by this/these users</li> <li><code>--name=</code>namelist - only show jobs with this list of names</li> <li><code>-n</code>  noheader</li> <li><code>-p</code>  parsable  puts a | between fields and at end of line</li> <li><code>-P</code>  parsable2  does not put a | at end of line</li> <li><code>--delimeter</code>  - use that char instead of | for <code>-p</code> or <code>-P</code> <li><code>--units=[KMGTP]</code> - display in this unit</li> <li><code>-k</code> minimum time - looking for jobs with time limits in a range</li> <li><code>-K</code> maximum time - looking for jobs with time limits in a range</li> <li><code>-q</code> qoslist - list of qos used</li>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#start-and-end-times","title":"Start and End Times","text":"<p>It is best to use always specify a <code>-S</code> start time and a <code>-E</code> end time.</p> <p>Special time words: today, midnight, noon, now</p> <p>now[{+|-}count[seconds(default)|minutes|hours|days|weeks]]</p> Examples: <code>now-3day</code> <code>now-2hr</code> <p>Valid time formats are:</p> <pre><code>               HH:MM[:SS][AM|PM]\n               MMDD[YY][-HH:MM[:SS]]\n               MM.DD[.YY][-HH:MM[:SS]]\n               MM/DD[/YY][-HH:MM[:SS]]\n               YYYY-MM-DD[THH:MM[:SS]]\n</code></pre>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#job-state-values","title":"Job State Values","text":"<p>Using the <code>-s &lt;state&gt;</code> option, you can prune your search by looking for only jobs which match the state you need, such as F for failed. (All of these work: f, failed, F, FAILED)</p> <p>Warning</p> <p>Different steps of a job can have different end states. For example the \"extern\" step is often COMPLETED when the \"batch\" and overall steps are FAILED</p> <p>See this section of the manual page, which has also been saved to a text file you can copy for your own reference <code>/jhpce/shared/jhpce/slurm/docs/job-states.txt</code></p> <p>Primary job states of interest:</p> <ul> <li>CA CANCELLED</li> <li>CD COMPLETED</li> <li>F FAILED</li> <li>OOM OUT_OF_MEMORY</li> <li>PD PENDING</li> <li>R RUNNING</li> <li>TO TIMEOUT</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#available-fields","title":"Available fields","text":"<p>Field meanings are explained in this section of the manual page.</p> What output fields are available?<pre><code>sacct -e\n</code></pre> See all fields for a job<pre><code>sacct -o ALL -j &lt;jobid&gt;\n</code></pre>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#formatting-fields","title":"Formatting fields","text":"<p>You can put a %NUMBER after a field name to specify how many characters should be printed, e.g.</p> <ul> <li>format=name%30 will print 30 characters of field name right justified.  </li> <li>format=name%-30 will print 30 characters left justified.</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can define environment variables in your shell to reduce the complexity of issuing sacct commands. You can also set these in shell scripts. Command line options will always override these settings.</p> <p>SACCT_FORMAT</p> <p>SLURM_TIME_FORMAT</p>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#formatting-datestimes","title":"Formatting Dates/Times","text":"<p>You can use most variables defined by the STRFTIME(3) system call. This web page is a starting point, but what SLURM has chosen to implement may not match.</p> <ul> <li>%a - abbrieviated name of day of the week</li> <li>%m - month as decimal, 01 to 12</li> <li>%d - day of month as decimal</li> <li>%H - hour as decimal in 24-hour notation</li> <li>%M - minute as decimal, 00 to 59</li> <li>%T - time in 24-hour notation (%H:%M:%S)</li> </ul> <p>Day of week MM-DD HH:MM<pre><code>export SLURM_TIME_FORMAT=\"%a %m-%d %H:%M\" \n</code></pre> The start and end field widths show below are suitable for the time format shown above.</p> Resources requested, used<pre><code>export SACCT_FORMAT=\"user,jobid,jobname,nodelist%12,start%-20,end%-20,state%20,reqtres%40,TRESUsageInTot%200\"\n</code></pre>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#output-fields-of-interest","title":"Output Fields of Interest","text":"<p>These fields are probably the ones you'll want. See this section of the manual page for the list and their meaning. Capitalization does not matter; it is used for readability.</p> <ul> <li>TRES means Trackable RESources, such as RAM and CPUs.</li> <li>A number of fields (not listed) are available to tell you on which node a maximum occurred. Similarly there are fields to tell you minimum, average and maximum values for some items.</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#basics","title":"Basics","text":"<ul> <li>User</li> <li>JobId</li> <li>JobName</li> <li>Partition</li> <li>State</li> <li>ExitCode</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#times","title":"Times","text":"<ul> <li>Submit</li> <li>Start</li> <li>Elapsed - in format [DD-[HH:]]MM:SS</li> <li>End</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#nodes","title":"Nodes","text":"<ul> <li>AllocCPUS</li> <li>AllocNodes</li> <li>NNodes - number of nodes requested/used</li> <li>NodeList - </li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#resources-requested","title":"Resources Requested","text":"<ul> <li>ReqTRES # this is what you will be billed for</li> <li>ReqNodes</li> <li>ReqCPUS</li> </ul>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#resources-consumed","title":"Resources Consumed","text":"<ul> <li>TRESUsageInTot</li> <li>CPUTime - (elapsed)*(AllocCPU) in HH:MM:SS format</li> <li>MaxRSS - Max resident set of all tasks in job</li> <li>MaxVMSize - Max virtual memory of all tasks in job</li> <li>MaxDiskRead - Number bytes read by all tasks in job</li> <li>MaxDiskWrite - Number bytes written by all tasks in job</li> </ul> <p>Virtual Memory Size (VMSize) is the total memory size of a job. It includes both memory actually in RAM (the RSS) and parts of executabilities which were not needed to be read in off of disk into RAM. Because, for example, routines in dynamically linked libraries were never called, so those libraries were not loaded. </p> <p>RSS - resident set size (RSS) is the portion of memory (measured in megabytes) occupied by a job that is held in main memory (RAM). The rest of the memory required by the job exists in the swap space or file system, either because some parts of the occupied memory were paged out, or because some parts of the executable were never loaded.</p>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#exit-error-codes","title":"Exit Error Codes","text":"<p>In addition to the job's \"state\", SLURM also records error codes. Unfortunately their Job Exit Codes page doesn't provide a meaning for the numerical values.</p> <p>Error <code>0:53</code> often means that something wasn't readable or writable. For example, job output or error files couldn't be written in the directory in which the job ran (or where you told SLURM to put them with a directive).</p> <pre><code>a guide for exit codes:\n\n0 \u2192 success\nnon-zero \u2192 failure\nExit code 1 indicates a general failure\nExit code 2 indicates incorrect use of shell builtins\nExit codes 3-124 indicate some error in job (check software exit codes)\nExit code 125 indicates out of memory\nExit code 126 indicates command cannot execute\nExit code 127 indicates command not found\nExit code 128 indicates invalid argument to exit\nExit codes 129-192 indicate jobs terminated by Linux signals\nFor these, subtract 128 from the number and match to signal code\nEnter kill -l to list signal codes\nEnter man signal for more information\n</code></pre>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/tips-squeue/#diagnostic-arguments","title":"Diagnostic Arguments","text":"<p>These can be useful to double-check what someone actually did.</p> See the full command issued to submit the job<pre><code>sacct -o SubmitLine -j &lt;jobid&gt;\n</code></pre> See batch file used<pre><code>sacct -B -j &lt;jobid&gt;\n</code></pre> Directory used by the job to execute commands<pre><code>sacct -o WorkDir -j &lt;jobid&gt;\n</code></pre> See jobs given a time limit btwn 1min &amp; 1 day<pre><code>sacct -k 00:01 -K 1-0\n</code></pre>","tags":["needs-to-be-written","jeffrey","slurm"]},{"location":"slurm/user-guide-collection/","title":"SLURM USER GUIDES","text":"<p>Yale: their page</p> <p>Umich: their page</p> <p>New Mexico State Univ: their page</p> <p>BIH: their page</p> <p>C.E.C.I: their page (they have many good pages)</p> <p>Texas Advanced Computing Center: their Frontera cluster's home page</p> <p>Arctic Univ of Norway: their page</p>","tags":["slurm","in-progress"]},{"location":"slurm/whenstart/","title":"Factors Affecting Job Scheduling","text":"","tags":["slurm"]},{"location":"slurm/whenstart/#overview","title":"Overview","text":"<p>One of SLURM's primary functions is to schedule jobs so they run on various nodes. Running jobs only in the order that they are submitted on nodes in hostname order is one approach the scheduler could follow, but it turns out to be inefficient. That also doesn't allow organizations to implement policies to favor some jobs over others. So schedulers like SLURM have incorporated many features over the decades which help make maximum use of the cluster's resources and implement other goals.</p> <p>What are some of the things that go into these decisions?</p> <p>How does it choose which of a set of pending jobs to start in what order on which node and which CPU cores on the node?</p> <p>There are a number of vendor documents which document scheduling. See the \"Workload Prioritization\" and \"Slurm Scheduling\" sections at this site.</p>","tags":["slurm"]},{"location":"slurm/whenstart/#tldr","title":"TL;DR","text":"<p>If you have used a lot of resources in public partitions recently, (in the last week, especially over the last three days), your jobs will have a lower priority than someone who has been idle.</p> <p>No matter what priority is assigned or why, your jobs will start faster if you request the fewest resources required for their success, including duration.</p> <p>Smaller jobs \"fit\" into more slots between other jobs than larger jobs, so consider whether you can divide up your work. You should also direct your job to the most appropriate partition. For example, we have an interactive partition for small jobs.</p> <p>If the scheduler has been able to determine an estimated start date for your job, it will be shown in the output of</p> Start time estimate<pre><code>squeue --me --start\n</code></pre>","tags":["slurm"]},{"location":"slurm/whenstart/#backfill","title":"Backfill","text":"<p>In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 30 cores and it will have to wait 20 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that shorter job in the meantime. This GREATLY enhances utilization of the cluster.</p> <p>For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. </p>","tags":["slurm"]},{"location":"slurm/whenstart/#priority","title":"Priority","text":"<p>Cluster-specific</p> <p>As of 20240320, multifactor priority is not enabled on the C-SUB.</p> <p>Multiple factors are used to assign a single priority value to each job. This is described in the Multifactor Job Priority document.</p> <p>(This priority is only used to decide which jobs to dispatch first. It is not used to set a UNIX process <code>nice</code> value on the processes created by jobs out on the compute nodes.)</p> <p>Once a job starts running, its priority no longer has much meaning.</p> <p>The job's priority is an integer that ranges between 0 and 4,294,967,295. The larger the number, the higher the job will be positioned in the queue, and the sooner the job will be scheduled and started. A job's priority, and hence its order in the queue, can vary over time.</p> <p>The final priority is determined by multiplying pairs of (weights and factors) and adding the results. Factors range from 0.0 to 1.0. Weights range from 0 to 65,533.</p> <p>Currently we are using three components: Age, Fairshare and Partition</p> <p>Tip</p> <p>You can see pending job's priority values and the contributors to the final value with the <code>sprio</code> command. This sorts jobs by total prio, partition, user. Pending jobs sorted by priority<pre><code>sprio -S -y,p,u | less\n</code></pre></p> <p>Tip</p> <p>A better formatted of that command which prints only the factors we are currently using<sup>1</sup> is:</p> Pending jobs sorted by priority, well-formatted<pre><code>sprio -o \"%.15i %9r %.8u %.10Y %.10A %.10F %.10P\" -S -y,p,u\n</code></pre> <p>Tip</p> <p>You can change the priority of your jobs among your jobs with <code>scontrol</code> commands like <code>top</code> and <code>nice</code>. See this document for details. </p>","tags":["slurm"]},{"location":"slurm/whenstart/#fairshare","title":"Fairshare","text":"<p>To help provide equitable access to the public partitions of the cluster, the FAIRSHARE priority component is based on your recent usage of those partitions. If you have used fewer CPU minutes than someone else in the last week, then your jobs will receive a higher fairshare value.</p> <p>The fairshare priority is the result of multiplying a weight stored in a variable, PriorityWeightFairshare, and a factor which is derived from the accounting database.</p> <p>Tip</p> <p>You inspect fairshare values for ALL users with this command:</p> Fairshare values<pre><code>sshare -a | sort -k7nr | less\n</code></pre> <p>You should focus on the values in the right-hand-most column. Heaviest users of the cluster in recent days have values closer to 0.0. People who haven't run any jobs lately will have values closer to 1.0. Jobs submitted by the latter will be given higher fairshare priority values.</p>","tags":["slurm"]},{"location":"slurm/whenstart/#age","title":"Age","text":"<p>In addition to fairshare, any pending job will accrue AGE priority over time. Currently (20240217) this maxes out to 100 over the course of a week.</p> <p>Job arrays which started running tasks many days ago will wind up with high age priority values for all of their future tasks. You can see that fairshare somewhat counteracts that age advantage.</p> <p>If you decide that you want to change something about a pending job, consider whether you can do so using <code>scontrol</code> commands as described here instead of killing the job with <code>scancel</code> and resubmitting it. That would preserve the age priority your job has accrued.</p>","tags":["slurm"]},{"location":"slurm/whenstart/#partition-priority","title":"Partition Priority","text":"<p>We have set this experimentally on the <code>interactive</code> partition to try to aid in quick access to (small) interactive sessions.</p> <ol> <li> <p>This command's output will be incomplete if we begin using other priority factors.\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"slurm/includes/all-job-states/","title":"All job states","text":"<p>FROM https://slurm.schedmd.com/squeue.html#lbAG</p> BF BOOT_FAIL Job terminated due to launch failure, typically due to a hardware failure (e.g. unable to boot the node or block and the job can not be requeued). CA CANCELLED Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. CD COMPLETED Job has terminated all processes on all nodes with an exit code of zero. CF CONFIGURING Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG COMPLETING Job is in the process of completing. Some processes on some nodes may still be active. DL DEADLINE Job terminated on deadline. F FAILED Job terminated with non-zero exit code or other failure condition. NF NODE_FAIL Job terminated due to failure of one or more allocated nodes. OOM OUT_OF_MEMORY Job experienced out of memory error. PD PENDING Job is awaiting resource allocation. PR PREEMPTED Job terminated due to preemption. R RUNNING Job currently has an allocation. RD RESV_DEL_HOLD Job is being held after requested reservation was deleted. RF REQUEUE_FED Job is being requeued by a federation. RH REQUEUE_HOLD Held job is being requeued. RQ REQUEUED Completing job is being requeued. RS RESIZING Job is about to change size. RV REVOKED Sibling was removed from cluster due to other cluster starting the job. SI SIGNALING Job is being signaled. SE SPECIAL_EXIT The job was requeued in a special state. This state can be set by users, typically in EpilogSlurmctld, if the job has terminated with a particular exit value. SO STAGE_OUT Job is staging out files. ST STOPPED Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. S SUSPENDED Job has an allocation, but execution has been suspended and CPUs have been released for other jobs. TO TIMEOUT Job terminated upon reaching its time limit."},{"location":"slurm/includes/all-pending-reasons/","title":"All pending reasons","text":"<p>AssocGrp*Limit The job's association has reached an aggregate limit on some resource.</p> <p>AssociationJobLimit The job's association has reached its maximum job count.</p> <p>AssocMax*Limit The job requests a resource that violates a per-job limit on the requested association.</p> <p>AssociationResourceLimit The job's association has reached some resource limit.</p> <p>AssociationTimeLimit The job's association has reached its time limit.</p> <p>BadConstraints The job's constraints can not be satisfied.</p> <p>BeginTime The job's earliest start time has not yet been reached.</p> <p>Cleaning The job is being requeued and still cleaning up from its previous execution.</p> <p>Dependency This job has a dependency on another job that has not been satisfied.</p> <p>DependencyNeverSatisfied This job has a dependency on another job that will never be satisfied.</p> <p>FrontEndDown No front end node is available to execute this job.</p> <p>InactiveLimit The job reached the system InactiveLimit.</p> <p>InvalidAccount The job's account is invalid.</p> <p>InvalidQOS The job's QOS is invalid.</p> <p>JobHeldAdmin The job is held by a system administrator.</p> <p>JobHeldUser The job is held by the user.</p> <p>JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc.</p> <p>Licenses The job is waiting for a license.</p> <p>NodeDown A node required by the job is down.</p> <p>NonZeroExitCode The job terminated with a non-zero exit code.</p> <p>PartitionDown The partition required by this job is in a DOWN state.</p> <p>PartitionInactive The partition required by this job is in an Inactive state and not able to start jobs.</p> <p>PartitionNodeLimit The number of nodes required by this job is outside of its partition's current limits. Can also indicate that required nodes are DOWN or DRAINED.</p> <p>PartitionTimeLimit The job's time limit exceeds its partition's current time limit.</p> <p>Priority One or more higher priority jobs exist for this partition or advanced reservation.</p> <p>Prolog Its PrologSlurmctld program is still running.</p> <p>QOSGrp*Limit The job's QOS has reached an aggregate limit on some resource.</p> <p>QOSJobLimit The job's QOS has reached its maximum job count.</p> <p>QOSMax*Limit The job requests a resource that violates a per-job limit on the requested QOS.</p> <p>QOSResourceLimit The job's QOS has reached some resource limit.</p> <p>QOSTimeLimit The job's QOS has reached its time limit.</p> <p>QOSUsageThreshold Required QOS threshold has been breached.</p> <p>ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available.</p> <p>Reservation The job is waiting its advanced reservation to become available.</p> <p>Resources The job is waiting for resources to become available.</p> <p>SystemFailure Failure of the Slurm system, a file system, the network, etc.</p> <p>TimeLimit The job exhausted its time limit.</p> <p>WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</p>"},{"location":"slurm/includes/common-job-states/","title":"Common job states","text":"Short Name Long Name Explanation PD PENDING Job is waiting to start R RUNNING Job is currently running CG COMPLETING Job has ended, clean-up has begun CD COMPLETED Job finished normally, with exit code 0 F FAILED Job finished abnormally, with a non-zero exit code CA CANCELLED Job was cancelled by the user or a sysadmin OOM OUT_OF_MEMORY Job was killed for exceeding its memory allocation TO TIMEOUT Job was killed for exceeding its time limit"},{"location":"slurm/includes/common-pending-reasons/","title":"Common pending reasons","text":"Name Explanation Notes BeginTime The job's earliest start time has not yet been reached Dependency Job waiting on a user-defined dependency See why with <code>showjob &lt;jobid&gt;|grep -i depend</code> DependencyNeverSatisfied Something went wrong You should investigate why. Incorrect dependency specified? JobArrayTaskLimit Array job configured to run only so many tasks at a time Good way to control resource use JobHeldAdmin The job is held by a system administrator JobHeldUser The job is held by the user Priority One or more higher priority jobs exist for this partition or advanced reservation QOSJobLimit The job's QOS has reached its maximum job count QOSMaxCpuPerUserLimit Your other running jobs have consumed your CPU quota <code>slurmuser --me</code> will show your used/pending resources QOSMaxMemoryPerUser Your other running jobs have consumed your RAM quota <code>slurmuser --me</code> will show your used/pending resources Reservation Job waiting for its advanced reservation to become available Resources Needed resources not currently available in partition <code>slurmpic -p &lt;partition&gt;</code> can show you what's used &amp; consumed in that partition ReqNodeNotAvail Some node specifically required by the job is not currently available Usu seen when all nodes in a partition are unavailable"},{"location":"slurm/includes/sacct-output-fields/","title":"Sacct output fields","text":"<pre><code>Account             AdminComment        AllocCPUS           AllocNodes         \nAllocTRES           AssocID             AveCPU              AveCPUFreq         \nAveDiskRead         AveDiskWrite        AvePages            AveRSS             \nAveVMSize           BlockID             Cluster             Comment            \nConstraints         ConsumedEnergy      ConsumedEnergyRaw   Container          \nCPUTime             CPUTimeRAW          DBIndex             DerivedExitCode    \nElapsed             ElapsedRaw          Eligible            End                \nExitCode            Flags               GID                 Group              \nJobID               JobIDRaw            JobName             Layout             \nMaxDiskRead         MaxDiskReadNode     MaxDiskReadTask     MaxDiskWrite       \nMaxDiskWriteNode    MaxDiskWriteTask    MaxPages            MaxPagesNode       \nMaxPagesTask        MaxRSS              MaxRSSNode          MaxRSSTask         \nMaxVMSize           MaxVMSizeNode       MaxVMSizeTask       McsLabel           \nMinCPU              MinCPUNode          MinCPUTask          NCPUS              \nNNodes              NodeList            NTasks              Partition          \nPriority            QOS                 QOSRAW              Reason             \nReqCPUFreq          ReqCPUFreqGov       ReqCPUFreqMax       ReqCPUFreqMin      \nReqCPUS             ReqMem              ReqNodes            ReqTRES            \nReservation         ReservationId       Reserved            ResvCPU            \nResvCPURAW          Start               State               Submit             \nSubmitLine          Suspended           SystemComment       SystemCPU          \nTimelimit           TimelimitRaw        TotalCPU            TRESUsageInAve     \nTRESUsageInMax      TRESUsageInMaxNode  TRESUsageInMaxTask  TRESUsageInMin     \nTRESUsageInMinNode  TRESUsageInMinTask  TRESUsageInTot      TRESUsageOutAve    \nTRESUsageOutMax     TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutMin    \nTRESUsageOutMinNode TRESUsageOutMinTask TRESUsageOutTot     UID                \nUser                UserCPU             WCKey               WCKeyID\n</code></pre>"},{"location":"storage/","title":"STORAGE OVERVIEW","text":"","tags":["topic-overview"]},{"location":"storage/#types-of-storage","title":"Types of storage","text":"<p>There are three types of data storage spaces available on the JHPCE cluster.</p> TypeExample PathQuota?UseCost Home directory/users/USERID100GBSmall datasets, Programs, Applications$350/TB/yr - max $35/yr if 100GB used Project space/dcs0?/*grpname*/dataSize of the purchased allocationResearch dataBetween $25/TB/yr and $40/TB/yr Scratch space$MYSCRATCH1TBTemporary filesFree","tags":["topic-overview"]},{"location":"storage/#home-directories","title":"Home directories","text":"<p>All users have access to their own personal home directory space.  The path to your home directory space is /users/USERID.  By default, only you have access to you home directory.</p> <p>Home directories can be used for storing programs you write, data that you will be working with, or applications that you need to run.  In a Linux shell, your home directory can be referred to by ~ or $HOME</p> <p>All home directories have a 100 GB quota.  You can see how much space you are using by running the \"hquota\" command.  This informaiton is also shown to you each time you login to the cluster. If you are finding that you need more space than the 100GB provided, please email us at bitsupport@lists.jhu.edu. We will work with you to find additional available space to use. This may include utilizing your 1TB of temporary scratch space </p> <p>Home directories are backed up, but other storage areas are probably not. So if you are working in another directory, and you are generating unique data, you should copy it to your home directory, or copy it off of the JHPCE cluster.</p> <p>For home directory space you are only charged for the actual storage you are using at a rate of $0.35/GB/yr.  As you add or remove files, your charge will increase or decrease based on your usage. Home directory space is already compressed on the storage server, so you won't save any money by compressing your files with <code>tar -czf</code> etc.</p>","tags":["topic-overview"]},{"location":"storage/#project-spaces","title":"Project Spaces","text":"<p>Every 12 - 18 months, a new large storage array is purchased for the JHPCE cluster, and allocations for these storage spaces are sold to  the various PIs or groups that need storage space.  As part of the planning process for bringing a new storage array online, we will reach out to all active PIs on the cluster, and survey them for their expected storage needs. We will then size the new storage array based on those needs.</p> <p>Our last large storage build was in 2023, for the DCS07 storage array.  We do still have some unsold capacity on this array, so please reach out to us at bitsupport@lists.jhu.edu if you have a need for additional storage.</p> <p>As of 2024-05-01, the currect project storage arrays in places are:</p> Storage NameYear Built# of DisksDisk Size# of JBODsUseable SpaceCostCost per TB DCL0220184408TB102.4PB$164,870.14$66.57 DCS04202072012TB105.0PB$202,451.29$40.45 DCS05202148020TB86.2PB$240,881.36$38.83 DCS062021167.68188TB$34,207.00$305.17 DCS07202330022TB54.8PB$145,453.99$30.61","tags":["topic-overview"]},{"location":"storage/#notes","title":"Notes:","text":"<pre><code>- Part of DCS04 was used for legacy-dcs01 space\n- Part of DCS05 was used for legacy-dcl01 space\n- SSD-based array used by CSUB cluster\n</code></pre> <p>Other Storage Arrays currently in use on the JHPCE cluster:</p> Storage NameUseYear Built# of DisksDisk Size# of JBODsUseable SpaceCostCost per TB DCS02/home,/jhpce,/legacy2016406TB1172TB$21,168.50$122.50 DCS03Backups20174504TB,6TB102.1PB$136,919.94$62.55 FastscratchScratch2018241TB124TB$17,983.45$749.29 <p>These storage arrays are built on the Dirt Cheap Storage and Dirt Cheap Lustre architecutre, hence the \"DCS\" and \"DCL\" names, as described in this paper from 2013. The storage arrays have been historically built on commodity servers and large JBODS (Just a Bunch Of Disks) from Supermicro, and run ZFS with Lustre on top of it in the case of the DCL arrays.</p>","tags":["topic-overview"]},{"location":"storage/#scratch-space","title":"Scratch Space","text":"<p>We have a \"Fastscratch\" storage array that can be used by all users for storing files for less than 30 days. This storage array is in place to provide an additional 1TB of temporary storage space beyond the 100GB of home directory space. It is not for long-term storage of data, and all files older than 30 days are purged from this \"Fastscratch\" space.  More information about using Fastscratch, including important restrictions, can be found here.</p>","tags":["topic-overview"]},{"location":"storage/#backing-up-storage","title":"Backing up storage","text":"<p>You need to ensure that you have copies of your most vital files located somewhere else. See this document for more information.</p>","tags":["topic-overview"]},{"location":"storage/#encrypted-filesystem","title":"Encrypted filesystem","text":"<p>Encrypted filesystem are used to provide \u201cEncryption At Rest\u201d, meaning that the data on disk will be safely stored in an encrypted format, and only available in an unencrypted state when the data is accessed by an approved user. This may be desireable when working with more sensitive data sources, or where Data Use Agreements require \u201cEncryption At Rest\u201d.</p> <p>The JHPCE Cluster currently supports the following mechanisms for providing encrypted filesystems:</p> <ul> <li>Userspace encrypted filesystems using encfs. See ENCFS on JHPCE.</li> <li>If need be, a Project Storage space can be encrypted with ZFS encryption.</li> <li>The <code>DCL02</code> storage array is built on encrypted disk devices.</li> </ul>","tags":["topic-overview"]},{"location":"storage/#deprecated-storage-arrays","title":"Deprecated Storage Arrays:","text":"<p>For historicla purposes, these are the storage arrays that have been used over  time on the JHPCE cluster: <sup>information deemed reliable but not guaranteed</sup></p> Storage Name Year Built Year Decom. # of Disks Disk Size # of JBODs Total Useable Space Cost Cost per TB DCL01+exp 2015 2023 440 8TB 20 3.4PB $164,870.14 $66.57 DCS01 2013 2021 360 3TB 8 688TB $109,961.00 $159.82 amber03 2012 2020 72 2TB 2 100TB $64,861.00 $648.61 amber02 2011 2016 24 1TB 1 16TB $14,730.00 $920.62 dexter 2011 2016 12 1TB 1 30TB $13,690.00 $456.33 thumper02 2010 2016 24 500G 1 16TB $21,025.00 $1314.06 amber01 (/home) 2009 2016 96 500GB+1TB 2 72TB $92,984.00 $1291.44 nexsan2 2009 2016 12 2TB 1 12TB $14,436.00 $1203.00 thumper01 2008 2016 24 500G 1 16TB $17,079.00 $1067.00 nexsan1 2006 2016 12 1TB 1 6TB $19,060.00 $3176.66","tags":["topic-overview"]},{"location":"storage/backups-restores/","title":"Backups and Restores","text":""},{"location":"storage/backups-restores/#caveat","title":"Caveat","text":"<p>We try to protect your data, but ultimately you need to keep copies of your most vital files elsewhere.</p>"},{"location":"storage/backups-restores/#home-directories","title":"Home directories","text":"<p>We do backup all users' home directories on a nightly basis in 2 manners. First we use ZFS snapshots to create a point-in-time view of the home directory.  We maintain 2 week's work of snapshots, so if you need to recover a file from your home directory to a state that it was in within the last 2 weeks, you can restore you files using the instructions in the Self Service Restores section below.</p> <p>We also do nightly disk-to-disk backups from the cluster located at Bayview to a storage array in the BSPH building. We retain snapshots of these backups using what is commonly called a Grandfather-Father-Son (GFS) backups scheme (or Grandparent-Parent-Child using non-gendered terminology). </p> <p>So, we have daily backups that we keep for 1 week, weekly backups (taken on Sunday) that we keep for 1 month, monthly backups (taken on the 1st of the month) that we keep for 1 year, and yearly backups (taken on January 1st) that we keep for 10 years.  So we wouldn't be able to recover a file as it looked on a specific date, say October 18, 2019, but we would have the file as it was on January 1st 2019 or 2020. </p> <p>File recovery from the backup server would be done via a request to bitsupport@lists.jhu.edu.</p>"},{"location":"storage/backups-restores/#backing-up-other-filesystems","title":"Backing up other filesystems","text":"<p>In addition to home directories, we also have a number of PIs that have requested to have backups of their DCS and/or DCL spaces. We do charge for backing up project space, and backup space is billed space is via the standard JHPCE quarterly billing process. The cost has historically been about $20/TB/yr.  If you are interested in backing up your project storage space, please email us at bitsupport@lists.jhu.edu.</p>"},{"location":"storage/backups-restores/#self-service-restores","title":"Self Service Restores","text":"<p>We make snapshots of the /users file system for fourteen days. You can restore files you have deleted recently by changing directory to the appropriate location and then copying the file or files back to your home directory (or anywhere else you desire).</p> <p>At any one time there are fourteen subdirectories in the path <code>/users/.zfs/snapshot</code></p> <p>Here is an example of looking through the collection of snapshots to find copies of a file you want to restore. Let's say that you deleted a file inside your home directory named susan that was stored in the absolute path <code>/users/your-userid/bob/frank/susan</code> You can see from the <code>ls -ld</code> output when the file existed and also the size of the possibly various versions of that file across the collection of snapshots (perhaps you changed it several times in the last two weeks).</p> <pre><code>cd /users/.zfs/snapshot\nls -ld */your-userid/bob/frank/susan\ncp -p 2024-02-16-23:00/your-userid/bob/frank/susan $HOME/restored-susan\n</code></pre> <p>If restoring substantial amounts of data, please do that work on a compute node instead of a login node. Thank you.</p>"},{"location":"storage/fastscratch/","title":"Fastscratch","text":"<p>A 22TB file system created from fast Solid State Disk is available for your use. This provides a fast place to store input or output files for your compute jobs. There is no cost for using your personal scratch space.</p> <p>Note</p> <p>This scratch space is meant to be a short-term storage location; it is not a long-term storage solution. Please remove data from your personal scratch space once you have finished using it.</p> <p>You can access your scratch space by using the <code>$MYSCRATCH</code> environment variable from an interactive cluster node session, or within a submitted job.</p> <p>The actual absolute path to your personal scratch space is <code>/fastscratch/myscratch/$USER</code>.</p>"},{"location":"storage/fastscratch/#key-details","title":"Key details","text":"<p>Because this limited resource is shared by all users, there are some very important restrictions for using it.</p> <ul> <li>There is a generous 1TB quota set on the personal scratch space. (See this document for more information about disk quotas.)</li> <li>All files older than 30 days will be removed without exception.  </li> <li>Even though there is a 30 day automatic deletion of data, we ask that you please remove data from your personal scratch space once you have finished using it. If only 22 users used their full quotas and left their files behind counting on automatic deletion then the file system would fill up and many jobs would fail.</li> <li>The personal scratch space is not backed up. Therefore if you delete a file it cannot be recovered.</li> <li>The fastscratch file system has NO redundancy, so scratch space drive failures result in data loss. Thus, move important files needed to be kept from scratch space ASAP.</li> <li>Abuse of this space may result in files being deleted on an as needed basis.</li> </ul> <p>Danger</p> <p>If you <code>untar</code> or <code>unzip</code> an archive file, and the extracted files have a timestamp older than 30 days from the original bundle, they will be removed when the daily purge begins. To work around this, you can use the <code>touch</code> command to update the timestamp on the extracted files.</p>"},{"location":"storage/fastscratch/#use-chains-of-jobs-to-stage-data","title":"Use chains of jobs to stage data","text":"<p>By using the dependent jobs feature of SLURM, you can arrange a series of jobs to stage data into and out of /fastscratch.</p> <p>This document has information about dependent jobs https://jhpce.jhu.edu/slurm/crafting-jobs/#dependent-jobs</p> <p>You can use a batch job to copy files from project storage space (e.g. /dcs07/something) or your home directory as they are needed. Or even download a file from the internet via a job submitted to the <code>transfer</code> partition.</p> <p>Then you can create a second batch job that starts automatically when the copy batch job completes successfully by making the second job dependent on the first one using the right condition tag.</p> <p>Third or later jobs can be run to clean out your finished files, whether by deleting them or creating archive files and copying them to a permanent location.</p>"},{"location":"storage/quotas/","title":"Disk Quotas","text":"<p>Disk quotas are used to control disk space for certain file systems. We use \"hard\" quotas. You are not allowed to use more than your quota.  </p> <p>Danger</p> <p>Reaching your disk quota can become an obstacle of simply logging in, as even a small file needed to record some detail about your login session, such as $HOME/.Xauthority, cannot be created. Keep your usage below your quota cap.</p> <p>We use ZFS file systems for large volumes. Unfortunately, ZFS does not provide an end-user quota command with which to inspect your usage and remaining space.</p> <p>Therefore we have configured our login nodes to display your home directory disk consumption and quota during the login process.</p> <p>The figure shown during login are updated periodically. Every 30 minutes to an hour.</p> <p>Tip</p> <p>We have written a <code>getquota</code> command, which will look up your or someone else's quota.</p>"},{"location":"storage/quotas/#home-directory","title":"Home Directory","text":"<p>In the JHPCE cluster, this quota is set to 100GB.</p> <p>In the C-SUB cluster, this quota is set to 500GB.</p>"},{"location":"storage/quotas/#file-deletion-and-delayed-change-in-quota","title":"File Deletion and Delayed Change in Quota","text":"<p>When you delete files you may not see an immediate change in your disk consumption as far as the disk quota system is concerned.</p> <p>You can see how much space you are using in your home directory with the commands</p> <pre><code>cd\ndu -sh .\n</code></pre> <p>We use ZFS snapshots for home directories to make automated backups once an day<sup>1</sup>. These are kept for a period of time<sup>2</sup> so users and systems administrators can perform restores. See this document for instructions on performing your own restores!!!</p> <p>Snapshots work by making a record of your files at an instant in time.  They take zero space at first. As your files change, disk space is consumed to hold the changed material. Snapshot consumption is counted as part of your disk quota.</p> <p>Therefore it can take a number of days<sup>2</sup> for files you have changed in the past but now deleted to stop being counted against your quota.</p> <p>As you can imagine, the rate of change and size of files involved determines the amount of space held in snapshots.</p> <p>If you find yourself in the position where you have run into your disk quota limit, have deleted significant amounts of files but are still impacted by your snapshot'ed files, please email us at bitsupport with the details. We will give you a temporary increase in disk quota to accomodate the snapshot \"overhang\"</p>"},{"location":"storage/quotas/#fastscratch","title":"Fastscratch","text":"<p>The /fastscratch file system has a 1TB quota per user.</p> <p>We have defined in the standard environment a variable <code>$MYSCRATCH</code> for users to use to access their space. (The actual absolute path to your personal scratch space is <code>/fastscratch/myscratch/$USER</code>)</p> <p>There is no reporting system currently available to display fastscratch disk usage and your quota. You can use these commands to view your current usage:</p> <p><pre><code>cd $MYSCRATCH\ndu -sh .\n</code></pre> For more information on using fastscratch, please see This page</p> <ol> <li> <p>At eleven pm (as of 20240215).\u00a0\u21a9</p> </li> <li> <p>Fourteen days (as of 20240215).\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"storage/storage-tips/","title":"STORAGE TIPS","text":""},{"location":"storage/storage-tips/#how-much-disk-space-are-my-files-using","title":"How much disk space are my files using?","text":"<p>There are several ways to get information about the amount of disk space used by files and directories in a Linux environment.</p> <p>The most common way to see how much space a file is using is with the <code>ls -l</code> command.</p> <p><pre><code>[login31 /users/mmill116]$ ls -l .bash_history \n-rw------- 1 mmill116 mmi 586047 Apr 19 12:35 .bash_history\n</code></pre> The middle column of numbers gives the size in bytes.  So, my .bash_history file is 586,047 bytes in size.</p> <p>You can also use the <code>du</code> (disk usage) command to find the total disk space usage for files and directories.  By default, the <code>du</code> command will report the size of the file files in KB.  For example: <pre><code>[login31 /users/mmill116]$ du .bash_history\n229 .bash_history\n</code></pre> reports that my .bash_history is using 229KB of space. Now, you may have noticed that the output of <code>du</code> doesn't match the value shown in <code>ls -l</code>. This is due to the fact that most of the storage systems on JHPCE use native ZFS compression to minimize the amount of actual disk space a file is using. So, if you want to see how much disk space a file is using without the compression you would add the <code>--apparent-size</code> option to your  <code>du</code> command. <pre><code>[login31 /users/mmill116]$ du --apparent-size .bash_history \n573 .bash_history\n</code></pre> This shows us a value that more closely matches the value from the <code>ls</code> command (586047 bytes / 1024 bytes/KB = 573 KB).</p> <p>You can also add the <code>-h</code> option to the <code>du</code> command to show human-readable units. This is helpful when dealing with files that are many GBs in size. <pre><code>[login31 /users/mmill116]$ du --apparent-size -h .bash_history \n573K    .bash_history\n</code></pre></p> <p>Another commonly used option is the <code>-s</code> option.  This is typically used on directories, and will sum up the total amount of disk space used by all of the files within the directory, and all subdirectories therein. <pre><code>[login31 /users/mmill116]$ du -sh --apparent-size R\n^C\n[login31 /users/mmill116]$ srun --pty bash\nsrun: job 4707899 queued and waiting for resources\nsrun: job 4707899 has been allocated resources\n[compute-048 /users/mmill116]$ time du -sh --apparent-size R\n4.9G    R\n\nreal    4m49.219s\nuser    0m0.406s\nsys 0m7.840s\n</code></pre> Note that this can take quite a long time to run on directories with a lot of files or subdirectories in them. In the above example, I started the <code>du</code> command on the login node, and after a minute I cancelled it, logged into a compute node, and ran it from the compute node, so as not to put a heavy IO load on the login node.</p> <p>It took nearly 5 minutes to run, and found that the files in my R directory are using about 4.9GB  of actual space.  I ran this again without the <code>--apparent-size</code> option, and found that my R directory is using about 4.2 GB of actual disk space, factoring in the compression. <pre><code>[compute-048 /users/mmill116]$ time du -sh R\n4.2G    R\n\nreal    0m51.687s\nuser    0m0.486s\nsys 0m11.553s\n</code></pre> It also ran much quicker this time, due to caching done on the compute node. When data is accessed from a storage array, it is cached in the node's RAM for a while so that subsequent access to the same data will be done more efficiently.</p>"},{"location":"storage/storage-tips/#another-example-of-the-effect-of-compression","title":"Another example of the effect of compression","text":"<p>As mentioned above, we have compression enabled on the storage arrays on the JHPCE cluster.  An extreme example of compression at work can be seen if we create a file of all zeros, which is very easliy compressible, vs a file of  random data, which is not easily compressible. <pre><code>[compute-048 /users/mmill116]$ dd if=/dev/zero of=$MYSCRATCH/zero-file bs=1M count=10000\n10000+0 records in\n10000+0 records out\n10485760000 bytes (10 GB, 9.8 GiB) copied, 60.8073 s, 172 MB/s\n[compute-048 /users/mmill116]$ dd if=/dev/urandom of=$MYSCRATCH/zero-file-rand bs=1M count=10000\n10000+0 records in\n10000+0 records out\n10485760000 bytes (10 GB, 9.8 GiB) copied, 181.059 s, 57.9 MB/s\n</code></pre> If we now use the <code>du</code> command with and without the <code>--apparent-size</code>  option, we can see how compression makes a difference. <pre><code>[compute-048 /users/mmill116]$ du -sh $MYSCRATCH/zero-file\n512 /fastscratch/myscratch/mmill116/zero-file\n[compute-048 /users/mmill116]$ du -sh $MYSCRATCH/zero-file-rand\n9.8G    /fastscratch/myscratch/mmill116/zero-file-rand\n[compute-048 /users/mmill116]$ du -sh --apparent-size $MYSCRATCH/zero-file\n9.8G    /fastscratch/myscratch/mmill116/zero-file\n[compute-048 /users/mmill116]$ du -sh --apparent-size $MYSCRATCH/zero-file-rand\n9.8G    /fastscratch/myscratch/mmill116/zero-file-rand\n</code></pre></p>"},{"location":"storage/storage-tips/#how-much-space-do-i-have-available","title":"How much space do I have available?","text":"<p>As you're working on the JHPCE cluster, you may come across situations where your job reports that it is out of disk space.  There are 2 main limiting factors to space usage on the JHPCE cluster.  One is the user quotas that are in place on home directories and fastscratch. The other is the size of the filesystem that you're working in.</p>"},{"location":"storage/storage-tips/#home-directory-quota","title":"Home directory quota","text":"<p>If you are working out of your home directory, and receive a message that you  are out of disk space, you can see how much if your 100GB quota you are using by running the <code>hquota</code> command. <pre><code>[compute-048 /users/mmill116]$ hquota\nUsername     Space Used         Quota     \nmmill116     62G                100G      \n</code></pre> If you have exhausted your quota you should remove some files in your home directory to free up some space.  Note that the <code>hquota</code> information is updated every 15 minutes, so if you delete files, it may take some time for the change to be reflected in the <code>hquota</code> output.</p>"},{"location":"storage/storage-tips/#fastscratch-quota","title":"Fastscratch quota","text":"<p>All users have a 1TB quota on their fastscratch space.  To see how much space you are using in your fastscratch space, you can use the <code>du</code> command.</p> <pre><code>[compute-048 /users/mmill116]$ du -sh $MYSCRATCH\n9.9G    /fastscratch/myscratch/mmill116\n</code></pre>"},{"location":"storage/storage-tips/#filesystem-usage-project-space","title":"Filesystem Usage - project space","text":"<p>If you are working in a project storage space and you receive an error that you are out of disk space, you can check the amount of available storage by using the <code>df</code> command. <pre><code>[compute-048 /users/mmill116]$ df -h /dcs04/proj1/data\nFilesystem                       Size  Used Avail Use% Mounted on\n192.168.11.209:/srv/dcs04/proj1   60T   60T   43G 100% /dcs04/proj1\n</code></pre> In this example the entire 60TB of /dcs04/proj1/data is used, and  people using that space will need to delete some files to free up space. You may want to check with your PI to see if they have another space available. If you need more space, please reach out to us at bitsupport@lists.jhu.edu.</p>"},{"location":"storage/storage-tips/#filesystem-usage-tmp-space","title":"Filesystem Usage - /tmp space","text":"<p>Many programs by default will use /tmp for storing temporary files.  While this is fine for a single-use system, in a shared environment where multiple users are accessing and utilizing /tmp simultaneously, there's a risk of resource contention and performance issues. If one user's processes generate large temporary files in /tmp, it can consume valuable disk space and potentially impact the performance of other users' processes. This can lead to slowdowns, crashes, or even denial of service for users relying on the shared resources. Overall, it's advisable to avoid using /tmp for and instead use your fastscratch space for temporary files</p> <p>If you must use /tmp you should first check to make sure that there is sufficient space for your temporary files. You could, for example, use the code below to make sure there is ate least 10GB (10000000 KB) in /tmp. <pre><code>FREETMP=`df -k /tmp | grep tmp | awk '{print $4}'`\nif [ $FREETMP -lt 10000000 ]\nthen\n   echo \"Not enough space in /tmp. Only $FREETMP KB available.\"\n   exit 1\nfi\n</code></pre> You should also make sure that your job deletes the files it created in /tmp.</p> <p>Different applications will have different options for specifying the location to use for temptoray files.  While we can't provide an exhausive list, here  is how some commonly used applicaiotn on JHPCE set their temporary location.   + In R, the <code>tmpdir()</code> setting will dictate where temporary files are stored.     If you are generating 10s of GB of temporary files, change <code>tmpdir()</code> to <code>fastscratch</code>.   + In SAS, the default <code>WORK</code> directory will be located under your     <code>fastscratch</code> directory.   + In Stata, the default <code>tempfile</code> location is under <code>/tmp</code>. This can be     changed by setting the <code>STATATMP</code> environment variable.</p>"},{"location":"storage/storage-tips/#backing-up-storage","title":"Backing up storage","text":"<p>Home directory spaces get backed up nightly, however other project spaces may not.  You should check with your PI to see if your project space is getting backed up.</p> <p>If not, you should be sure to copy any unique or difficult-to-repoduce results to your home directory, or transfer them off of the JHPCE cluster, so that you have a backup of the files.  See this document for more information.</p>"},{"location":"sw/building-from-src/","title":"Compile from Source","text":""},{"location":"sw/building-from-src/#compiling-and-installing-software-with-no-root-privilege","title":"Compiling and installing software with no root privilege","text":"<p>You are allowed to download and install small software packages in your own home directory. In this page, we go through an example of installing a piece of free software that converts between different units of measurements, to show you the general steps needed to install a software from source.</p> <p>Warning</p> <p>Please do the software installation on a compute node</p>"},{"location":"sw/building-from-src/#step-1-download-source-code","title":"Step 1: Download source code","text":"<pre><code>[test@compute-110 ~]$ mkdir download\n[test@compute-110 ~]$ cd download/\n[test@compute-110 download]$ wget http://ftp.gnu.org/gnu/units/units-2.23.tar.gz\n--2024-03-18 11:57:38--  http://ftp.gnu.org/gnu/units/units-2.23.tar.gz\nResolving ftp.gnu.org (ftp.gnu.org)... 209.51.188.20, 2001:470:142:3::b\nConnecting to ftp.gnu.org (ftp.gnu.org)|209.51.188.20|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1423494 (1.4M) [application/x-gzip]\nSaving to: \u2018units-2.23.tar.gz\u2019\n\nunits-2.23.tar.gz                                 100%[==========================================================================================================&gt;]   1.36M  --.-KB/s    in 0.1s    \n\n2024-03-18 11:57:39 (12.7 MB/s) - \u2018units-2.23.tar.gz\u2019 saved [1423494/1423494]\n</code></pre>"},{"location":"sw/building-from-src/#step-2-extract-the-source-code","title":"Step 2: Extract the source code","text":"<pre><code>[test@compute-110 download]$ ls -l\n-rw-r--r-- 1 test test 1423494 Feb 18 22:45 units-2.23.tar.gz\n\n[test@compute-110 download]$ tar -zxvf units-2.23.tar.gz \nunits-2.23/\nunits-2.23/definitions.units\nunits-2.23/units.txt\n...\n</code></pre>"},{"location":"sw/building-from-src/#step-3-configure-the-software","title":"Step 3: Configure the software","text":"<pre><code>[test@compute-110 download]$ cd units-2.23/\n[test@compute-110 units-2.23]$ ./configure --prefix=$HOME/mysoftware/units-2.23\n</code></pre> <p>Note</p> <p>The first thing to do is carefully read the <code>README</code> and <code>INSTALL</code> text files (use the <code>less</code> command). These contain important information on how to compile and the run the software. Since you do not have root privilege to install the software on system area, you will need to specify the installation directory using <code>--prefix=/path/to/your/softwre</code>.  </p>"},{"location":"sw/building-from-src/#step-4-build-and-install","title":"Step 4: Build and install","text":"<pre><code>[test@compute-110 units-2.23]$ make\n[test@compute-110 units-2.23]$ make install\n</code></pre> <p>Note</p> <p>This will install the files into the <code>~/mysoftware/units-2.23</code> directory that you specified with <code>./configure</code>.</p>"},{"location":"sw/building-from-src/#step-5-add-the-software-to-path","title":"Step 5: Add the software to path","text":"<pre><code>[test@compute-110 ~]$ export PATH=$PATH:$HOME/mysoftware/units-2.23/bin\n</code></pre> <p>Note</p> <p>You can add the above line in your <code>.bashrc</code> file so the software would be available when you login</p>"},{"location":"sw/building-from-src/#step-6-run-the-software","title":"Step 6: Run the software","text":"<pre><code>[test@compute-110 ~]$ units\n...\nYou have: tempF(75)\nYou want: tempC\n    23.888889\nYou have: exit\n[test@compute-110 ~]$\n</code></pre>"},{"location":"sw/chrome/","title":"Chrome","text":""},{"location":"sw/chrome/#introduction","title":"Introduction","text":"<p>JHPCE has installed the Google Chromium web browser. This Linux software is a version of the Google Chrome web browser found on other operating systems.<sup>1</sup></p> <p>This page describes some solutions and recommendations for our users.</p>"},{"location":"sw/chrome/#locked-profiles","title":"Locked profiles","text":"<p>It is not uncommon that cluster users run into the following error message when they try to launch Chrome:</p> <p><pre><code>The profile appears to be in use by another Chromium process (199919)\non another computer (compute-102.cm.cluster). Chromium has locked the\nprofile so that it doesn't get corrupted. If you are sure no other \nprocesses are using this profile, you can unlock the profile and\nrelaunch Chromium.\n</code></pre> (The process number and computer name will be different in each case.)</p> <p>The way to avoid this problem is to always quit Chrome properly before your SLURM interactive session ends. </p>"},{"location":"sw/chrome/#solving-the-locked-profile-problem","title":"Solving the locked profile problem","text":"<p>See the explanation in the next section of why you need to take these actions.</p> <p>You need to  1. quit your Chromium session (if Chromium was launched as a part of starting SAS, then quit the SAS session too, because it has created a reference to that particular Chromium process) 2. then rename or delete the files which comprise the \"lock\". (Deleting will delete your browsing history, bookmarks, etc. This is often okay in our environment.) with a command like one of these (choose one):</p> <pre><code>mv ~/.config/chromium ~/.config/chromium-locked\n</code></pre> <p><pre><code>cd ~/.config/\n/bin/rm -rf chromium\n</code></pre> We haven't tested changing only the profile-specific files, but believe it works.</p> <pre><code>cd ~/.config/\n/bin/rm -rf chromium/your-profile-name\n</code></pre> <p>There is a possibility that the Chromium process mentioned by the \"profile locked\" error message is still running even though your SLURM interactive job has ended. If that happens, then, substituting in your specific process id and  compute node name, </p> <ol> <li>Log into that node <code>srun --pty --nodelist=compute-102 bash</code></li> <li>Look for that process id number <code>ps -f -p 12345</code></li> <li>If you see a process listing where your name is listed under UID, and \"chromium-browser\" under the CMD header, then you should kill it. If you don't see anything, then the process mentioned by the \"profile locked\" message died.</li> <li>Kill that process by running <code>kill -KILL 12345</code></li> <li>After a few seconds delay, do that again.</li> <li>You should receive the message `12345: No such process\"</li> </ol>"},{"location":"sw/chrome/#what-causes-this-error","title":"What causes this error?","text":"<p>All web browsers maintain, in a user's home directory, an extensive set of files supporting bookmarks, browsing history, possibly multiple user profiles, etc.</p> <pre><code>On Linux, this directory is named `~/.config/chromium/`\n</code></pre> <p>By default there is a single profile. The information for that profile is stored in a subdirectory of the primary directory, e.g. <code>~/.config/chromium/Default/</code></p> <p>The UNIX environment at many organizations contain multiple networked computers  which share a single user home directory. (That directory is stored on a storage server and shared to select other machines.) Therefore, the authors of Chromium had to accomodate the possibility that the same home directory would be asked to support Chromium use on multiple computers. They chose to support use by a single profile on a single computer at a time. </p> <p>When Chromium is launched, it creates a lock file within the profile directory which records the computer hostname, the process id (\"pid\") on that computer, the date, etc. This lock file is removed if one quits Chrome by choosing Exit from Chromium's menu. </p> <p>If your interactive session ends unexpectedly (e.g. runs out of memory) or you  exit your shell without first quitting Chromium, then the browser process is  killed off without it having the time to clean up after itself, by, among other things, removing the lock file. That file remains whether or not the processes associated with the browser are still running.</p>"},{"location":"sw/chrome/#running-multiple-concurrent-chromium-sessions","title":"Running multiple concurrent Chromium sessions","text":"<p>If you wanted to do this, then you would launch each session with different command-line arguments specifying a different profile name. This places the files for that particular session in a different subdirectory of <code>~/.config/chromium/</code></p> <p>First, in an existing Chromium session, you would create a new profile. Try to specify profile names using hyphens instead of spaces, as UNIX does not handle spaces in file names without the user enclosing them in quotes or escaping them with the backslash character.</p> <p>Second, you would launch a new browser using that profile with a command like:</p> <p><code>chromium --profile-directory=Profile-1</code> or <code>chromium --profile-directory=\"Profile 1\"</code> or <code>chromium --profile-directory=\"Profile\\ 1\"</code></p>"},{"location":"sw/chrome/#chromium-spews-out-error-messages-when-launched","title":"Chromium spews out error messages when launched","text":"<p>Web browsers are normally run on a system sitting physically in front of you. Those are equipped with a graphics card. Our compute nodes don't have such cards or the software to support them. When Chromium launches, it doesn't find that card and software and emits a stream of warnings which you can ignore.</p> <p>You can avoid seeing them if you redirect standard output and standard error data streams. This is entirely optional.</p> <p>Tip</p> <p>The active environment is the one with an asterisk (*)</p>"},{"location":"sw/chrome/#handy-bash-routine-to-add-to-your-environment","title":"Handy bash routine to add to your environment","text":"<pre><code>conda activate env-name\nconda install package-name\n</code></pre> <p>Note</p> <ul> <li>Conda is a package manager, similar to pip.  </li> <li>Anaconda is a \"batteries included\" distribution of Python. It uses Conda as its package manager.</li> </ul> <ol> <li> <p>(Our web site uses the terms Chrome and Chromium interchangeably. But it is actually Chromium. The process names shown in <code>ps</code> output include \"chromium-browser\")\u00a0\u21a9</p> </li> </ol>"},{"location":"sw/conda/","title":"Conda Environment","text":""},{"location":"sw/conda/#introduction","title":"Introduction","text":"<p>Conda gives you the ability to create environments with packages and their dependencies that will be separate from other environments. This page goes over the basic usage of it.</p>"},{"location":"sw/conda/#create-an-environment","title":"Create an environment","text":"<ul> <li> <p>load conda module <pre><code>module load conda\n</code></pre> or <pre><code>module load anaconda\n</code></pre></p> </li> <li> <p>create a new environment <pre><code>conda create -n &lt;evn-name&gt;\n</code></pre></p> </li> </ul> <p>Note</p> <ul> <li>Conda is a package manager, similar to pip.  </li> <li>Anaconda is a \"batteries included\" distribution of Python. It uses Conda as its package manager.</li> </ul>"},{"location":"sw/conda/#list-environments","title":"list environments","text":"<ul> <li>lista all your conda environment <pre><code>conda info --envs\n</code></pre></li> </ul> <p>Tip</p> <p>The active environment is the one with an asterisk (*)</p>"},{"location":"sw/conda/#install-a-conda-package-in-the-new-created-environment","title":"Install a conda package in the new created environment","text":"<pre><code>conda activate env-name\nconda install package-name\n</code></pre> <p>Notes:</p> <p>To install a specific version: <pre><code>conda install package-name=2.3.4\n</code></pre> To specify only a major version <pre><code>conda install package-name=2\n</code></pre></p>"},{"location":"sw/conda/#specifying-channels-to-use","title":"specifying channels to use","text":"<ul> <li>Channels are locations where packages are stored. By default, conda searchs for packages in its default channels. You can specify a channel when installing the package (e.g. conda-forge channel) <pre><code>conda install conda-forge::numpy\n</code></pre></li> </ul>"},{"location":"sw/conda/#mamba","title":"Mamba","text":"<p>Mamba is a package manager that is fully compatible with conda but performs better on certain tasks such as resolving requirements.</p>"},{"location":"sw/containers-stub/","title":"Containers","text":"","tags":["needs-to-be-written","mark"]},{"location":"sw/containers-stub/#using-our-container-solutions","title":"Using Our Container Solutions","text":"<p>We have two solutions that you can use (R Studio Server, JupyterHub). See which documents about using them???</p> <p>Maybe this is a good time to at least get credit for the work Mark has done building the singularity containers we now offer.</p> <p>What do you need to know about using them? Quitting them? Waiting how long for them to finish starting?</p> <p>What verbiage about this can be shared with Web Portal docs?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers-stub/#building-your-own-containers","title":"Building Your Own Containers","text":"<p>Maybe we can document this later in terms of how users can build them. In the mean time, provide some links??</p> <p>They don't have root access, so what is that trick that allows you to make a container image?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers-stub/#how-do-you-run-a-batch-slurm-job-using-a-container","title":"How do you run a batch SLURM job using a container?","text":"<p>Where is the best file system to host a container file if you're going to run it in batch mode on multiple nodes? /fastscratch? Does it make a difference?</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/containers-stub/#examples-from-elsewhere","title":"Examples from Elsewhere","text":"<p>The Frontera site's section on containers mentions something that users building their own containers on JHPCE need to know to add to their containers -- e.g. which file systems to include.</p> <p>NERSC has a well-written set of pages about using podman-hpc, which might serve as an example of just building and using containers even without podman-hpc</p> <p>Zurich pages on Singularity and Recipes and Hints</p> <p>USC pages on Singularity</p>","tags":["needs-to-be-written","mark"]},{"location":"sw/gui-tools/","title":"Helpful GUI Programs","text":"<p>We have installed a number of text editors, file managers, graphics viewers and editors, etc with Graphic User Interfaces (GUI). You may find these useful.</p>","tags":["gui-tools"]},{"location":"sw/gui-tools/#jhpce-cluster","title":"JHPCE Cluster","text":"<p>This chart (pdf) is for the main cluster.</p>","tags":["gui-tools"]},{"location":"sw/gui-tools/#c-sub-cluster","title":"C-SUB Cluster","text":"<p>This chart (pdf) is C-SUB specific.</p>","tags":["gui-tools"]},{"location":"sw/jupyter-stub/","title":"Jupyter","text":"<p>Everything you need to know to get started using Jupyter and related friends.</p>","tags":["needs-to-be-written"]},{"location":"sw/jupyter-stub/#using-the-web-portal","title":"Using the Web Portal","text":"<p>Authoring Note</p> <p>Put here information about aspects of using this tool. Perhaps a warning that it can take some time to launch.</p> <p>Please see this document about this service.</p>","tags":["needs-to-be-written"]},{"location":"sw/locally-written/","title":"Locally Written Tools","text":"<p>We here try to capture information about programs you may want to use. Most of them lack on-line manual pages. We try to add support for usage statements and the <code>-h</code> (help) argument. Because all of them are shell scripts or python, you can look at the source code.</p>","tags":["in-progress"]},{"location":"sw/locally-written/#where-are-they","title":"Where are they?","text":"<p>Many of these programs live in <code>/jhpce/shared/jhpce/core/JHPCE_tools/3.0/bin/</code> If that directory is not in your PATH, then you are not seeing the normal and supported JHPCE environment. (If it occurs after your additions to the PATH, then you may see only parts of the supported environment.)</p> <p>Your environment, including your PATH variable, is created when logging in. A number of configuration files are processed. Your <code>.bashrc</code> file is a core one. By default it will consult one in <code>/etc/bashrc</code>  Files in <code>/etc/profile.d/</code> are also processed. It is via these latter files that we do things like provide support for modules, display messages of the day on login nodes, etc.</p> <p>You can use the command <code>which cmd</code> to be told where \"cmd\" comes from.</p>","tags":["in-progress"]},{"location":"sw/locally-written/#general","title":"General","text":"<ul> <li>auth_util: Interface for working with OTP MFA configuration. View and generate verification tokens for Google Authenticator. See orientation documents for details on using it to configure your smartphone, etc. (no man page yet)</li> </ul>","tags":["in-progress"]},{"location":"sw/locally-written/#disk-usage","title":"Disk Usage","text":"<ul> <li>getquota: Displays your quota stats (not yet working on C-SUB)</li> <li>cmsquota-dcs05: For C-SUB users, (only works on compute nodes)</li> <li>cmsquota-dcs06: For C-SUB users, (works on both login and compute nodes)</li> </ul>","tags":["in-progress"]},{"location":"sw/locally-written/#container-computing","title":"Container Computing","text":"<ul> <li>jupyterlab.sh: Start an Jupyter server available via an SSH tunnel.</li> <li>jupyterlab-gpu.sh: Start an Jupyter server available via an SSH tunnel with CUDA libraries in LD_LIBRARY_PATH.</li> <li>jhpce-rstudio-server: Start an RStudio server available via an SSH tunnel.</li> <li>csub-rstudio-server: Cluster-specific version.</li> </ul>","tags":["in-progress"]},{"location":"sw/locally-written/#slurm-related","title":"SLURM-related","text":"<p>We are creating a growing number of scripts and resources. They are found in this document.</p>","tags":["in-progress"]},{"location":"sw/matlab/","title":"Matlab","text":""},{"location":"sw/matlab/#using-matlab-interactively","title":"Using Matlab interactively","text":"<pre><code>[test@login31 ~]$ srun --mem 10G --x11 --pty bash\nsrun: job 3109996 queued and waiting for resources\nsrun: job 3109996 has been allocated resources\n[test@compute-152 ~]$ module load matlab\n[test@compute-152 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) matlab/R2023a\n\n[test@compute-152 ~]$ matlab\n</code></pre>"},{"location":"sw/matlab/#running-matlab-program-in-batch-mode","title":"Running Matlab program in batch mode","text":"<ol> <li> <p>Write your matlab source in a file with .m extension (e.g. matlab_example.m) <pre><code>x = [1 2 3 4];\nfprintf('Example number = %i\\n', x)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. matlab_example.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=5:00\n\n#SBATCH -o slurm.%N.%J.%u.out   # STDOUT\n#SBATCH -e slurm.%N.%J.%u.err   # STDERR\n\nmodule load matlab\nmatlab -nojvm -nodisplay -r \"matlab_example;quit;\"\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch matlab_example.sh \nSubmitted batch job 3110505\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3110505    shared matlab_e    test  R       0:03      1 compute-100\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 12 17:09 slurm.compute-100.3110505.test.err\n-rw-r--r-- 1 test test 410 Mar 12 17:10 slurm.compute-100.3110505.test.out\n</code></pre></p> </li> <li> <p>Look at the results from output file, file with stderr is empty(job without errors): <pre><code>[test@login31 ~]$ cat slurm.compute-100.3110505.test.out\n                                May 25, 2023\n\nFor online documentation, see https://www.mathworks.com/support\nFor product information, visit www.mathworks.com.\n\nExample number = 1\nExample number = 2\nExample number = 3\nExample number = 4\n</code></pre></p> </li> </ol>"},{"location":"sw/modules/","title":"Modules","text":""},{"location":"sw/modules/#environment-modules","title":"Environment Modules","text":""},{"location":"sw/modules/#introduction","title":"Introduction","text":"<p>Environment Modules provide a convenient way to dynamically change the users\u2019 environment through modulefiles. When a user loads a module for a specific version of a software package, appropriate changes are made to the user's environment, such as adding new locations in the PATH environment variables. When a package is no longer needed, users should unload the module from their environment, (to prevent interactions between module settings).</p> <p>The JHPCE cluster uses the Lmod module system to allow users to configure their shell environments.</p>"},{"location":"sw/modules/#use-modules-on-jhpce-cluster","title":"Use modules on JHPCE cluster","text":"<p>The naming convention for modules is \"software_name/version\" (e.g. bowtie/2.5.1). If a module is loaded, it will be followed by <code>(L)</code> The default version, if designatedk will be followed by a <code>(D)</code></p> <ul> <li>list loaded modules in your environment</li> </ul> <pre><code>[test@compute-107 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0\n</code></pre> <pre><code>[test@compute-107 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0\n</code></pre> <ul> <li>list available modules <pre><code>[test@compute-107 ~]$ module avail\n</code></pre></li> </ul> <p>Notes:</p> <ul> <li>in order to use a software that is not in shell's standard search path, you need first load its module  </li> <li>LIBD contributes many modules. If you need help with any of these, please email bithelp</li> </ul> <ul> <li>load a module (e.g. R module)</li> </ul> <pre><code>[test@compute-107 ~]$ module load R\nLoading R/4.3\n(4.3)[test@compute-107 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) conda/3-23.3.1   4) R/4.3\n</code></pre> <p>Note: Some software packages depend on other software. Loading module for the package may load modules for the other software as well. In this case, loading R also loads conda module. If you have your own conda environment, please make sure there is no conflicts.</p> <ul> <li>unload a module (e.g. when R is no longer needed, you can unload it from your environment)</li> </ul> <p><pre><code>(4.3)[test@compute-107 ~]$ module unload R\nUnloading R/4.3\n[test@compute-107 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) conda/3-23.3.1\n</code></pre> - get help on using module <pre><code>[test@compute-107 ~]$ module help\nUsage: module [options] sub-command [args ...]\n...\n</code></pre></p>"},{"location":"sw/modules/#basic-module-commands-for-users","title":"Basic module commands for users","text":"<p>A module file is a script that sets up the paths and environment variables that are needed for a particular application or development environment. Most users will just use our modulefiles. But if you want to finely control your shell environment, you can start developing your own custom module files. </p> <p>There are a few basic commands that users should know:</p> <pre><code>module list                 # list your currently loaded modules\nmodule load   &lt;MODULEFILE&gt;  # configures your environment according to modulefile \nmodule unload &lt;MODULEFILE&gt;  # rolls back the configuration performed by the associated load\nmodule avail                # shows what modules are available for loading\nmodule help                 # \nmodule spider               # lists all modules, including ones not in your MODULEPATH\nmodule spider &lt;NAME&gt;        # search for a module whose name includes &lt;NAME&gt;\nmodule save &lt;NAME&gt;.         # save currently-loaded modules to \"default\" or optionally to &lt;NAME&gt;\nmodule restore &lt;NAME&gt;.      # load the saved collection\n</code></pre>"},{"location":"sw/modules/#defaults","title":"Defaults","text":"<p>By default the following modules are loaded on all compute hosts and the login hosts when you log in</p> <pre><code>JHPCE_ROCKY9_DEFAULT_ENV\nJHPCE_tools/3.0\n</code></pre>"},{"location":"sw/modules/#configuring-your-bashrc","title":"Configuring your .bashrc","text":"<p>It is critical that your <code>.bashrc</code> file sources the system-wide bashrc file. Otherwise basic programs will not work! After you source the system-wide bashrc file you can add code which will modify your environment. For example:</p> <pre><code># Always source the global bashrc\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n\n# If I prefer gcc/4.8.1 as my default compiler\nmodule load gcc/4.8.1\n</code></pre>"},{"location":"sw/python-pkg/","title":"Python Packages","text":""},{"location":"sw/python-pkg/#installing-python-packages","title":"Installing Python Packages","text":"<ul> <li>Choose the Python version you want to use <pre><code>[bob@compute-107 /users/bob]$ module load python\n[bob@compute-107 /users/bob]$ python3 --version\nPython 3.9.14\n</code></pre></li> </ul> <p>Note</p> <p>You can use <code>module avail python</code> to see available python versions on the cluster, and load the one you want to use.</p> <ul> <li> <p>Ensure you can run pip from the command line <pre><code>[bob@compute-107 /users/bob]$ python3 -m pip --version\npip 23.1.2 from /jhpce/shared/jhpce/core/python/3.9.14/lib/python3.9/site-packages/pip (python 3.9)\n</code></pre></p> </li> <li> <p>Installing a package into your own home directory (e.g. the \"cowsay\" package) <pre><code>[bob@compute-107 /users/bob]$ python3 -m pip install --user cowsay\n</code></pre></p> </li> </ul> <p>When you install a python package, the program will be installed into youur local Python bin directory, which by defaul is the directory \".local/bin\" under your home directory.  In order to easily use the program, you will want to add this directory to your PATH environment variable.  If you don't have the \".local/bin\" in your path, you will likely get a warning message when you \"pip install\" the program, and also likely get a \"command not found\" error when trying to run the command. <pre><code>[bob@compute-107 /users/bob]$ python3 -m pip install --user cowsay\nCollecting cowsay\n  Downloading cowsay-6.1-py3-none-any.whl (25 kB)\nInstalling collected packages: cowsay\n\n. . .\n\nWARNING: The script cowsay is installed in '/users/bob/.local/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n\n. . .\n\nSuccessfully installed cowsay-6.1\n[bob@compute-107 /users/bob]$ cowsay -t \"Hello\"\n-bash: cowsay: command not found\n</code></pre> In order to easily run the program, you would need to add your local Python bin directory to your PATH environment variable. <pre><code>[bob@compute-107 /users/bob]$ echo $PATH\n/jhpce/shared/jhpce/core/JHPCE_tools/3.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\n[bob@compute-107 /users/bob]$ export PATH=$PATH:$HOME/.local/bin\n[bob@compute-107 /users/bob]$ cowsay -t \"Hello\"\n  _____\n| Hello |\n  =====\n     \\\n      \\\n        ^__^\n        (oo)\\_______\n        (__)\\       )\\/\\\n            ||----w |\n            ||     ||\n[bob@compute-107 /users/bob]$ \n</code></pre> Setting your PATH this way will only be effective for your current session, so if you log out, you would need to set your PATH again.  To make this change to your PATH permanent you can use a text editor to edit your the .bash_profile file in your home directory and add the \"export PATH=$PATH:$HOME/.local/bin\" to the end of that file, or use the commands below to make a backup copy of your .bash_profile file, and append the PATH line to the end of the .bash_profile file: <pre><code>[bob@compute-107 /users/bob]$ cp $HOME/.bashrc $HOME/.bashrc-bak\n[bob@compute-107 /users/bob]$ echo \"export PATH=\\$PATH:\\$HOME/.local/bin\" &gt;&gt; $HOME/.bash_profile\n</code></pre></p> <p>Notes:</p> <p>To install a specific version: <pre><code>[bob@compute-107 /users/bob]$ python3 -m pip install --user cowsay==5.0\n</code></pre></p>"},{"location":"sw/python/","title":"Python","text":""},{"location":"sw/python/#using-python-interactively-on-command-line","title":"Using Python Interactively on command line","text":"<pre><code>[test@login31 ~]$ srun --pty bash\nsrun: job 3168742 queued and waiting for resources\nsrun: job 3168742 has been allocated resources\n[test@compute-100 ~]$ module load python\n[test@compute-100 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) python/3.9.14\n\n[test@compute-100 ~]$ python3\nPython 3.9.14 (main, May 16 2023, 14:32:18) \n[GCC 11.3.1 20220421 (Red Hat 11.3.1-2)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n</code></pre>"},{"location":"sw/python/#running-python-program-in-batch-mode","title":"Running Python program in batch mode","text":"<ol> <li> <p>Write your python source in a file with .py extension (e.g. python_example.py) <pre><code>values = [88, 47, 96, 85, 72]\nfor i in values:\n    print('Example number = ', i)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. python_example.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\n#SBATCH -o slurm.%N.%J.%u.out   # STDOUT\n#SBATCH -e slurm.%N.%J.%u.err   # STDERR\n\nmodule load python\npython3 python_example.py\n</code></pre></p> </li> <li> <p>Submit your python job <pre><code>[test@login31 ~]$ sbatch python_example.sh \nSubmitted batch job 3169570\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3169570    shared python_e    test R       0:01      1 compute-153\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 13 12:45 slurm.compute-153.3169570.test.err\n-rw-r--r-- 1 test test 105 Mar 13 12:45 slurm.compute-153.3169570.test.out\n</code></pre></p> </li> <li> <p>Look at the results from output file, file with stderr is empty(job without errors): <pre><code>[test@login31 ~]$ cat slurm.compute-153.3169570.test.out\nExample number =  88\nExample number =  47\nExample number =  96\nExample number =  85\nExample number =  72\n</code></pre></p> </li> </ol>"},{"location":"sw/python/#installing-your-own-packages","title":"Installing Your Own Packages","text":"<p>Please see this document</p>"},{"location":"sw/r-n-friends/","title":"Running R in JHPCE","text":"","tags":["in-progress"]},{"location":"sw/r-n-friends/#r-basics","title":"R Basics","text":"<p>There are a number of ways to run R on the JHPCE cluster.  There is a text-based version of R which is available as a module.  We also have a way to run rstudio over the X11 interface. We have a way to run Rstudio Server through an ssh tunnel.  Lastly we have a way to run Rstudio Server through our Web Application Server.</p> <p>During orientation, participants are shown a number of example files stored in a directory copied into new account home directories. There is a copy of the latest set of those in the directory /jhpce/shared/jhpce/class-scripts/.  We have one set for users of the main cluster and a second for users of the C-SUB.  You can copy the latest files into your home directory with this command, replacing \"clustername\" with either \"jhpce\" or \"c-sub\" <code>rsync -av /jhpce/shared/jhpce/slurm/class-scripts/clustername/ $HOME/class-scripts/</code> For that rsync command to work correctly, there needs to be a trailing slash on the path to the originals of the class-scripts.</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#ways-to-run-r-on-jhpce","title":"Ways to run R on JHPCE","text":"","tags":["in-progress"]},{"location":"sw/r-n-friends/#using-the-text-based-version-of-r","title":"Using the text-based version of R","text":"<p>There are 2 ways to make use of the text-based version of R on the cluster.  You can either run R interactively, and enter your R code in the R, or you can submit an non-interactive batch program that uses R.</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#running-r-interactively","title":"Running R Interactively","text":"<pre><code>[test@login31 ~]$ srun --pty bash\nsrun: job 3176550 queued and waiting for resources\nsrun: job 3176550 has been allocated resources\n[test@compute-152 ~]$ module load conda_R\nLoading conda_R/4.3.x\n(4.3.x)[test@compute-152 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) conda/3-23.3.1   4) conda_R/4.3.x\n\n(4.3.x)[test@compute-152 ~]$ R\n\nR version 4.3.2 Patched (2024-02-08 r85876) -- \"Eye Holes\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-conda-linux-gnu (64-bit)\n...\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n...\n&gt; \n</code></pre>","tags":["in-progress"]},{"location":"sw/r-n-friends/#running-r-in-batch-mode","title":"Running R in batch mode","text":"<p>In $HOME/class-scripts/R-demo there is an example of submitting an R job on the cluster.  You'll note that there are two files here, a SLURM batch job file and an R program file. Here is an example of running an R session interactively.</p> <ol> <li> <p>Write your R source in a file with .r extension (e.g. plot1.r). Here is what the example file looks like: <pre><code>[test@login31 R-demo]$ cat plot1.r\n# Set output file name\npdf(\"plot1-R-results.pdf\")\n\n# Define 2 vectors\ncars &lt;- c(1, 3, 6, 4, 9)\ntrucks &lt;- c(2, 5, 4, 5, 12)\n\n# Graph cars using a y axis that ranges from 0 to 12\nplot(cars, type=\"o\", col=\"blue\", ylim=c(0,12))\n\n# Graph trucks with red dashed line and square points\nlines(trucks, type=\"o\", pch=22, lty=2, col=\"red\")\n\n# Create a title with a red, bold/italic font\ntitle(main=\"Autos\", col.main=\"red\", font.main=4)\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. plot1.sh) <pre><code>[test@login31 R-demo]$ cat plot1.sh \n#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load conda_R\nR CMD BATCH plot1.r\n</code></pre></p> </li> <li> <p>Submit your R job <pre><code>[test@login31 ~]$ sbatch plot1.sh \nSubmitted batch job 3177833\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3177833    shared plot1.sh    test  R       0:03      1 compute-095\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r--    1 test test        22 Mar 13 16:29  slurm-3177833.out\n-rw-r--r--    1 test test      4988 Mar 13 16:29  plot1-R-results.pdf\n</code></pre></p> </li> <li> <p>Look at the result file, plot1-R-results.pdf; you can use xpdf to view it if you have set X11 forward when login.    Otherwise, you need download the file to local machine to view it. <pre><code>[test@login31 ~]$ xpdf plot1-R-results.pdf\n</code></pre></p> </li> </ol>","tags":["in-progress"]},{"location":"sw/r-n-friends/#running-the-x11-version-of-rstudio","title":"Running the X11 version of RStudio","text":"<p>Note</p> <p>In order to run the X11 version of Rstudio, you need to have X11 setup on your local system. - For Windows, MobaXterm has an X server built into it - For Mac, you need to have the XQuartz program installed (which requires a reboot), and you need to add the \"-X\" option to ssh: <pre><code>ssh -X yourusername@jhpce01.jhsph.edu\n</code></pre> See more detailed info on X11 here</p> <p>If you prefer the graphical version of R, you can the X11 version of Rstudio. Here is an example of running the X11 version of Rstudio: <pre><code>[test@login31 ~]$ srun --mem 10G --x11 --pty bash\nsrun: job 3244190 queued and waiting for resources\nsrun: job 3244190 has been allocated resources\n[test@compute-097 ~]$ module load R\nLoading R/4.3\n(4.3)[test@compute-097 ~]$ module load rstudio\n(4.3)[test@compute-097 ~]$ rstudio\n</code></pre></p> <p>When you run rstudio, you should see the familiar graphical Rstudio interface come up.</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#running-rstudio-server","title":"Running RStudio Server","text":"<p>Depending on which cluster you are using, run one of the following two scripts after starting an interactive session and landing on a compute node. (The discussion below assumes that you are on the jhpce cluster.) </p> <ul> <li>jhpce-rstudio-server</li> <li>csub-rstudio-server</li> </ul> <p>Rstudio Server is a web based environment for developing R programs.  On the JHPCE cluster we have put together a script called \u201cjhpce-rstudio-server\u201d which will allow you to run your own personal copy of Rstudio Server and access it from a browser on your laptop or desktop.  When the \u201cjhpce-rstudio-server\u201d program is run, it starts an instance of the Rstudio Server web server within a Singularity image on a unique port number, and then provides instructions for setting up an ssh tunnel to allow you to access Rstudio Server from your local system.</p> <p>You will need to perform one step to enable access to this Rstudio Server from your local laptop/desktop;  specifically, you will need to add a tunnel to your existing ssh session to the JHPCE cluster.</p> <p>Tip</p> <p>In UNIX, you send an interrupt signal to a running foreground program using the key combination Ctrl+C This is historically written as <code>^C</code>  Note that you DO NOT actually use the SHIFT key to capitalize the C. The way it is written is misleading. You do NOT type Ctrl+Shift+C. However this <code>^C</code> is the way it has been written for decades, and we will do so.</p> <p>Warning</p> <p>However, you DO need to capitalize the letter <code>c</code> when trying to send an interrupt signal to the ssh program on Mac or Linux computers. The key combination for an ssh interrupt is <code>~ SHIFT c</code> or ~+Shift+C</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#for-mac-or-linux-computers","title":"For Mac or Linux computers:","text":"<p>To add this tunnel, first type ~C (while holding down SHIFT, press \u201c~\u201d then \u201cC\u201d).  The ~C is used to send an interrupt to your ssh session.  The ~C will likely not show up, but you should see an \u201cssh&gt;\u201d prompt as a result.  At this \u201cssh&gt;\u201d prompt you activate the tunnel by typing  -L XXXXX:compute-YYY:XXXXX  .  This will allow your laptop/desktop to access the compute node compute-YYY on port XXXXX (in the above example, the port used was 12345 and the compute node used was compute-012).</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#for-windows-computers-or-from-the-safe-desktop","title":"For Windows computers, or from the SAFE Desktop:","text":"<p>If you connected to the JHPCE cluster with MobaXterm from a Windows-based system or SAFE desktop, you should ignore the first step (entering ~C and adding the local tunnel). </p> <p>Instead, you will need to  add a tunnel from MobaXterm.</p> <p>Before setting up the tunnel you may find it helpful to set up an SSH key using the steps at this page about ssh   While not a requirement, this will eliminate the need to login using your password and Google Verification Code.  Note that if you are setting up the tunnel for the C-SUB, you will not be able to use SSH keys due to the enhanced security of the C-SUB.</p> <p>To start, click on the \u201cTunneling\u201d icon at the top of MobaXterm, and you should see the window below:</p> <p></p> <p>Click on \u201cNew SSH Tunnel\u201d, and you should see:</p> <p></p> <p>Enter the following in the window:</p> <ul> <li>For \u201cForwarded Port\u201d, enter the port number displayed (this is 12345 in the example)</li> <li>For \u201cRemote Server\u201d, enter the compute node (compute-012 in the example)</li> <li>For \u201cRemote Port\u201d, enter the port number (12345 in the example)</li> <li>For \u201cSSH Server\u201d, enter \u201cjhpce01.jhsph.edu\u201d if you are on the JHPCE cluster, or \u201cjhpcecms01.jhsph.edu\u201d if you are on the CSUB.</li> <li>For \u201cSSH Login\u201d, enter your JHPCE cluster login name (jdoe1 in the example)</li> <li>For \u201cSSH Port\u201d, enter \u201c22\u201d</li> </ul> <p>Then click \u201cSave\u201d, and you\u2019ll be take back to the \u201cMobaSSHTunnel\u201d screen with your new tunnel displayed.</p> <p></p> <p>Next, click on the yellow \u201cKey\u201d icon   and browse to the location of your private key (the \u201c.ppk\u201d file).  Now start your tunnel by clicking on the Green triangle icon in the \u201cStart/Stop\u201d column.</p> <p>Now that the tunnel is established, you can then access Rstudio Server from your laptop/desktop by using a web browser, and connecting to the url, http://localhost:XXXXX .  Once connected you will be prompted for your login and password, and you will need to enter you your JHPCE login and password at this point.</p> <p></p> <p>Once you enter your login and password, you should see Rstudio running in your browser.</p> <p></p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#shutting-down-the-rstudio-server","title":"Shutting down the Rstudio Server","text":"<p>When you have finished using Rstudio Server, you should close the browser tab or window that you are using to run Rstudio Server, and then return to the ssh session where you ran the \u201cjhpce-rstudio-server\u201d command.  To stop the Rstudio Server, type \u201c^C\u201d.  You will then be given a few additional steps to run to deactivate the port forward. As with the establishment of the tunnel, these steps are for MacOS and Linux based desktops/laptops.  You will again be prompted to type \u201c~C\u201d, and then enter \u201c-KL XXXXX\u201d at the \u201cssh&gt;\u201d prompt to stop the forwarding (NOTE: you\u2019ll need to hit  once before typing \u201c~C\u201d).  The session should look similar to: <pre><code>[login31 /users/jdoe1] $ srun --pty --x11 --mem=10G bash\nLast login: Wed May 1 17:02:29 2019 from login31.cm.cluster\nLoading conda_R/3.5.x\n[compute-012 /users/jdoe1] $ jhpce-rstudio-server\n------------\n1) Establish your SSH tunnel.\n* Windows users see: https://jhpce.jhu.edu/sw/rstudio-server/\n* Mac or Linux Desktop users, please type:\n~C\n-L 12345:compute-012:12345\n\n2) From the browser running on your local system or SAFE desktop, go to:\n\n  http://localhost:12345\n\n3) When prompted for Username, enter your JHPCE ID:  jdoe1\n4) When prompted for Password, enter your JHPCE password.\n\nWhen you are finished using Rstudio Server, type &lt;CTRL&gt;-C\n\n~C\nssh&gt; -L 12345:compute-012:12345\nForwarding port.\n\n****Cleaning up...\n------------\nNow type:\n~C\n-KL 41354\n\n[compute-012 /users/jdoe1] $\n[compute-012 /users/jdoe1] $**~C**\nssh&gt; **-KL 12345**\nCanceled forwarding.\n[compute-012 /users/jdoe1] $ exit\n[login31 /users/jdoe1] $ exit\n</code></pre>","tags":["in-progress"]},{"location":"sw/r-n-friends/#running-rstudio-server-via-web","title":"Running RStudio Server via web","text":"<p>You can run RStudio via JHPCE Application Portal by login with your JHED ID and password. Please see our page about using the JHPCE Web Portal.</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#r-on-jhpce-common-considerations","title":"R on JHPCE - Common Considerations","text":"","tags":["in-progress"]},{"location":"sw/r-n-friends/#installing-r-packages","title":"Installing R Packages","text":"<p>In all cases, there are a number of R packages preinstalled in each of the R environments.</p> <p>When running the text based version of R or the X11 based version of R, you will be making use of one of the R modules. This module is named conda_R and is managed by Dr. Kasper Hansen in the Biostatistics department in the BSPH.  Dr. Hansen preloads a </p> <p>You can see the various version of the conda_R packages that are available (as of April 2025) with the \"module avail conda_R\" command. </p> <pre><code>[mmill116@compute-112 ~]$ module avail conda_R\n\n--------------------- /jhpce/shared/community/modulefiles ----------------------\n   conda_R/devel    conda_R/4.3.x    conda_R/4.4.x\n   conda_R/test     conda_R/4.3      conda_R/4.4   (D)\n</code></pre> <p>Dr. Hansen preinstals a ton of R packages in these R modules.  If you find that you need to install your own modules, you can also install your own R packages to your home directory.  By deafult, these packages will be installed in the the R directory in your home directory</p> <p><pre><code>[mmill116@jhpce01 ~]$ srun --pty --X11 bash\n[mmill116@compute-112 ~]$ module load conda_R\n(/jhpce/shared/community/core/conda_R/4.4) [mmill116@compute-112 ~]$ R\nR version 4.4.0 Patched (2024-05-22 r86590) -- \"Puppy Cup\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\n. . .\n&gt; install.packages(\"sf\")\n&gt; library('sf')\n&gt; q()\n(/jhpce/shared/community/core/conda_R/4.4) [mmill116@compute-112 ~]$ ls -la R/4.4\ntotal 61\ndrwxr-xr-x  4 mmill116 mmi  4 Apr 10 10:58 .\ndrwxr-xr-x 19 mmill116 mmi 20 May 15  2024 ..\ndrwxr-xr-x 19 mmill116 mmi 25 Apr 10 10:58 sf\n</code></pre> The same directory is used for both the text-based version of R and the X11 version of Rstuido, since they rely on the same \"conda_R\" packages.</p> <p>However for the Rstudio Server options mentioned above the R version will be contained in the singularity used to run Rstudio Server.  So, any R packages that were installed under the conda_R will need to be reinstalled under the Rstudio Server environment.  Any R packages you install will be installed in the directory $HOME/R/packages/singularity-R$VERSION.</p> <p>In all of the above cases, when you change the version of R that you are using, you will need to reinstall any R packages.</p> <p>When installing R packages from source with compiled programs, you can add custom compiler flags in ~/.R/Makevars. Adding optimization flags may provide a boost in performance for some packages.  <pre><code>STDFLAGS = -O2 -pipe -Wall \n</code></pre> We have a wide variety of CPU architecture across the cluster, so you probably don't want to add to STDFLAGS <code>-march=</code> and <code>-mtune=</code> arguments.</p> <p>For the X11 version of Rstudio, you will also be making </p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#installing-your-own-r","title":"Installing your own R","text":"<p>If you find that you have a specific need in R that is not available in our current offerings, you can compile your own instance of R in your home directory. We don't have a guide for doing this, but if you are at this point, you probably have compiled R before, and know what specific settings and customizations you need, and know what you're doing more than we do :-) Here is one example from the Iowa Biostat Site</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#from-lieber-working-with-slurm-via-r-and-slurmjobs","title":"From Lieber - Working with SLURM via R and slurmjobs","text":"<p>The Lieber group has put together some tools for working with R on the JHPCE cluster</p> <p>slurmjobs provides helper functions for interacting with SLURM-managed high-performance-computing environments from R. It includes functions for creating submittable jobs (including array jobs), monitoring partitions, and extracting info about running or complete jobs.</p> <p>R is an open-source statistical environment which can be easily modified to enhance its functionality via packages. </p> <p>slurmjobs is a R package available via the Bioconductor repository for packages. R can be installed on any operating system from CRAN after which you can install slurmjobs </p> <p>For more information about slurmjobs, see</p> <p>http://research.libd.org/slurmjobs/articles/slurmjobs.html</p> <p>For Windows desktops/laptops, you should also use \u201c^C\u201d to terminate the Rstudio Server, but to stop the tunnel you will need to return to the MobaSSHTunnel screen, and use the \u201cStop\u201d icon  in the Start/Stop column.  You can keep this tunnel configuration in MobaSSHTunnel, and reuse it the next time you run Rstudio Server, however you will need to edit the tunnel configuration and change the \u201cRemote Server\u201d to match the compute node you are running on.</p>","tags":["in-progress"]},{"location":"sw/r-n-friends/#faqscomments","title":"FAQs/Comments","text":"<p>Q) Why did you do this?  R works just fine for me on the cluster!</p> <p>A) On the JHPCE cluster we have historically had several ways to run R programs.  Often  people will use the text-based version of R to run programs, and that works well for a lot of people.  Some people prefer to work in a graphical environment, so we also have the X11-based \u201cRstudio\u201d available on the cluster, which is great, except that on a slower network connection, this can get quite laggy.  The web based Rstudio Server provides the same graphical version available in Rstudio, but over a much lighter network protocol than the X11-based Rstudio, so it is much faster and more responsive to use.</p> <p>Q) Why not just set up a dedicated web server to run Rstudio Server like I had back at ZZZZ?</p> <p>A) Rstudio Server does not play well with clusters.  For us to run a dedicated Rstudio Server server, we would need to purchase a fairly large system with lots of RAM and CPU power.  This was considered, but in the end was deemed cost prohibitive, and it didn\u2019t allow the use of the JHPCE cluster resources to run R programs.  This solution allows the nice web-based Rstudio Server to be used, while making use of the existing CPU and RAM resources available on the cluster.</p> <p>Q) My program can\u2019t run because it needs XXX package!</p> <p>A) The R that is run within the Rstudio Server is completely separate  from the default version of R that is used on the JHPCE cluster, therefor you may need to install packages using the install.packages() function, or through the Rstudio Server GUI .</p> <p>Q) I forgot to cleanly disconnect from the Rstudio Server/My session got disconnected.</p> <p>A) This should be fine.  Your interactive srun session will eventually time out and will kill the Rstudio Server that was running.  You may get warning messages about ports being in use \u2013 if so, please wait a few minutes and try again.</p> <p>Q) When I try to add the port forward, I get an error message about \u201cPort is in use\u201d.  How do I fix this?</p> <p>A) You can only run one instance of Rstudio Server. You likely have another SSH session running that has the port forward lingering.  If you had been using Rstudio Server in another SSH session, you will need to either need to log out of that ssh session, or run the \u201c~C\u201d \u201c-KL XXXXX\u201d command to tear down the port forward.</p> <p>If you have any questions  about using Rstudio Server, please feel free to email us at bitsupport.</p>","tags":["in-progress"]},{"location":"sw/r-pkg/","title":"R Packages","text":""},{"location":"sw/r-pkg/#installing-r-packages","title":"Installing R packages","text":"<ul> <li>load R module from a compute node <pre><code>[test@compute-107 ~]$ module load R\nLoading R/4.3\n(4.3)[test@compute-107 ~]$\n</code></pre></li> </ul> <p>Note</p> <p>R points to conda_R. Therefore, loading either R or conda_R would use the same version of R.</p> <ul> <li>use <code>install.package(pkgname)</code> to install a package <pre><code>(4.3)[test@compute-107 ~]$ R\n\nR version 4.3.1 Patched (2023-07-19 r84711) -- \"Beagle Scouts\"\nCopyright (C) 2023 The R Foundation for Statistical Computing\nPlatform: x86_64-conda-linux-gnu (64-bit)\n...\n[Previously saved workspace restored]\n\n&gt; install.package(\"your-package-name\")\n</code></pre></li> </ul>"},{"location":"sw/sas/","title":"SAS","text":""},{"location":"sw/sas/#we-have-a-sas-partition","title":"We Have A SAS Partition","text":"<p>We have a limited number of licenses. They run on computers with limited amounts of RAM. The command <code>slurmpic -p sas</code> will display the current resources used out of total available.</p> <p>Please use this partition by specifying <code>-p sas</code> when submitting your jobs. You can see the available resources in this partition with the command <code>slurmpic -p sas</code> If your job will not fit in the partition or you want to run your job in a PI partition you are authorized to use, you can use a different one.</p>"},{"location":"sw/sas/#other-ways-to-access-sas","title":"Other ways to access SAS","text":"<p>SAS is available in a virtual Windows environment called SAFE maintained by Hopkins central IT. For small jobs, or collaborating on files with private personal information with others, or if you don't have SAS installed on your personal computer, you might want to register for this free service (you have to wait for someone to create your account).</p> <p>Here is a link to information about SAFE.</p>"},{"location":"sw/sas/#marketscan-database","title":"MarketScan Database","text":"<p>Many of our SAS users work with this licensed database.</p>"},{"location":"sw/sas/#how-to-get-permission-to-access-it","title":"How To Get Permission To Access It","text":"<p>Contact  Xuya (Rayna) Xiao  xxiao14@jhmi.edu in the Epidemiology Department. Her group controls the access to the Marketscan data on the JHPCE cluster.  They have some good guides on how the data is organized on the cluster.  She will then ask us to add you to the \"alexander\" UNIX group so you can see the database's files.</p>"},{"location":"sw/sas/#where-is-it-located","title":"Where Is It Located?","text":"<p>As of April 2024 it is located in <code>/dcl02/alexande/data/MARKETSCAN/</code></p> <p>As of August 2024 the latest version of the data is located in <code>/dcl02/alexande/data/MARKETSCAN2024/</code></p>"},{"location":"sw/sas/#using-sas-interactively","title":"Using SAS interactively","text":"<p>The amount of memory you request for your interactive session depends on the size of the data you will be manipulating.</p> <p>If you learn how to work with SAS in batch mode, you can execute jobs without having to wait for slow X11 protocol transmissions of the SAS graphic interface. And you can queue up a series of jobs to run without being present.  See our SAS batch mode section below.</p>"},{"location":"sw/sas/#without-browser-support","title":"Without Browser Support","text":"<p>If you do not intend to do any plotting or use the built-in help, you can start sas with a simple \"sas\" command. <pre><code>[test@login31 ~]$ srun --partition sas --mem 10G --x11 --pty bash\nsrun: job 3178287 queued and waiting for resources\nsrun: job 3178287 has been allocated resources\n\n[test@compute-101 ~]$ module load sas\n[test@compute-101 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) sas/9.4\n\n[test@compute-101 ~]$ sas\n</code></pre></p>"},{"location":"sw/sas/#with-browser-support","title":"With Browser Support","text":"<p>To display either built-in help or graphical output when running SAS, you need to specify options to allow a web browser to be launched if called for. We recommend using the Chromium browser</p> <p>Note</p> <ul> <li>You may need to accept popups in the chromium browser that gets started in order to see the windows that SAS is trying to display.</li> <li>If you do not redirect standard output and standard error data streams, you will see a bunch of error messages. These can be ignored because the browser wants to run on a local system with a graphics card, and the our compute nodes don't have such cards or the software to support them. HOWEVER, redirecting all I/O means that if SAS itself emits important error messages, you won't see them. So don't redirect I/O if SAS isn't working as expected!</li> </ul> <p>The initial steps are the same as we used before: <pre><code>[test@login31 ~]$ srun --partition sas --mem 10G --x11 --pty bash\nsrun: job 3178287 queued and waiting for resources\nsrun: job 3178287 has been allocated resources\n\n[test@compute-101 ~]$ module load sas\n[test@compute-101 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) sas/9.4\n</code></pre></p> <p>Your next step is choose which SAS command to run and which optional arguements to give it to about your files or how you want SAS to behave.</p> <p>Choose one of the following variations (note that you can click on the faint pages icon at the right end of the section to copy the contents to your copy-and-paste buffer):</p> Chrome, no I/O redirection<pre><code>sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/chromium-browser'\" -xrm \"SAS.helpBrowser:'/usr/bin/chromium-browser'\"\n</code></pre> Chrome, all I/O redirected to a black hole (see tip below)<pre><code>csas\n</code></pre> Define these handy aliases for a better life <p>Here is some code you can add to your .bashrc file which contain some convenient bash aliases for starting SAS with browser support configured. Once that becomes part of your environment (by sourcing the file (<code>. ~/.bashrc</code>) or by logging out and back in again), after loading the SAS module you can start SAS using either <code>csas</code> or <code>fsas</code> so that it can open the desired web browser if needed. These definitions include $@ symbols, which are replaced by any additional arguments to SAS that you provide. Note that you can click on the faint pages icon at the right end of the section to copy the contents to your copy-and-paste buffer.</p> <p>Warning</p> <p>These definitions include optional syntax (<code>&gt; /dev/null 2&gt;&amp;1</code>) which hide error messages. If you are having problems launching or using SAS, you may need to run SAS without that output redirection!!!! The \"srun half duplex\" error is an example of such a case.</p> <pre><code># SAS routines for __interactive__ sessions where plotting is involved\n# (because SAS generates HTML for the plots when run in interactive mode)\n# \n# (YOU HAVE TO RUN \"module load SAS\" before calling either of these routines)\n#\n# To use Chromium-browser as your web browser for SAS to display html help files and output:\n#\ncsas() { sas -helpbrowser SAS -xrm \"SAS.webBrowser:'/usr/bin/chromium-browser'\" -xrm \"SAS.helpBrowser:'/usr/bin/chromium-browser'\" \"$@\" &gt; /dev/null 2&gt;&amp;1; }\n</code></pre>"},{"location":"sw/sas/#running-sas-job-in-batch-mode","title":"Running SAS job in batch mode","text":"<p>The amount of memory you request for your batch session depends on the size of the data you will be manipulating.</p> <ol> <li> <p>Write your sas source in a file with .sas extension (e.g. class-info.sas) <pre><code>DATA CLASS;\n     INPUT NAME $ 1-8 SEX $ 10 AGE 12-13 HEIGHT 15-16 WEIGHT 18-22;\nCARDS;\nJOHN     M 12 59 99.5\nJAMES    M 12 57 83.0\nALFRED   M 14 69 112.5\nALICE    F 13 56 84.0\n\nPROC MEANS;\n     VAR AGE HEIGHT WEIGHT;\nPROC PLOT;\n     PLOT WEIGHT*HEIGHT;\nENDSAS;\n;\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. sas-demo1.sh) <pre><code>#!/bin/bash\n\n#SBATCH --partition=sas\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load sas\nsas class-info.sas\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch sas-demo1.sh\nSubmitted batch job 3178475\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            3178475       sas sas-demo    test R       0:01      1 compute-101\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test    0 Mar 13 16:49 slurm-3178475.out\n-rw-r--r-- 1 test test 2108 Mar 13 16:49 class-info.lst\n</code></pre></p> </li> <li> <p>Look at the results from output file <pre><code>[test@login31 ~]$ cat class-info.lst\n\n                                                        The MEANS Procedure\n\n                           Variable    N            Mean         Std Dev         Minimum         Maximum\n                           -----------------------------------------------------------------------------\n                           AGE         4      12.7500000       0.9574271      12.0000000      14.0000000\n                           HEIGHT      4      60.2500000       5.9651767      56.0000000      69.0000000\n                           WEIGHT      4      94.7500000      14.0386372      83.0000000     112.5000000\n                           -----------------------------------------------------------------------------\n</code></pre></p> </li> </ol>"},{"location":"sw/sas/#tips-for-using-the-sas-editor","title":"Tips for Using The SAS Editor","text":"<p>Duke's Sociology department has a page titled Giving the SAS Program Editor Under Linux A Windows Look and Feel</p> <p>Other advice can be found in this presentation for Windows users transitioning to using SAS in a Linux environment.</p>"},{"location":"sw/singularity-stub/","title":"stub page for the \"Software\" topic","text":"<p>This is a stub page for the \"Software\" topic.</p> <p>Until this document is written, we have related info in the document about containers</p>","tags":["needs-to-be-written"]},{"location":"sw/stata/","title":"Stata","text":""},{"location":"sw/stata/#using-stata-interactively","title":"Using Stata interactively","text":"<pre><code>[test@login31 ~]$ srun --mem 10G --x11 --pty bash\nsrun: job 3179341 queued and waiting for resources\nsrun: job 3179341 has been allocated resources\n[test@compute-092 ~]$ module load stata\n[test@compute-092 ~]$ module list\n\nCurrently Loaded Modules:\n  1) JHPCE_ROCKY9_DEFAULT_ENV   2) JHPCE_tools/3.0   3) stata/17\n\n[test@compute-092 ~]$ xstata\n</code></pre>"},{"location":"sw/stata/#running-stata-program-in-batch-mode","title":"Running Stata program in batch mode","text":"<ol> <li> <p>Write your Stata source in a file with .do extension (e.g. stata-demo1.do) <pre><code>program define hello\ndi \"Hello There World 123\"\nend\nhello\n</code></pre></p> </li> <li> <p>Write a submit job script (e.g. stata-demo1.sh) <pre><code>#!/bin/bash\n\n#SBATCH --mem=2G\n#SBATCH --time=2:00\n\nmodule load stata\nstata -b stata-demo1.do\n</code></pre></p> </li> <li> <p>Submit your matlab job <pre><code>[test@login31 ~]$ sbatch stata-demo1.sh\nSubmitted batch job 3179530\n</code></pre></p> </li> <li> <p>Monitor the job status <pre><code>[test@login31 ~]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            3179530    shared stata-de    test  R       0:01      1 compute-092\n</code></pre></p> </li> <li> <p>When the job is finished, the output files are created in your Current Working Directory <pre><code>-rw-r--r-- 1 test test   0 Mar 13 17:29 slurm-3179530.out\n-rw-r--r-- 1 test test 890 Mar 13 17:29 stata-demo1.log\n</code></pre></p> </li> <li> <p>Look at the result <pre><code>[test@login31 ~]$ cat stata-demo1.log\n...\n. do \"stata-demo1.do\" \n\n. program define hello\n  1. di \"Hello There World 123\"\n  2. end\n\n. hello\nHello There World 123\n\n. \nend of do-file\n</code></pre></p> </li> </ol>"},{"location":"sw/sw-overview/","title":"Overview","text":""},{"location":"sw/sw-overview/#introduction","title":"Introduction","text":"<p>The Joint High Performance Computing Exchange (JHPCE) offers a variety of software for use on the cluster. They are installed and running on Rocky Linux 9.4. We use Lmod module system to manage the installed software. You can run <code>module aval</code> at the cluster prompt for the available software.</p>"},{"location":"sw/sw-overview/#current-available-software","title":"Current available software","text":"<pre><code>[test@compute-110 ~]$ module avail\n\n--------------------------------------------------------------------------------- /jhpce/shared/jhpce/modulefiles ---------------------------------------------------------------------------------\n   JHPCE_ROCKY9_DEFAULT_ENV (L)    bcl2fastq/2.17.1              freesurfer/7.4.1           gmp/6.2.1                 mathematica/13.3        python/3.11.8             stata/17\n   JHPCE_tools/3.0          (L)    bowtie/2.5.1                  fsl/6.0.6.5                go/20.6                   matlab/R2023a    (D)    rstudio/2023.06.1         tex/20240117\n   R_test/4.3.2                    conda/3-23.3.1-testing        gcc/9.5.0                  helix/23.10.0             matlab/R2023b           rust/1.76.0               wine/7.11\n   afni/23.3.09                    conda/3-23.3.1         (D)    gcc/13.1.0          (D)    java/19            (D)    mpc/1.3.1               sas/9.0\n   alphafold/4.3.1                 dcmtk/3.6.7                   gdal/3.6.0                 julia/1.9.2               mpfr/4.2.0              sas/9.4            (D)\n   anaconda/2023.03                encfs/1.9.5                   ghostscript/10.02.1        kakoune/2023-08-05        python/3.9.14    (D)    shapeit/5.1.1\n   aws/2.12.7                      ffmpeg/6.0                    glpk/5.0                   latex/20240117            python/3.10.13          singularity/3.11.4\n\n------------------------------------------------------------------------------- /jhpce/shared/community/modulefiles -------------------------------------------------------------------------------\n   R/4.3    bedtools/2.31.0    conda_R/devel    conda_R/test    conda_R/4.3.x (D)    conda_R/4.3\n\n--------------------------------------------------------------------------------- /jhpce/shared/libd/modulefiles ----------------------------------------------------------------------------------\n   PRSice/2.2.13           cell2location/0.1.3          fusion_twas/github            java/17                   plink/1.90b              samtools/1.18       (D)    trimgalore/0.6.6\n   Salmon/1.2.1            cell2location/0.8a0   (D)    gatk/4.2.0.0                  java/18                   plink/2.00a4.6    (D)    samui/1.0.0-next.24        trimmomatic/0.39\n   Salmon/1.10.1    (D)    cellpose/2.2.2               gffread/github                kallisto/0.46.1           qctool/2.0.7             samui/1.0.0-next.45 (D)    vampire/3.4.4\n   arioc/1.51              cellprofiler/4.2.6           gffread/0.12.7         (D)    ldsc/1.0.1                qtl_gtex/dae3369         spaceranger/2.1.0          vcftools/0.1.16\n   bamtofastq/1.4.1        cellranger/7.0.0             git-lfs/3.4.0                 magma/1.10                r_nac/1.0                spagcn/1.2.0               wiggletools/1.2.1\n   bcftools/1.10.2         cellranger/7.2.0      (D)    git-status-size/github        methyldackel/0.5.2        regtools/0.5.33g         star/2.7.8a                wigtobigwig/2.9\n   bcftools/1.18    (D)    cellranger_arc/2.0.2         graphst/da29b75               methylpy/1.4.3            resept/1.0.0             subread/2.0.0\n   bfg/1.13.0              cibersortx/04_04_2020        hipstr/0.7                    nextflow/20.01.0          rmate/1.5.10             synapse/2.7.2\n   bismark/0.23.0          dissect/dc45940c             hisat2/2.2.1                  nextflow/22.10.7          rseqc/3.0.1              synapse/3.1.1       (D)\n   bs/1.3.0                fastqc/0.11.8                htslib/1.10.2                 nextflow/23.10.0   (D)    samblaster/0.1.26        tangram/1.0.4\n   bustools/0.39.3         fastqc/0.12.1         (D)    htslib/1.18            (D)    paste/1.3.0               samtools/1.10            tensorqtl/1.0.8\n</code></pre>"},{"location":"sw/sw-overview/#resources-for-some-software","title":"Resources for some software","text":"<ul> <li>Matlab</li> <li>Python</li> <li>R</li> <li>SAS</li> <li>Stata</li> </ul>"},{"location":"sw/vscode/","title":"Virtual Studio Code","text":"<p>The currently only acceptable way to run vscode on the cluster is via the JHPCE Web Portal. By default, running VSCode with the normal \"Remote-SSH\" extension will run on the JHPCE login node, and this puts undue stress on the login node.</p>"},{"location":"sw/vscode/#rstudio-server","title":"RStudio Server","text":"<p>If you\u2019re trying to run a Singularity image program like RStudio Server which requires creating an SSH tunnel, you cannot issue a Ctrl+C key combination from within the terminal window of VSCode. That\u2019s (almost certainly) not going to work.  The Ctrl+C sends an interrupt back to your original ssh connection from your laptop to the JHPCE login node.  Since you\u2019re using vscode to make that connection, it\u2019s not a \u201creal\u201d ssh session, so it won\u2019t be able to interpret the Ctrl+C.  We don\u2019t believe there is a way around that\u2026 but if you find one please let us know.</p> <p>More information is at https://jhpce.jhu.edu/portal/web-apps/</p>"}]}