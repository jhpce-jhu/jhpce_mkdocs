{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Joint HPC Exchange Test The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This fee-for-service core is a long-standing collaborative effort between Biostatistics and the Computational Biology & Research Computing group in the department of Molecular Microbiology and Immunology. The facility is open to all Johns Hopkins affiliated researchers. The facility is used primarily by labs and research groups in the Johns Hopkins Bloomberg School of Public Health (SPH), the Johns Hopkins School of Medicine (SOM) and the Lieber Institute for Brain Development (LIBD). We have over 800 user accounts, 400 active users and approximately 100 unique users per quarter. The computing and storage systems are optimized for genomics and biomedical research. The cluster has 72 compute nodes, providing over 4000 cores, 30TB of DRAM and over 14 PB of networked mass storage. Networked mass storage uses open-source file systems (ZFS and Lustre-over-ZFS) and custom-built hardware. The latter includes a 5 PB of Lustre file system space that may be the lowest-cost and lowest-power parallel file system on the planet. We also have a 2PB disk-to-disk backup system for backing up more critical data. The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc. The network fabric consists of a 10 Gbps ethernet. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ. Job scheduling is performed with Open Grid Engine (OGE). The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders. If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the director (jhpce@jhu.edu) to determine whether we can accommodate your needs. If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form . Mark Miller and Brian Caffo Co-Directors, JHPCE","title":"Home"},{"location":"#joint-hpc-exchange","text":"","title":"Joint HPC Exchange"},{"location":"#test","text":"The Joint High Performance Computing Exchange (JHPCE) is a High-Performance Computing (HPC) facility in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This fee-for-service core is a long-standing collaborative effort between Biostatistics and the Computational Biology & Research Computing group in the department of Molecular Microbiology and Immunology. The facility is open to all Johns Hopkins affiliated researchers. The facility is used primarily by labs and research groups in the Johns Hopkins Bloomberg School of Public Health (SPH), the Johns Hopkins School of Medicine (SOM) and the Lieber Institute for Brain Development (LIBD). We have over 800 user accounts, 400 active users and approximately 100 unique users per quarter. The computing and storage systems are optimized for genomics and biomedical research. The cluster has 72 compute nodes, providing over 4000 cores, 30TB of DRAM and over 14 PB of networked mass storage. Networked mass storage uses open-source file systems (ZFS and Lustre-over-ZFS) and custom-built hardware. The latter includes a 5 PB of Lustre file system space that may be the lowest-cost and lowest-power parallel file system on the planet. We also have a 2PB disk-to-disk backup system for backing up more critical data. The JHPCE cluster is optimized for the embarrassingly parallel applications that are the bread-and-butter of our stakeholders, e.g., genomics and statistical applications, rather than the tightly-coupled applications that are typical in traditional HPC fields, e.g., physics, fluid-dynamics, quantum simulation etc. The network fabric consists of a 10 Gbps ethernet. The facility is connected via a 40Gbps network to the University\u2019s Science DMZ. Job scheduling is performed with Open Grid Engine (OGE). The JHPCE operates as a formal Common Pool Resource (CPR) Hierarchy with rights to specific resources based on stakeholder ownership of resources. To benefit the entire research community, excess computing capacity is made available to non-stakeholders on an as-available basis, in exchange for fees that defray the operating costs of the stakeholders. If your lab is interested in joining the JHPCE community, either as a stakeholder or as a non-stakeholder, please contact the director (jhpce@jhu.edu) to determine whether we can accommodate your needs. If your lab is already a member, and you need to add new users, then have the users fill out the JHPCE new user request form . Mark Miller and Brian Caffo Co-Directors, JHPCE","title":"Test"},{"location":"documents/","text":"Help Ways to Seek Help and Support Documents Search this website : There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Email for advice : bithelp@lists.johnshopkins.edu . This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff : bitsupport@lists.johnshopkins.edu . This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty.","title":"Documents"},{"location":"documents/#help","text":"","title":"Help"},{"location":"documents/#ways-to-seek-help-and-support","text":"Documents Search this website : There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Email for advice : bithelp@lists.johnshopkins.edu . This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff : bitsupport@lists.johnshopkins.edu . This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty.","title":"Ways to Seek Help and Support"},{"location":"help/","text":"Help Ways to Seek Help and Support Search this website : There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Email for advice : bithelp@lists.johnshopkins.edu . This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff : bitsupport@lists.johnshopkins.edu . This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty.","title":"Help"},{"location":"help/#help","text":"","title":"Help"},{"location":"help/#ways-to-seek-help-and-support","text":"Search this website : There is a lot of information on this site. You can search the site for helpful bits of information by entering keywords into the text box on the upper left. Email for advice : bithelp@lists.johnshopkins.edu . This is the main list to use for \"how-to\" or \"why doesn\u2019t this work\" types of questions. It is comprised of the JHPCE community, including many power-users and system administrators. It's used for questions about running and installing applications, and about R, Bio-conductor, Perl, SAS, C, SLURM etc. This list is also used for announcements by the maintainers of various community tools and resources. To be added to the bithelp list, send a request to bitsupport@lists.johnshopkins.edu. Contact system administration staff : bitsupport@lists.johnshopkins.edu . This email is for communicating with the system administration staff about operational and administrative issues like login problems, quota issues, and system downtime. It is monitored by the system administrators and some faculty.","title":"Ways to Seek Help and Support"},{"location":"knowledge_base/","text":"JHPCE Knowledge Base Categories Authentication 2 Factor Authentication SSH Keypair Disk Storage Space on the JHPCE Cluster Encrypted Filesystem Encrypted Filesystem with encfs Fastscratch Space on JHPCE General Storage Knowledge Sharing Environment Modules File Transfer Setting up MobaXterm for File Transfers with the JHPCE Cluster Using rclone to Access OneDrive GPUs on the JHPCE Cluster under SLURM Granting File Permissions using ACLs JHPCE Web Portal Knowledge Base Articles from Aki Nishimura Knowledge Base Articles from Lieber Institute MobaXterm Configuration Numerical Linear Algebra Libraries Running RStudio Server on the JHPCE Cluster SLURM Using Globus to Transfer Files HOW-TO Community Perl Python R Short Read Tools","title":"Knowledge base"},{"location":"knowledge_base/#jhpce-knowledge-base","text":"","title":"JHPCE Knowledge Base"},{"location":"knowledge_base/#categories","text":"","title":"Categories"},{"location":"knowledge_base/#authentication","text":"2 Factor Authentication SSH Keypair","title":"Authentication"},{"location":"knowledge_base/#disk-storage-space-on-the-jhpce-cluster","text":"Encrypted Filesystem Encrypted Filesystem with encfs Fastscratch Space on JHPCE General Storage Knowledge Sharing","title":"Disk Storage Space on the JHPCE Cluster"},{"location":"knowledge_base/#environment-modules","text":"","title":"Environment Modules"},{"location":"knowledge_base/#file-transfer","text":"Setting up MobaXterm for File Transfers with the JHPCE Cluster Using rclone to Access OneDrive","title":"File Transfer"},{"location":"knowledge_base/#gpus-on-the-jhpce-cluster-under-slurm","text":"","title":"GPUs on the JHPCE Cluster under SLURM"},{"location":"knowledge_base/#granting-file-permissions-using-acls","text":"","title":"Granting File Permissions using ACLs"},{"location":"knowledge_base/#jhpce-web-portal","text":"","title":"JHPCE Web Portal"},{"location":"knowledge_base/#knowledge-base-articles-from-aki-nishimura","text":"","title":"Knowledge Base Articles from Aki Nishimura"},{"location":"knowledge_base/#knowledge-base-articles-from-lieber-institute","text":"","title":"Knowledge Base Articles from Lieber Institute"},{"location":"knowledge_base/#mobaxterm-configuration","text":"","title":"MobaXterm Configuration"},{"location":"knowledge_base/#numerical-linear-algebra-libraries","text":"","title":"Numerical Linear Algebra Libraries"},{"location":"knowledge_base/#running-rstudio-server-on-the-jhpce-cluster","text":"","title":"Running RStudio Server on the JHPCE Cluster"},{"location":"knowledge_base/#slurm","text":"","title":"SLURM"},{"location":"knowledge_base/#using-globus-to-transfer-files","text":"","title":"Using Globus to Transfer Files"},{"location":"knowledge_base/#how-to","text":"","title":"HOW-TO"},{"location":"knowledge_base/#community","text":"Perl Python R Short Read Tools","title":"Community"},{"location":"new_users/","text":"Registration For Principal Investigators If you are a Principal Investigator registering a new project or organization, please fill out this form . If you have never registered a project and budget number with the JHPCE, we request that you contact the director of the JHPCE to arrange a 1/2 hour orientation (either in person or via telephone). For New Users If you are prospective user, please start by getting the approval of a PI who has provided us with a budget number. Then request a user account by filling out this form . Once we have received your form, the JHPCE management team will contact you to schedule your attendance at the next JHPCE Orientation. All new users are required to attend a 1 1/2 hour orientation session. Orientations are generally scheduled every other week.","title":"Register"},{"location":"new_users/#registration","text":"","title":"Registration"},{"location":"new_users/#for-principal-investigators","text":"If you are a Principal Investigator registering a new project or organization, please fill out this form . If you have never registered a project and budget number with the JHPCE, we request that you contact the director of the JHPCE to arrange a 1/2 hour orientation (either in person or via telephone).","title":"For Principal Investigators"},{"location":"new_users/#for-new-users","text":"If you are prospective user, please start by getting the approval of a PI who has provided us with a budget number. Then request a user account by filling out this form . Once we have received your form, the JHPCE management team will contact you to schedule your attendance at the next JHPCE Orientation. All new users are required to attend a 1 1/2 hour orientation session. Orientations are generally scheduled every other week.","title":"For New Users"},{"location":"storage/","text":"Current Storage Offerings There are 2 main categories of storage space available for purchase on the JHPCE cluster, detailed below: Pay-as-you-go space : This includes home directory space, legacy storage space, and leased spaces. Users are charged only for the actual space used and the time data is stored there. For example, using 10 TB of /dcl01/leased space for a year costs $500/year. Project spaces : These are large storage arrays built approximately every 18 months, funded by various labs purchasing allocations on the storage array. For example, the buy-in cost for /dcl02 was $43/TB, with a $300/year storage management fee for a 10TB allocation. Pay-as-you-go spaces are generally more expensive than project spaces due to their smaller size, upfront costs being included in the annual fee, and higher maintenance requirements. Additionally, two types of scratch storage space are available on the cluster: - SGE scratch space : A unique $TMPDIR directory is created for each job/task on the cluster's compute nodes, ranging from 500GB to 4TB. - Personal Scratch space : A network-based filesystem with a backend NVME based storage array, intended for short-term storage of large files. Off-site backup space is also available, with /users directory currently backed up, and other directories backed up upon request. For inquiries about purchasing storage, please email bitsupport@lists.jhu.edu . Current Offerings Type Location Env Var Capacity Quota Lifetime FY21Q3 Rate Charge Basis Notes Home ZFS /users/ $HOME 34TB 100GB Long $345/TByr Used TB - Temp scratch /scratch/temp/ $TMPDIR 500GB-4TB None Transient Free - [1][2] Personal scratch /fastscratch/myscratch/ $MYSCRATCH 22TB 1TB 30 days Free - [1][3] Leased Lustre /dcl02/leased - 200TB As agreed Intermediate $50/TByr Used TB - Project ZFS /dcs04, /dcs05 - 5,000TB As purchased Long $20/TByr + buyin Purchased TB [4] Backup ZFS varies - 2,675TB - Permanent $11/TByr Purchased TB [5] Notes: - [All] Rates fluctuate slightly from quarter to quarter based on actual JHPCE expenses and capacities. - [1] Scratch space is only visible on compute and transfer nodes. Scratch is not visible on the login node. <JQT> = 'job.queue.task'. - [2] Scratch space varies from node to node based on local disk space. - [3] Currently there is a 1TB quota on files in /fastscratch/myscratch, with a 30-day file retention limit. - [4] The buy-in cost for DCS04 is $42/TB. Limited capacity still available for sale. - [5] Backups are done of the /users filesystem. Other filesystems may be backed up upon arrangement with PI. Previous/Legacy Offerings The storage spaces listed below are currently in use but are no longer available for purchase. Type Location Env Var Capacity Quota Lifetime FY2019Q2 Rate Charge Basis Notes Leased Lustre /dcl01/leased - 200TB As agreed Intermediate $50/TByr Used TB - Leased ZFS /legacy - 100TB From legacy Short < $1350/TByr Used TB [1] Leased ZFS /starter/starter-02 - 10TB 10TB Short $1,041/TByr Used TB [1] Project Lustre /dcl01 - 3,400TB As purchased Long $26-$","title":"Storage"},{"location":"storage/#current-storage-offerings","text":"There are 2 main categories of storage space available for purchase on the JHPCE cluster, detailed below: Pay-as-you-go space : This includes home directory space, legacy storage space, and leased spaces. Users are charged only for the actual space used and the time data is stored there. For example, using 10 TB of /dcl01/leased space for a year costs $500/year. Project spaces : These are large storage arrays built approximately every 18 months, funded by various labs purchasing allocations on the storage array. For example, the buy-in cost for /dcl02 was $43/TB, with a $300/year storage management fee for a 10TB allocation. Pay-as-you-go spaces are generally more expensive than project spaces due to their smaller size, upfront costs being included in the annual fee, and higher maintenance requirements. Additionally, two types of scratch storage space are available on the cluster: - SGE scratch space : A unique $TMPDIR directory is created for each job/task on the cluster's compute nodes, ranging from 500GB to 4TB. - Personal Scratch space : A network-based filesystem with a backend NVME based storage array, intended for short-term storage of large files. Off-site backup space is also available, with /users directory currently backed up, and other directories backed up upon request. For inquiries about purchasing storage, please email bitsupport@lists.jhu.edu .","title":"Current Storage Offerings"},{"location":"storage/#current-offerings","text":"Type Location Env Var Capacity Quota Lifetime FY21Q3 Rate Charge Basis Notes Home ZFS /users/ $HOME 34TB 100GB Long $345/TByr Used TB - Temp scratch /scratch/temp/ $TMPDIR 500GB-4TB None Transient Free - [1][2] Personal scratch /fastscratch/myscratch/ $MYSCRATCH 22TB 1TB 30 days Free - [1][3] Leased Lustre /dcl02/leased - 200TB As agreed Intermediate $50/TByr Used TB - Project ZFS /dcs04, /dcs05 - 5,000TB As purchased Long $20/TByr + buyin Purchased TB [4] Backup ZFS varies - 2,675TB - Permanent $11/TByr Purchased TB [5] Notes: - [All] Rates fluctuate slightly from quarter to quarter based on actual JHPCE expenses and capacities. - [1] Scratch space is only visible on compute and transfer nodes. Scratch is not visible on the login node. <JQT> = 'job.queue.task'. - [2] Scratch space varies from node to node based on local disk space. - [3] Currently there is a 1TB quota on files in /fastscratch/myscratch, with a 30-day file retention limit. - [4] The buy-in cost for DCS04 is $42/TB. Limited capacity still available for sale. - [5] Backups are done of the /users filesystem. Other filesystems may be backed up upon arrangement with PI.","title":"Current Offerings"},{"location":"storage/#previouslegacy-offerings","text":"The storage spaces listed below are currently in use but are no longer available for purchase. Type Location Env Var Capacity Quota Lifetime FY2019Q2 Rate Charge Basis Notes Leased Lustre /dcl01/leased - 200TB As agreed Intermediate $50/TByr Used TB - Leased ZFS /legacy - 100TB From legacy Short < $1350/TByr Used TB [1] Leased ZFS /starter/starter-02 - 10TB 10TB Short $1,041/TByr Used TB [1] Project Lustre /dcl01 - 3,400TB As purchased Long $26-$","title":"Previous/Legacy Offerings"}]}